[["index.html", "Data Analysis for Clinical Researchers Preface", " Data Analysis for Clinical Researchers Albert Cobos Preface This is an introductory book on data analysis for clinical researchers. Data analysis methods are presented through real data examples relevant to clinical research. We focus on concepts, interpretation, and statistical thinking, avoiding or minimizing the mathematical basis of statistical methods, in line with the Guidelines for Assessment and Instruction in Statistics Education (GAISE), and with modern data science. Data analysis requires the use of some software to read and analyze data, and to produce some outputs. There are lots of software tools one may use for this purpose, such as SAS, SPSS, Stata, and many other. This book is based on the R language (R, for short), a programming language for statistical computing and graphics. These are the main reasons to choose R: It is a free and open-source software under the GNU General Public License. May be used with the three major platforms: Windows, Linux and Mac. Has very good graphic capabilities. Is well documented, including an easily accessible help system. It is extensible, and has a growing and very active user community. "],["structure-of-this-book.html", "Structure of this book", " Structure of this book In chapter 1 we present the tools used throughout the book: R and RStudio. Chapter 2 is an introduction to data structures frequently used in R to store data. This is a wearisome subject, but it is important to become familiar with R. The remaining chapters follow the chronological order of tasks to be carried out in any real case of data analysis: acquire the data (chapter 3), prepare the data for analysis (chapter 4), explore the data (chapter 5), conduct inferential analyses (chapters 6-9), and report the results (chapter 10). Throughout the book you will find R tutorials in gray areas containing R code, just like this one: # This will print the date of your system in the console Sys.Date() By hovering the cursor over the top-right corner of gray area, you will be able to copy the whole of its contents, and paste it in the RStudio editor. It is extremely important that you run the R code shown in tutorials (in the first tutorial we provide detailed instructions on how to do this). R programming is a skill, and you won’t develop skills by just reading how to do something. Rather, you need to get your hands dirty by doing. In this case, this means to write R code, or at least copying and running it to see what happens. "],["introduction-to-r-and-rstudio.html", "1 Introduction to R and RStudio 1.1 Installing R and RStudio 1.2 First R session 1.3 R objects 1.4 R functions 1.5 Installing R packages 1.6 Loading packages Resources Exercises", " 1 Introduction to R and RStudio In this chapter we introduce R and RStudio. R is a programming language particularly suited for data analysis and data visualization (via graphics), and its free. Rstudio is an integrated development environment (IDE) that greatly facilitates to work with R, and there is also a free desktop version. 1.1 Installing R and RStudio First of all, you need to have R and RStudio installed in you computer. I you don’t have them, you should proceed as follows: Install the last version of R from the CRAN website; see how in this video. Install the free Desktop version of RStudio from here. Now you have both R and RStudio and, for starters, I recommend you to look at the short video Take a tour of RStudio’s IDE, here. As you have seen in the video, the RStudio screen is divided in different areas or panes, some of which have several tabs. This is a very short description of the main purpose of some of these panes and tabs: The top-left pane is and editor where you can write R code. It is likely that you won’t see this pane when you launch RStudio for the first time after installation. The bottom-left pane is the R Console, and here is where text results will be printed. You can also write R code in the console, after the prompt symbol (&gt;), and submit it by pressing Enter. The top-right pane has several tabs: The Environment tab shows the R objects created in an R session; it will be empty if you have not created any. You will wonder what is an object. Well, the best we can say by now is that anything that exists in an R session is an object. For instance, a collection of data, a graphic, or the results obtained by a statistical analysis procedure, are objects. The History tab shows a log of all the commands you have submitted. The Tutorial tab contains interactive tutorials. The bottom-right pane has several tabs: The Files tab shows the contents of the folder R is pointing to, and is similar to the file explorer in Windows. The Plots tab is where graphics are displayed. The Packages tab is to see and manage R packages. In a fresh installation of R, you will see many packages, but many more can be easily installed from this tab. The Help tab is where you can look for information of specific R commands or functions, and contains several resources to learn R and RStudio, R manuals, and more. The Viewer tab is used to show results that are formatted in specific ways, such as html. This is a lot of information, and is presented just for reference. But don’t worry, you will get familiar with all these as you use RStudio. 1.2 First R session It’s time to open RStudio and have your first R session, proceeding as follows: Open RStudio. Open a new scripting tab using the menu option File &gt; New file &gt; R script: a new tab will appear in the top-left pane, with name Untitled1. Copy the R script in the gray area below and paste it in this new tab (you can easily copy the script using the Copy to clipboard icon that will appear on hovering the cursor over the top-right corner of the gray area). Place the cursor in the very first line you pasted, and press Ctrl+Enter (or click the Run icon in the top-right corner of the Untitled1 tab). The code will be submitted to the console and the result will be printed below. In addition, the cursor will move to the next line, so that you can directly press Ctrl+Enter again to run the line. Note that texts preceded by a hash (#) are comments, and are ignored by the R interpreter. In this first tutorial we show how to use R as a calculator, and we introduce some common mathematical functions, such as sqrt(), exp(), log(), and round(). # Arithmetic operators 10 + 2 # addition 10 - 2 # subtraction 10 * 2 # multiplication 10 / 2 # division 10 ^ 2 # power # Combine them, using brackets for precedence as in standard maths notation 2 * 10 -1 # usual precedence rules 2 * (10 -1) # use brackets for precedence # Mathematical functions sqrt(25) # square root exp(1) # exponential log10(100) # decimal log log(10) # natural log # Combine functions with arithmetic operators or with other functions sqrt(25) * log10(100) / 10^2 # Nested functions: use functions within functions sqrt(exp(2)) # square root of the exponential of 2 exp(sqrt(2)) # exponential of the square root of 2 # Round to the desired number of decimals round(3.141593, 2) round(3.141593, 0) round(3.141593) Once you have finished the tutorial, you can save the script in the Untitled1 pane to a file, using the save button, or the File &gt; Save menu option. This will open a dialog from which you can navigate to a desired folder, provide a file name (e.g. my_first_R_session) and click the Save button. A new file will appear in the folder you chose, named my_first_R_session.R. The .R extension means that this is an R script file. If you save an R script to file, you will be able to reopen it in RStudio using the menu option File &gt; Open file.... We recommend you save the scripts of all the tutorials in this book, so that by the end of it you will have a nice collection of examples to refer to in the future. 1.3 R objects In the previous tutorial we produced some results that were printed in the console, but we did not save any. In this tutorial you will learn how to store your results in objects. In particular, you will learn: how to create an object by assigning some result to an object name with the assignment operator &lt;- (a handy way to compose this operator in RStudio is by pressing Alt+-). that R is case-sensitive: lowercase and uppercase letters are not the same, e.g., \\(A \\ne a\\). how to remove objects from the workspace (Environment tab), with function rm(). how to save the objects in the workspace to a file, with function save.image(), and how to load these file at a later time, with function load(). how to know what is the current working directory in an R session (the folder of your computer R is pointing to), with function getwd(). Before you start, ensure that the workspace (Environment tab of the top-right pane of RStudio) is empty. If it’s not, you can delete all objects listed by using the brush icon. # Create object by assigning a name to a value or expression, # using the assignment operator: &lt;- weight &lt;- 90 height &lt;- 1.80 BMI &lt;- weight / height^2 # These objects now appear in the Environment tab # When you create an object, its contents is NOT printed in the console # to print (the contents of) an existing object, just run its name weight BMI # Object names cannot include blank spaces body weight &lt;- 90 # this is wrong (and gives an error) body_weight &lt;- 90 # this is ok # The assignment operator is an arrow composed with a left angle bracket (&lt;) and # a hyphen or dash (-), WITHOUT blank spaces between them weight &lt; - 50 # this is wrong... but does not give error: why? # Case sensitivity bmi # gets an error: there is no object with this name BMI # ok Round(BMI, 1) # gets an error: there is no function with this name round(BMI, 1) # ok # Care when using both upper and lowercase names a &lt;- 1 A &lt;- 2 A == a # is A equal to a ? a A # Remove objects with function rm() rm(body_weight, a, A) # Now the environment has only three objects: BMI, height and weight # Save your workspace (i.e., all objects in there) to a file save.image(&quot;my_objects.RData&quot;) # A file of name &quot;my_objects.RData&quot; was saved to your working directory # The working directory is the folder R is pointing to. # But what is this folder? getwd() # Now, close RStudio and reopen it for a fresh new session, or # simulate a new session by just removing all objects in the workspace. # This will load the file you saved load(&quot;my_objects.RData&quot;) # Now you should see the objects back in the Environment tab Note that, with RStudio, you can change the working directory using the menu option Session &gt; Set working directory. Alternatively, you can use function setwd() to specify the folder in your computer you want to become the working directory, as in this example: setwd(&quot;C:/Users/acobo/Documents&quot;) A warning for Windows users: note the route of a folder shoud be specified using forward slashes (/). 1.4 R functions An R function is what we use to make something happen. Functions perform a specialized task and provide a result. In previous sections we used some functions, like log(), round(), and Sys.Date(), but there are many other built-in functions in R. In the following tutorial we discuss some general ideas about R functions, how to use them and how to get help on them. To use a function we write its name followed by brackets. Often (but not always), we write some things within the brackets which are called function arguments. A function may take one or more arguments, or no arguments at all. When a function takes more than one argument, we separate them with commas. # a function with no arguments Sys.Date() # a function call with one argument round(10/3) # a function call with two arguments (comma separated) round(10/3, 2) # a function call with three arguments (comma separated) hist(rnorm(100), main = &quot;Histogram (n = 100)&quot;, xlab = &quot;Z score&quot;) The arguments a function can take do have names. When calling a function, arguments can be identified either by name (if explicit), or by position (otherwise). For instance, the round function can take two arguments: x: the number we want to round. digits: the number of decimals we want to round x to. # arguments identified by name: order is irrelevant round(digits = 2, x = 10/3) # arguments identified by position: order is relevant! round(10/3, 2) # a mixture of both; order is relevant for arguments not explicitly named! round(10/3, digits = 2) # how to know the arguments a function can take, and their names? # looking at the function help with ?function_name, e.g.: ?round The help of a function is always structured in several sections, like Description, Usage, Arguments, Details, and Examples. In the Arguments section you can see what are the argument names, an explanation of what they are, and their order. You can access the help of a function by running its name (without the brackets!) after a question mark (e.g., ?round), or writing the function name in the search box of the RStudio Help tab of the bottom-right pane. 1.5 Installing R packages R is a software structured in packages. A package is a bunch of functions packed together. When you install R, a collection of packages is downloaded to your computer. If you look at the Packages tab of RStudio (bottom-right pane) you will see the packages that are already installed. For instance, you will see packages base and stats, since they come with the basic installation of R. However, there are many other packages available in the CRAN repository, and you need to know how to install them. For instance, there is a package called mosaic you will probably not find among your installed packages. Since we will use this package in future chapters, it’s a good idea to install it. To install a package, click the Install button in the Packages tab of RStudio and you will be presented the dialog shown in figure 1.1. Figure 1.1: Dialog to install a package You only need to write the name mosaic in the Packages box (or pick it from the list that will appear as you start writing) and click on the Install button. Be sure the Install dependencies box is checked (dependencies are other packages needed by the one you are installing). You will notice that, when you click the Install button, the following is written in the R console: install.packages(&quot;mosaic&quot;) This means that the install.packages() function is what is actually used to install packages, and the Install button in RStudio only sends this to the console. After running this function, you will see a confirmation message in the console (package ‘mosaic’ successfully unpacked and MD5 sums checked). Installing a package means downloading it to your computer, and therefore is a one time operation: you need to do it just once per R installation. 1.6 Loading packages In previous sections we have used functions from packages in the base R installation such as round(), log() and others. These functions are readily available in any R session, but this is not the case of functions contained in packages you install. For instance, the following script calls favstats(), a function from the mocaisc package, but when we run it we get an error message stating that function favstats() was not found: favstats(age ~ sex , data = HELPrct) Error in favstats(age ~ sex, data = HELPrct): could not find function &quot;favstats&quot; To use a function from packages you have installed, you need to do something special, and there are two options: prefix the function with the package name plus ::, as in mosaic::favstats(), or load the package first using function library(). # Option 1 mosaic::favstats(age ~ sex , data = HELPrct) sex min Q1 median Q3 max mean sd n missing 1 female 21 31 35 40.5 58 36.25234 7.584858 107 0 2 male 19 30 35 40.0 60 35.46821 7.750110 346 0 # Option 2 library(mosaic) favstats(age ~ sex , data = HELPrct) sex min Q1 median Q3 max mean sd n missing 1 female 21 31 35 40.5 58 36.25234 7.584858 107 0 2 male 19 30 35 40.0 60 35.46821 7.750110 346 0 As you see in the previous scripts, both options work, but while library(mosaic) in option 2 loads the entire package into memory, prefixing the function with the package name (option 1) does not. Another important difference is that prefixing the function with the package name makes explicit the function you are using, and this may be important because some packages have functions sharing their name with some base R functions. For instance: The base package has a function mean() (see ?base::mean). The mosaic package has a function mean() (see ?mosaic::mean). Then, which one is going to be used when we call mean()? Well, if you load the mosaic package with library(mosaic), the following warning is printed in the console: The following objects are masked from ‘package:base’: max, mean, min, prod, range, sample, sum) This means that, from now on in your session, every time you call one of the functions mentioned in the above message (such as mean()), the one from mosaic will be used. However, when you look at a long script and see a call to mean(), it may be hard to know if this refers to mean() from the base package, or to mean() from the mosaic package. Prefixing a function with the package name (e.g., mosaic::mean()) removes any possible ambiguity, and for this reason, is considered a good programming practice. Last, in case you prefer to avoid prefixing functions with the package name, remember to load the package first with library(), and to do it in every new R session. While installing a package is a one time operation, loading an installed package is an operation you need to do in every R session (every time you open RStudio). Resources CRAN is the official website of R. Here you can not only download R, but also find useful information, such as the CRAN task views providing guidance on which packages are relevant for tasks related to a certain topic. The RStudio website has a Resources section where you can find webinars and videos, cheatsheets and books, among other things. A two-pages RStudio IDE Cheat Sheet. R-bloggers is a blog on R publishing lots of articles on the use of R, including tutorials for learning R. A post is R-bloggers on how to use the help page for a function in R Another post is R-bloggers with a complete guide to installing R packages form CRAN and other R package repositories. Quick-R is an extremely well organized website on R, particularly useful for beginners. There are sections on data management, statistics, graphics and more. Exercises Look at the help of round() and read sections Description and Usage. What do you get when using floor(10/3) and ceiling(10/3)? In the Usage section you will see round(x, digits = 0). What do you think digits = 0 means? (hint: read the explanation of round in the Description section). Start at fresh RStudio session (or empty the workspace) and write a script in a new editor tab (File &gt; New File &gt; R script) to: Create an object with name my_weight taking the value of your body weight, in kilograms. Create an object with name my_height taking the value of your body height, in meters (remember to use the dot as decimal separator, e.g. 1.78) Compute the body mass index (by dividing the weight over the square of the height) and assign it to an object with name BMI. Round the value of BMI to the first decimal. Look for a package called dplyr in the Packages tab of RStudio. If you are working with a new installation of R, you will not find it. Then, install this package using the Install button in this tab. When the installation finishes, verify that this package now appears in the Packages tab. Open RStudio and investigate what is your working directory with function getwd(). Then change it to another folder of your preference with the menu option Session &gt; Set working directory &gt; Choose Directory.... When done, use again getwd() to see what is now the working directory. "],["r-data-structures.html", "2 R data structures 2.1 Vectors 2.2 Lists 2.3 Dataframes 2.4 Factors 2.5 Dates 2.6 Other data structures Resources Exercises", " 2 R data structures The term data structures refers to objects devoted to store data. There are many different data structures in R. In this chapter we introduce the most basic, and commonly used ones. 2.1 Vectors Vectors are the simplest possible objects in R (sometimes called atomic objects). They are ordered collections of elements. There are six types of vectors in R, but here we will be concerned with the three types listed in table 2.1. Table 2.1: Vector types Type Elements are: Example numeric (real) numbers 1, 0, 3.14, 1984 character (quoted) character strings “Yes”, “No”, “Maybe” logical (unquoted) logical values TRUE, TRUE, FALSE, TRUE A vector can contain any number of elements, but all of them have to be of the same type: we cannot mix different data types in a single vector (e.g., character and numeric elements). In the following tutorial you will learn how to create vectors with function c(), verify what is the type of a vector with function class(), and what is the number of its elements with function length(). A vector can have missing elements, and these are represented with the symbol NA. Last, you will see how to get particular elements in a vector with the subsetting operator [], and how to set them using the assignment operator &lt;-. # Creating vectors with function c() ages &lt;- c(51, 65, 90, 93, 72) # a numeric vector ages antec &lt;- c(&quot;stroke&quot;,&quot;AMI&quot;,&quot;Breast ca.&quot;) # a character vector antec current_smoker &lt;- c(FALSE, FALSE, TRUE) # a logical vector current_smoker mixed &lt;- c(67, &quot;stroke&quot;, FALSE) # try to mix types: no error issued ? mixed # but coerced to character (note the quotes) # Getting the type of a vector class(ages) class(antec) class(current_smoker) class(mixed) # Getting the number of elements in a vector length(ages) length(antec) length(current_smoker) # Use the symbol NA for missing elements more_ages &lt;- c(69, NA, 37) more_ages all_ages &lt;- c(ages, more_ages) # can concatenate vectors with c() all_ages # Getting/setting vector elements # Refer to an element by position using square brackets: all_ages all_ages[7] # gets the 7th element all_ages[7] &lt;- 65 # sets the 7th element to 65 all_ages # verify all_ages[3:6] # gets elements from positions 3 to 6 Very often we need of perform arithmetic operations with numeric vectors. For instance, we may want to compute the body mass index (BMI) from a vector of body weights and a vector of body heights. Then, we may want to compute summary statistics of the BMI values, like the mean and standard deviation. weight &lt;- c(51, 65, 90, 93, 85) # weights of five patients height &lt;- c(1.65, NA, 1.85, 1.80, 1.60) # heights of the same five patients # Arithmetic operations bmi &lt;- round(weight / height^2, 1) # compute BMI bmi # result is NA if height or weight is NA # Statistical functions mean(weight) # the mean of weights median(weight) # the median sd(weight) # the standard deviation summary(weight) # some of the previous at once # Care with missings! If a vector contains missing data (NA), # you need to set argument na.rm = TRUE in most statistical functions: mean(bmi) # since some values are NA, result is NA mean(bmi, na.rm=TRUE) # removes NA for calculation sd(bmi) # same for sd() sd(bmi, na.rm=TRUE) summary(bmi) # but not needed for summary() Also, a very common task is to compare the elements of a numeric vector to some threshold value using relational operators such as ==, !=, &gt;, or &gt;=. We can also compare two vectors x and y element-wise, provided they have the same length (e.g., x == y). In any case, the result of a comparison is always a logical vector. Note that the equality operator is not a single (=) but a double equal sign (==): try to remember this, it’s a very common mistake! Another very common mistake is to compare something to NA using the equality operator (==): this does not work, and function is.na() should be used for this purpose. Comparisons are illustrated in the following tutorial, which is a continuation of the previous one (do not empty the workspace!). # Relational operators for comparisons bmi == 18.7 # is bmi equal to 18.7? note the DOUBLE equal sign! bmi != 18.7 # is x different from 18.7? bmi &lt; 25 # is bmi less than 25? bmi &lt;= 25 # is bmi less than, or equal to 25? bmi &gt; 30 # is bmi greater than 30? bmi &gt;= 30 # is v greater than, or equal to 30? # Can also compare two vectors having the same number of elements x &lt;- c(1, 0, 0, 0, 1, 0, 0, 0, 1) y &lt;- c(0, 0, 1, 1, 1, 1, 1, 0, 0) x == y x != y x &gt; y x &lt;= y # Very important! bmi == NA # this does not work with NA is.na(bmi) # use this instead Character vectors are useful to store textual information like diseases, symptoms or drugs. However, working with character data (sometimes called strings) is often a challenge for several reasons. First, a word may be written in lowercase, uppercase, or a mixture of both. Because R is case sensitive, “stroke” is not the same as “STROKE”. Second, white spaces are also characters (though they do not catch the eye!) and therefore “stroke” and “stroke \\(\\;\\)” are also different. For these reasons, it is sometimes useful to convert character vectors to uppercase (or lowercase), and to know how to remove leading or trailing blanks. Here is a demo: # Working with character vectors antec &lt;- c(&quot;stroke&quot;, &quot; stroke &quot;, &quot;Breast cancer&quot;, &quot;STROKE&quot;, &quot; Stroke &quot;) # Remember case sensitivity: # &quot;stroke&quot;, &quot; stroke &quot;, &quot;STROKE&quot;, and &quot; Stroke &quot; are different things! antec == &quot;stroke&quot; toupper(antec) # convert to uppercase tolower(antec) # convert to lowercase trimws(antec) # remove leading and trailing blanks tolower(antec) == &quot;stroke&quot; # useful for comparisons trimws(tolower(antec)) == &quot;stroke&quot; # better A very useful function to combine strings is the paste() function. In fact, you can combine not only strings, but also numeric values (that will be coerced to character). Sometimes we need to know what is the number of characters of the elements of a character vector, and this can be done with function nchar(). patient_no &lt;- 1:5 age &lt;- c(62, 88, 35, 75, 81) sex &lt;- c(&quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;) paste(patient_no, sex) # paste corresponding elements paste(patient_no, sex, sep = &quot;: &quot;) # can define a separator (default is a blank space) # Can paste more than two vectors, and mix with constants strings (&quot;Patient&quot;, &quot;is a&quot;) # note that constants are recycled (i.e., reused for all elements) narrative &lt;- paste(&quot;Patient&quot;, patient_no, &quot;is a&quot;, age, &quot;years old&quot;, sex, &quot;with antecedent of&quot;, trimws(tolower(antec))) narrative # Number of elements and number of characters length(narrative) # the number of elements in vector nchar(narrative) # the number of characters of each element Other common tasks with the strings are to extract just a part of them, or to substitute a part of them. These can be done with functions substr() (for substring) and sub() (for substitute), respectively. In both functions we indicate the character vector from which to extract or substitute a part in the first argument (named x). In substr() we need to specify the part we want to extract from x by indicating the first and last positions in arguments start and stop, respectively. In sub() we need to specify the pattern to be replaced, the replacement string, and the addition argument fixed = YES to avoid alternative interpretations of the pattern argument 1. # Extract a part of the string substr(narrative, start = 16, stop = 23) # characters 16 to 23 substr(narrative, 16, 17) # characters 16 to 17 substr(narrative, 16, nchar(narrative)) # characters 16 to last # Replace a part of the string sub(pattern = &quot;is&quot;, replacement = &quot;was&quot;, x =narrative, fixed = TRUE) Whatever the type of a vector, its elements can be named. We can provide names for the elements of a vector when creating it with function c(), or later with names() &lt;- . x &lt;- c(Peter = 31, Paul = 28, Mary = 25) x names(x) # gives the names as a character vector names(x)[3] # therefore, you can refer to any element names(x)[3] &lt;- &quot;Adeline&quot; # or set any element x y &lt;- 1:3 # a numeric vector names(y) # whose elements have no names names(y) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) # but we can set them later y Last, it is possible to make type conversions. For instance, you may need to convert a numeric vector containing zeros and ones to a logical vector, or viceversa. Sometimes, you want to extract part of a string containing digits with substr(). However, because a part of a string is by definition a string as well (even if it contains only digits!), we need to convert these to numeric. Other conversions are less common but possible, using a family of functions, the as. family. In some instances, type conversions are done automatically. For example if we try to sum the elements of a logical vector: because addition is an operation defined on numbers, logical values will be coerced to numeric behind the scene, as shown at the end of the following tutorial on type conversions. number_of_seizures &lt;- 0:4 number_of_seizures # numeric to logical (rule: 0 -&gt; FALSE, anything else -&gt; TRUE) seizures &lt;- as.logical(number_of_seizures) seizures # character to numeric x &lt;- substr(narrative, 16, 17) # extracts the age x class(x) # but the result is a character vector age &lt;- as.numeric(x) # now it&#39;s a numeric vector age class(age) # logical to numeric (rule: FALSE -&gt; 0, TRUE -&gt; 1) diabetes &lt;- c(TRUE, TRUE, FALSE, FALSE) as.numeric(diabetes) # character to numeric or logical (less common) z &lt;- c(&quot;0&quot;, &quot;One&quot;,&quot;2&quot;,&quot;120&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;, &quot;Are you tired?&quot;) z as.numeric(z) # only digits converted to numbers as.logical(z) # only words &quot;TRUE&quot; and &quot;FALSE&quot; converted to logical # Automatic conversion from logical to numeric (rule: FALSE -&gt; 0, TRUE -&gt; 1) sum(diabetes) Ops! just one more thing. In R, scalars (single numbers) are considered vectors of length one. Same for a single character string, or a single logical value. So, nothing exists in R simpler than a vector! x &lt;- 3.14 class(x) is.vector(x) length(x) x &lt;- &quot;A&quot; class(x) is.vector(x) length(x) x &lt;- FALSE class(x) is.vector(x) length(x) 2.2 Lists In the previous section we have seen that vectors cannot mix data types, but lists can. In fact, lists can mix not only data types, but object types. Lists are arbitrary collections of objects (including other lists!), and therefore provide a highly flexible structure to store data of any kind. Lists can be created with function list(), by enumerating its elements separated by commas. The length of a list is the number of elements it contains, and can be obtained with length(). # Creating a list x &lt;- list(&quot;male&quot;, # 1st element 79, # 2nd element c(&quot;arterial hypertension&quot;, &quot;diabetes&quot;, &quot;stroke&quot;), # 3rd element matrix(c(260,110, 190, 100, 185, 100, 160, 95), nrow=2, # 4th element dimnames = list(BP = c(&quot;SBP&quot;, &quot;DBP&quot;), day = 1:4))) x class(x) # object type? length(x) # number of elements it contains When working with lists, two common operations are subsetting and extraction: Subsetting a list means taking a part or subset of the list elements, and the result is always a list (even when we subset just one of its elements!); this is done with the subsetting operator: []. Extracting from a list, means getting an element of the list, and the result will not be a list, but an object of the class this element belongs to; this is done with the extraction operator: [[]]. When applied to vectors, these two operators produce the same result (x[3] == x[[3]]), but this is not the case with lists. For instance, in the example above, we created a list with data from a patient, the second of which elements is the age, a numeric vector of length one. Suppose we want to get this value and plug it in some mathematical formula. In this case we need to extract this element (x[[2]]), the result being a numeric vector we can use in arithmetic expressions. However, if we subset the second element of the list (x[2]), the result will be a list of length one (but still a list!), and we cannot plug a list in a mathematical expression. The following tutorial demonstrates this important difference. # Subset a list with the SUBSETTING operator: [] x[1:2] class(x[1:2]) # the result will be ALWAYS another list x[2] # even when subsetting just one element! class(x[2]) # see? # Extract an element from a list with the EXTRACTION operator: [[]] x[[2]] class(x[[2]]) # the result is the element class (numeric vector in this case) # We can use numeric vectors in mathematical expressions, but NOT lists! x[[2]] + 10 # is OK x[2] + 10 # is WRONG People is often confused about the difference between x[2] and x[[2]]. If this is your case, think of a list as a set of numbered boxes, each one containing something. Suppose that box number 2 contains a sandwich. Subsetting the list means taking some of the boxes, maybe just one, maybe just box number 2, but you still have a box in your hands and you can`t eat a box! If you want to eat your sandwich, you open the box to extract your sandwich, and then you eat it. If you are hungry, you better understand the difference between subsetting a list and extracting from a list. Just like vectors, lists can be named, and the names of elements can be provided when creating the list, or later with names(). In the previous tutorial we saw how to extract a single element from a list indicating its position with the extraction operator [[]]. When a list has names, we can extract elements by position, but also by name, using either the extraction operator or the construct listname$elementname. The last is very practical, and is shown below: # Provide names when creating the list x &lt;- list(sex = &quot;male&quot;, age = 79, antecedents = c(&quot;arterial hypertension&quot;, &quot;diabetes&quot;, &quot;stroke&quot;), matrix(c(260,110, 190, 100, 185, 100, 160, 95), nrow=2, dimnames = list(BP = c(&quot;SBP&quot;, &quot;DBP&quot;), day = 1:4))) x names(x) # get the names names(x)[4] # 4th element has no name names(x)[4] &lt;- &quot;BP&quot; # let`s set it names(x) # all named now # Extract an element x[[1]] # by position x[[&quot;BP&quot;]] # by name x$age # by name, using $ x$sex x$antecedents x$BP Lists are very important data structures for several reasons, but one of them is because the results produced by many functions in R, including functions for statistical analysis, are stored in a list. For instance, in the following tutorial we use a binomial test2 to evaluate if fathering a boy or a girl are equal likely, from a series of 100 deliveries. Then, we inspect the result to realize it is a list, we investigate the name of its elements, and we extract some of them. # A binomial test for the probability of boy being 0.5 # suppose we observe n = 100 births, among which x = 45 are boys res &lt;- binom.test(x = 45, n = 100) res class(res) # class &quot;htest&quot; is a specialized type of list, but still a list is.list(res) # see? names(res) # get the names of its elements res$p.value # get the p.value res$estimate # get the proportion of boys res$conf.int # get the confidence interval for this proportion 2.3 Dataframes Dataframes are named lists of vectors, all having the same length. Because they are lists, they can mix vectors of different types, making dataframes ideal to store tabular data (arranged in rows and columns). Dataframe vectors are the columns of a data frame, and are usually called variables. Dataframes can be created with data.frame(), by enumerating its vectors separated by commas. These can be already existing vectors, or vectors we create on the fly. The length()of a dataframe is the number of variables it contains. Because dataframes are named lists of vectors, we can refer to these vectors using the construct dataframename$variablename. This can be used in conjunction with the assignment operator to add a new variable to a dataframe, or to remove an existing variable from a dataframe (by passing it NULL), as shown in below. sex &lt;- c(&quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;) w &lt;- c(60, 69.5, 83.9, 76.3, 77.3, 88.8) # Creating a dataframe from vectors of the same length) d &lt;- data.frame(case = 1:6, # vector created on the fly age = c(25, 32, 19, 41, NA, 56), # same sex, # already existing vector weight = w) # same, but give it a name d class(d) length(d) # number of variables (columns) ncol(d) # same nrow(d) # the number of rows names(d) # get variable names as a character vector # Get dataframe variables d[[2]] # by position d$age # by name class(d$age) # class of this object? class(d$sex) # Note that sex and d$sex are two different objects sex == d$sex # equal by now sex[3] &lt;- NA # change sex sex # verify change d$sex # d$sex remains unaltered # Add variable to a dataframe d$height &lt;- c(165, 175, 158, 180, 175, 160) length(d) d # Remove variable from a dataframe d$case &lt;- NULL length(d) d The data.frame() function is not the only way to create a dataframe. In the next chapter you will learn how to read data stored in an external file (e.g., an Excel file), resulting in a dataframe. This is in fact a very common way to create a dataframe. 2.4 Factors A factor is the data structure used in R to represent categorical variables. Categorical variables are classifications having a number of classes, categories or levels. Examples of categorical variables are sex (male or female), smoker (never smoked, ex-smoker or current smoker) or previous surgery (yes or no). Data on a categorical variable is usually stored as either a character vector of level descriptors (e.g., “male”, “male”, “female”, …), or as a numeric vector using numeric codes (e.g., 1 for males, 2 for females). Factors are created with function factor(). The first argument to this function is the (numeric or character) vector containing the data. If numeric, we need to clarify the meaning of numeric codes, with two additional arguments: levels: to specify the numeric codes (as a numeric vector) labels: to specify the corresponding texts (as a character vector) The following script shows how to create a factor from a numeric vector. We first create a numeric vector x using the rep() function (to repeat values 1 and 2, five times), and then define a factor from it. x &lt;- rep(1:2, 5) # numeric vector using rep() x # Creating a factor from a numeric vector sex &lt;- factor(x, levels = c(1,2), labels = c(&quot;male&quot;, &quot;female&quot;)) sex class(sex) A couple of things are worth noting in the previous example. First, both levels and labels have to be passed as vectors. Second, it is the order of levels and labels what establishes their correspondence: first level with first label, second level with second label, and so on. Last, when a factor (like sex) is printed to the console, its values are not surrounded by quotes (as is the case with character vectors), and the levels of the factor are stated after printing the values. Let’s now see how to define a factor from a character vector. In this case, if the strings of this vector are self-explanatory (e.g., “male”, o “female”), we only need to specify the character vector from which the factor is to be created. However, the levels will be defined in alphabetical order, and this is not always what we want. If we want a specific ordering of the levels, we can just define them in the levels argument. Last, if the strings are not self-explanatory, or we want to change them, we can also use the labels argument. The following script illustrates these three options. x &lt;- rep(c(&quot;low&quot;, &quot;normal&quot;, &quot;high&quot;), 2) x # option 1: levels and labels not explicitly defined hb &lt;- factor(x) hb class(hb) levels(hb) # option 2: define levels to force desired ordering hb &lt;- factor(x, levels = c(&quot;low&quot;, &quot;normal&quot;, &quot;high&quot;)) hb class(hb) levels(hb) # option 3: define levels and labels x &lt;- rep (c(&quot;l&quot;, &quot;n&quot;, &quot;h&quot;), 2) hb &lt;- factor(x, levels = c(&quot;l&quot;, &quot;n&quot;, &quot;h&quot;), labels = c(&quot;low&quot;, &quot;normal&quot;, &quot;high&quot;)) hb class(hb) levels(hb) You may be wondering why we should care about the ordering of the levels of a factor. The reason is this ordering has implications for graphics, tables and statistical analyses. For instance, when we tabulate results of a factor, its levels appear in the order they are defined. Also, the first level of a factor, which is sometimes called the reference level, is the one other levels will be compared to in some analyses such as ANOVA or regression models. For this reasons, it is convenient to ensure a reasonable ordering of factor levels. 2.5 Dates Date is a class of objects used to represent dates in R. There are two related object classes that are used to represent date-time values (values that combine date and time, such as “21/03/2020 07:45:28”): POSIXct, and POSIXlt. When we have just date values (e.g., “21/03/2020), the appropriate class to use is Date. Dates can be created from a character vector with as.Date(). The format argument is used to specify the formatting of the character strings. If these are formatted as yyyy-mm-dd, we don’t need to specify the format value, but otherwise we need to specify it. Dates can be created from POSIXct and POSIXlt objects as well. # From a character vector of yyyy-mm-dd values x &lt;- c(&quot;1958-03-05&quot;, &quot;1991-10-28&quot;) x class(x) dob &lt;- as.Date(x) dob class(dob) # From a character vector of dd/mm/yyyy values x &lt;- c(&quot;05/03/1958&quot;, &quot;28/10/1991&quot;) x class(x) dob &lt;- as.Date(x, format = &quot;%d/%m/%Y&quot;) dob class(dob) # see help on format symbols (Details section) ?strptime # From POSIXct or POSIXt values x &lt;- as.POSIXct(c(&quot;1958-03-05 07:30:04&quot;, &quot;1991-10-28 10:05:39&quot;)) x class(x) dob &lt;- as.Date(x) dob class(dob) The reason why we should store dates in a Date object is to allow computations. For instance, we can compute the current age from birth dates. Or, we can determine the date in ninety days from today. When we operate on dates, the result is not a standard numeric value, but a difftime object. However, we can convert it to numeric with as.numeric(). This is recommended before further operations with time intervals. dob &lt;- as.Date(c(&quot;1958-03-05&quot;, &quot;1991-10-28&quot;)) # birth dates dob class(dob) today &lt;- Sys.Date() # current date today class(today) today + 90 # 90 days from today # Current age age &lt;- today - dob1 # difference as number of days age class(age) # a difftime object # Use as.numeric to get standard numeric vector age &lt;- as.numeric(today - dob1) age class(age) # To compute age in years we divide the NUMERIC version by 365.25 age_in_years &lt;- as.numeric(today - dob1) / 365.25 age_in_years # Use floor() to get whole years age_in_years &lt;- floor(as.numeric(today - dob1) / 365.25) age_in_years 2.6 Other data structures The data structures presented in previous sections are those you will be using again and again, so it’s really important you get familiar with them. In this section we briefly introduce a couple of additional data structures, because you may find them when looking at the help of some functions. These are matrices and arrays. A matrix is just a vector with a dim attribute defining the dimensions of the matrix, that is, the number of rows and columns. A matrix can be created with function matrix(), or by defining the dim attribute of a vector with dim(). Like vectors, matrices cannot mix data types (e.g., character and numeric). x &lt;- 1:10 # a vector x class(x) # Create a matrix of 2 rows and 5 cols m &lt;- matrix(x, nrow = 2) m dim(m) class(m) dim(x) # vectors do not have a dim attribute dim(x) &lt;- c(2, 5) # but we can define it x # and now its a matrix class(x) dim(x) Matrices are a special case of a more general data structure called array. In fact, a matrix is an array with two dimensions (rows and columns) but, in general, arrays can have multiple dimensions. Arrays are created with function array(), or by defining the dim attribute of a vector. x &lt;- 1:30 # Creating an three-dimensional array a &lt;- array(x, dim = c(3, 5, 2)) a class(a) dim(a) dim(x) # vectors do not have a dim attribute dim(x) &lt;- c(3, 5, 2) # but we can define it x # and now its an array class(x) dim(x) For more information on matrices and arrays see this section of the first edition of the book Advanced R by Hadley Wickham. Resources A very useful base R cheat sheet: a pdf with only two pages! Objects types and some useful R functions for beginners is a post in R-bloggers where you can find many of the concepts we covered in this chapter, and more. Fore a more thorough discussion of what we covered in this chapter, look at Data structures in the first edition of the book Advanced r by Hadley Wickham. Exercises Before undertaking each exercise, ensure your workspace is clean (i.e., there are no objects in the Environment pane). Create a vector w of body weights in kilograms, and a vector h of body heights in centimeters, with the following values corresponding to 10 patients: w: 47.3, 87.0, 80.9, 85.1, 95.4, 90.0, 70.3, 46.9, 86.7, 74.0 h: 160, 165, 180, 170, 190, 180, 172, 156, 170, 169 Then compute the body mass index (kg/m^2), store it as bmi, and write appropriate expressions to: Verify that bmi is a numeric vector. Get the number individuals with low weight (BMI below 18.5). Get the number individuals with overweight (BMI of 25 or more). Get the number individuals with obesity (BMI of 30 or more). Get the mean and standard deviation of bmi values. Hint: remember that the result of a comparison is a logical vector, and that the sum of a logical vector is the number of TRUEs it contains. LETTERS and lettersare built-in vectors containing all the letters in the alphabet (print them in the console to see them). What is length(letters)? Using these vectors, get the following: The letter in position 20. A character vector with the first five LETTERS only. A character vector whose first element is “A is an uppercase LETTER, and a is a lowercase letter” (and similar statements for the remaining letters). Hint: use the paste() function. A character vector whose first element is “A is in position 1 of the alphabet” (and similar statements for the remaining letters). Hint: use 1:length(LETTERS) in the paste() function. What is the result of 1:length(LETTERS)? The following are five CIP codes, whose structure is as follows: the first four characters are initials of surname and first name, the first digit is a code for sex (0 for male, 1 for female), followed by birth date in format yyddmm, and three trailing zeros: YAGU0741511000, DEFO0702702000, GITI1642102000, AOPU1620401000, BABE0740206000 Define vector cip containing these CIP codes and then: Get a vector containing the two digits corresponding to the year of birth. Is it a numeric vector? Get a vector of four-digit years of birth by adding 1900. Using only the year of birth (ignore month and day of birth), get an estimate of their ages by 2050. Get a vector with first element “YAGU was born in 1974” (and analogous for the remaining persons). See the help of the rep() function by running ?rep, and look at the first two examples. Use this function to create three vectors containing the following elements: 0, 1, 0, 1, 0, 1 0, 0, 0, 1, 1, 1 “male”, “female”, “male”, “female”, “male”, “female” rep(0:1, 3) rep(0:1, each = 3) rep(c(&quot;male&quot;, &quot;female&quot;), 3) See the help of the seq() function by running ?seq, and look at the examples. Use this function to create vectors containing: All multiples of five up to 100. All odd numbers between zero and ten. The following four probabilities: 0.2, 0.4, 0.6, 0.8. Run the following code to conduct a t-test on the lwb data in package COUNT (you need to install this package if you do not have it!): library(COUNT) data(lbw) res &lt;- t.test(bwt ~ smoke, data = lbw) res What is the class of object res? Is it a list? What are the names of the elements in res? Get the p-value of the test, with 3 decimals. Get the means of bwt for smokers and non-smokers, rounded to one decimal. Get their difference with function diff(), and round to one decimal. Get the confidence interval for this difference, rounded to one decimal. In one step, create a dataframe with these data: first_name age sex Peter 33 male Paul 29 male Mary 25 female Then extend this dataframe with the following additional variables: case_id containing numbers 1 to 3. age_in_10y reflecting the age they will have in 10 years from now. older_than_30 as TRUE or FALSE. is_male as TRUE of FALSE. Remove variable is_male from the dataframe What is now the length of the dataframe? and what is the number of rows? Using the rep() function, create numeric vector x with the sequence 1,2,3 repeated 3 times (so 9 values), an then define a factor from x with labels “good”, “neutral” and “bad”. Create a character vector x with two repetitions of the vector c(“never-smoker”, “ex-smoker”, “current-smoker”), and then define a factor from x. Is the ordering of levels good enough? Redefine the factor with a better ordering. Create two Date objects: one with today’s date, and the other with your date of birth. Then, use these objects to: Compute your age in days Compute your age in (whole) years Print your date of birth, including the weekday the full name of the month and the year with four digits When fixed = FALSE (the default value), this is interpreted as a regular expression; regular expressions are a powerful way to manipulate strings, but are out of the scope of this book.↩︎ This and other statistical tests will be introduced in future chapters. Our purpose here is to show how the results of statistical tests are organized, not their meaning.↩︎ "],["data-acquisition.html", "3 Data Acquisition 3.1 Reading MS Excel data 3.2 Reading text data 3.3 Reading SPSS, SAS or Stata data 3.4 Reading databases 3.5 Reading other formats 3.6 Getting data from R packages 3.7 Problems when importing data from external files Resources Exercises", " 3 Data Acquisition Data acquisition is the process of importing data into your R session, so that it can be viewed and analyzed. There are several ways in which you can get data, but the usual case is to read an external data file. Common data file formats are Excel, plain text, SPSS, SAS or Stata data files. It is very easy to read files in these file formats by clicking the Import Dataset button of the Evironment pane in RStudio; this will unfold a list of file formats, and you just need to select the appropriate one. Upon selection, a new window will show-up from which we can pick the data file and provide information on how to read it. This information is partially different depending on the format, and in the following sections we show how to proceed in each case. 3.1 Reading MS Excel data In the Import Dataset button of the Evironment pane of RStudio, select From Excel..., and you will be presented with a dialog like the one in figure 3.1. Figure 3.1: Dialog to read Excel files The File/URL box at the top of this dialog is where we should write the Excel file address. If the data file is located in you computer, you can pick it using the (top-right) Browse button. If the Excel file is somewhere in the internet, you can write (or paste) the URL in the box. As soon as you pick the file (or write its URL), you will see the result of a default reading in the Data Preview box. You should always look at the result of the reading in the Data Preview to ensure that you get what you want. In some cases, the default reading will be enough, but in some others you will realize that there is some problem. Then, you can use the Import options as appropriate: Name: to provide a name for the resulting dataframe; this is automatically filled but you can change it to any valid name. Sheet: to indicate the sheet you want to read (if there is more than one sheet in the Excel file). Range: to indicate the range of cells to read, in standard Excel syntax (e.g., B3:F51). Max rows: to indicate the maximum number of rows you want to read (may be handy for test-reading of very large files). Skip: to start reading data after skipping the indicated number of rows. NA: to define the character(s) used in the Excel file to represent missings. First row as names: check if the first row that will be read contains column names. Open data viewer check to open the resulting dataframe in a new tab at the top-left RStudio pane. Last, the Code preview box shows the R code that will be executed when you click the Import button. 3.2 Reading text data There are two entries in the list under the Import Dataset for reading text data: From Text (base) and From Text (readr). We recommend the later, which opens the dialog of figure 3.2. Figure 3.2: Dialog to read text files Some of the Import options are similar to those commented in the previous section for Excel files (Name, Skip and First Row as Names, NA, Open Data Viewer), but the following are specific of text data files: Trim spaces: to trim leading and trailing white spaces for each data point Delimiter: to define what is used to separate data points Quotes: to indicate the type of quotes used for texts (if any) Locale: to indicate the configuration of locale settings, such as the decimal mark, the thousands separator, or the time zone. Escape: does the file use backslash to escape special characters? or does the file escape quotes by doubling them? Comment: to indicate the symbol used in the file for comments (that should be ignored) Before reading a text data file, it is very important to inspect how the data are arranged. This can be done by opening the file with any text editor, like the Notepad in Windows, or with RStudio (using the menu File &gt; Open file...). For instance, figure 3.3 shows a text data file containing variable names in the first row, where data are delimited by tabs, texts are unquoted, and the dot is used as the decimal separator. Figure 3.3: Text data file (delim: tab) As a second example, the text data file shown in figure 3.4 also contains variable names in the first row, but data are delimited by semicolons, texts are quoted (with double quotes), and uses the dot as the decimal separator. Figure 3.4: Text data file (delim: semicolon) 3.3 Reading SPSS, SAS or Stata data The dialog opened by options From SPSS, From SAS and From Stata are very similar. In figure 3.5 we show the former, where there is only one active Input Option refering to the input data file: Format: to specify the type of SPSS file (default: .SAV) Figure 3.5: Text data file (delim: semicolon) 3.4 Reading databases There are many different database management systems (DBMS) that are widely used, like MySQL, PostgreSQL, Microsoft Access and SQL Server, Oracle, or Sqlite. To read a database (DB) with R we need to establish a connection to the the DB, either using specific drivers provided by the BD vendor, or using open DB connectivity (ODBC), a way to access any SQL-based DBMS which is implemented in the R package RODBC In the following tutorial we show how to connect to a Microsoft Access DB, using the RODBC package. It is important to note that this will only work when your are using 32-bits R to read 32-bits DB drivers. After installing and loading the RODBC package, a connection is established to the Access DB with function odbcConnectAccess(). Then, the sqlTables() function is used to get a list of all tables in the DB. To get a specific table from the DB we use function sqlFetch(). It is important to close the connection when the job is done. library(RODBC) con &lt;- odbcConnectAccess(&quot;./data/demo.mdb&quot;) # establish connection to the DB sqlTables(con) # print table-like objects in the DB d &lt;- sqlFetch(con, &quot;demo&quot;) # gets the table demo d class(d) # the result is a dataframe close(con) # don`t forget to close the connection! rm(con) # before removing it!! 3.5 Reading other formats Besides the facilities described in previous sections to read common data file formats from RStudio, many other file formats can be read with R. These are some of them: PDF files can be read with package pdftools (see this example). HTML, XML: there are several packages to work with HTML and XML files, but package xml2 can be use to read both (see here). JSON files can be read with package rjson (see this tutorial). Web pages: the rvest package, is useful for webscraping (getting data from web pages) (see this tutorial). There are several packages that can be used to read/write and visualize geospatial data (maps) (see this CRAN Task View). There are several packages to read/write and visualize medical images in formats DICOM, NALYZE or NIfTI-1 (see this CRAN Task View for more info). Genetic data can be imported from a wide range of formats, including those of popular population genetics software (GENETIX, STRUCTURE, Fstat, Genepop) with package adegenet (see here for more info). This list is by no means comprehensive. If you are interested in reading a particular file format not covered here, it is very likely you will find useful info by Googleing “R (format_name)”. 3.6 Getting data from R packages Many R packages contain example data sets. In the base installation of R you will find thedatasets package (look for it in the Packages tab of RStudio, and click on the name of the package) that contains many example datasets. However, only a few are related to clinical research (e.g., the Indometh dataset). A (non-comprehensive) list of clinical datasets contained in R packages can be found here. To work with any of these datasets you need to install the corresponding package, and then look at the help of the dataset. For instance, the MASS package includes the birthwt dataset. The following script prints its first rows with head() (six rows are printed by default, but you could indicate a different number in a second argument): head(MASS::birthwt) # this will print the first 6 rows low age lwt race smoke ptl ht ui ftv bwt 85 0 19 182 2 0 0 0 1 0 2523 86 0 33 155 3 0 0 0 0 3 2551 87 0 20 105 1 1 0 0 0 1 2557 88 0 21 108 1 1 0 0 1 2 2594 89 0 18 107 1 1 0 0 1 0 2600 91 0 21 124 3 0 0 0 0 0 2622 For a brief description of the data, look at the help after loading the package by running ?birthwt. Since the beginning of the COVID-19 pandemics, several resources have been developed with R, including R packages (see a list here). Among them, the COVID19 package provides a unified dataset by collecting worldwide fine-grained case data that can be easily extended with World Bank Open Data, Google Mobility Reports, and Apple Mobility Reports (see this short tutorial). In addition, some R packages provide more or less easy access to many different databases of interest for clinical researchers, such as the following: rentrez is a package for retrieving data from the National Center for Biotechnology Information (NCBI), including PubMed and GenBank. rclinicaltrials provides an interface to ClinicalTrials.gov, a registry and results database of clinical studies conducted around the world. nhanesA allows retrieval of data from the National Health and Nutrition Examination Survey (NHANES) conducted by the National Center for Health Statistics (NCHS). WHO allows to download public health data from the World Health Organization’s Global Health Observatory. wbstats provides access to the World bank data, a comprehensive source of global socio-economic data. Some additional R packages to access open public health data are commented in this post. 3.7 Problems when importing data from external files It is very common to have problems when reading real-life data, especially when data have been entered manually performing no validation. This is often the case when data are entered in Excel spreadsheets (though Excel has some data validation tools, these are ignored by many users). Some of the most frequent problems are: Inadequate variable names Incorrect reading of numeric data Incorrect reading of dates In this section we show how to deal with these problems using the dplyrpackage, which is very convenient to work with dataframes. 3.7.1 Package dplyr Before proceeding, you should install and load the dplyrpackage: library(dplyr) The dplyr package provides several functions to perform common tasks, and function names are verbs describing the task they perform. Some of these functions are: select(): selects variables from a dataframe (and optionally renames them) filter(): filters rows of a dataframe according to some logical condition mutate(): creates new variables in a dataframe rename(): renames variables in a dataframe arrange(): changes the ordering of the rows of a dataframe The first argument to all these (and other) dplyr functions is a dataframe, and all of them provide a dataframe as a result. Each function takes other function-specific arguments. Table 3.1 shows some examples of use of these functions with a description of the result they produce. In these examples, we assume that d is a dataframe containing variables sex, age, race, weight (in kilograms) and height (in centimeters). Table 3.1: Example of dplyr function calls Function call Result: a dataframe with… select(d, sex, age, race) all rows in d, and variables sex, age, race. select(d, gender=sex, age, race) all rows in d, and variables gender, age, race. filter(d, sex == \"male\") males in d, and all variables. rename(d, gender = sex) all rows in d, and variables gender, age, race, weight, height. mutate(d, height_meters = height/100) all rows and variables in d, plus new variable height_meters arrange(d, age) all rows and variables in d, with rows sorted by To note in the examples of table 3.1: variable names are neither prefixed with the dataframe name, nor quoted. a double equal sign is used to specify equality conditions, as in filter(d, sex == \"male\"). a single equal sign (instead of the assignment operator &lt;-), is used to define new variables with mutate(), as in mutate(d, height_meters = height/100). When you load the dplyr package, you can use the so called pipe operator (%&gt;%). As you will see soon, this allows to chain operations in a very practical way. The basic usage of the pipe is shown in table 3.2: Table 3.2: Usage of the pipe with dplyr functions This is equivalent… to this select(d, sex, age, race) d %&gt;% select(sex, age, race) filter(d, sex == \"male\") d %&gt;% filter(sex==\"male\") mutate(d, height_meters = height/100) d %&gt;% mutate(height_meters = height/100) arrange(d, age) d %&gt;% arrange(age) This works because d is the first argument of select(), filter(), mutate() and arrange(). You can use this operator with any function (not just with functions from dplyr), provided that the object before %&gt;% is the first argument of the function after %&gt;%. Table 3.3 shows a basic usage of the pipe with base R functions (in the first example we assume d is some existing dataframe): Table 3.3: Usage of the pipe with base R functions This is equivalent… to this names(d) d %&gt;% names() round(2.38, 1) 2.38 %&gt;% round(1) log(100) 100 %&gt;% log() What is really powerful about the pipe operator is that you can chain as many operations as you wish, provided you stick to the rule: what goes before %&gt;% is the first argument of the function after %&gt;%. For instance, the following two expressions are equivalent: round(log(100), 4) # nested functions [1] 4.6052 100 %&gt;% log() %&gt;% round(4) # chained functions [1] 4.6052 Notice that it is much more difficult to write the first of these expression: while you think “compute the log of 100 and round the result to four decimals”, nesting functions requires you to work from the inside out. However, when chaining operations with the pipe you follow the natural order of operations: “take 100, then compute the log, then round to four decimals”. As you see, each pipe can be read as then, or more precisely, then do whatever the next function indicates, taking the previous result as its first argument. Chaining can be applied to dataframe operations as well, as in the following example: take d, then filter males, then compute the height in meters, and then select the two height variables. # creating an example dataframe d &lt;- data.frame(sex = rep(c(&quot;male&quot;, &quot;female&quot;), 3), age = c(19, 23, 45, 38, 57, 61), height = c(180, 165, 174, 175, 168, 177)) # chaning operations with the pipe (%&gt;%) d %&gt;% filter(sex == &quot;male&quot;) %&gt;% mutate(height_meters = height/100) %&gt;% select(height, height_meters) height height_meters 1 180 1.80 2 174 1.74 3 168 1.68 We have only scraped the surface of the dplyr package, and there is much more power in it for dataframe operations. But what we covered should be enough to start using it as we do in the following section. 3.7.2 Reading the SARA data To illustrate problems that appear quite often when reading a real dataset, we will use a simplified version of the data collected in the SARA trial. This was a randomized clinical trial that compared catheter ablation vs. antiarrhythmic drug treatment in patients with persistent atrial fibrillation. The data has been modified in several ways for didactic purposes, and therefore the results shown here may differ from those described in the paper. You can download this file from here. We first read the dataset, and inspect variable names: library(readxl) raw_data &lt;- read_excel(&quot;data/SARA_simplified.xlsx&quot;) names(raw_data) [1] &quot;Patient no&quot; &quot;Birth Date (yyyymm-dd)&quot; [3] &quot;Sex&quot; &quot;Date of randomization&quot; [5] &quot;Random allocation of Treatment&quot; &quot;Body Weight at baseline (kg)&quot; [7] &quot;Body Height at baseline (cm)&quot; &quot;NYHA classification at baseline&quot; [9] &quot;SBP at baseline (mmHg)&quot; &quot;DBP at baseline (mmHg)&quot; [11] &quot;Heart Rate at baseline (bpm)&quot; The variable names contain blank spaces and some special characters like brackets, and are too long. We better clean them using function clean_names() from the janitor package, and revise the result: library(janitor) d &lt;- clean_names(raw_data) # clean variable names names(d) # review [1] &quot;patient_no&quot; &quot;birth_date_yyyymm_dd&quot; [3] &quot;sex&quot; &quot;date_of_randomization&quot; [5] &quot;random_allocation_of_treatment&quot; &quot;body_weight_at_baseline_kg&quot; [7] &quot;body_height_at_baseline_cm&quot; &quot;nyha_classification_at_baseline&quot; [9] &quot;sbp_at_baseline_mm_hg&quot; &quot;dbp_at_baseline_mm_hg&quot; [11] &quot;heart_rate_at_baseline_bpm&quot; Note that spaces have been replaced with underscores, and special character like brackets have been eliminated. This function made a good job, and in many instances the result will be good enough to proceed. In this case however, variable names are too long, so we rename them using the rename() function of dplyr: d &lt;- rename(d, # rename vars patient = patient_no, birth_dt = birth_date_yyyymm_dd, rand_dt = date_of_randomization, group = random_allocation_of_treatment, weight = body_weight_at_baseline_kg, height = body_height_at_baseline_cm, nyha = nyha_classification_at_baseline, sbp = sbp_at_baseline_mm_hg, dbp = dbp_at_baseline_mm_hg, hr = heart_rate_at_baseline_bpm) names(d) # review [1] &quot;patient&quot; &quot;birth_dt&quot; &quot;sex&quot; &quot;rand_dt&quot; &quot;group&quot; &quot;weight&quot; [7] &quot;height&quot; &quot;nyha&quot; &quot;sbp&quot; &quot;dbp&quot; &quot;hr&quot; Much better. Now, let’s have a first look at the data by printing the first rows of the dataframe: head(d) # prints first six rows # A tibble: 6 × 11 patient birth_dt sex rand_dt group weight height nyha sbp &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 00:00:00 1 11/05/2009 Cathet… 104 188 1 110 2 1-002 1970-02-27 00:00:00 1 12/05/2009 Antiar… 92 182 1 NA 3 1-003 1964-02-21 00:00:00 2 11/06/2009 Cathet… 95 177 1 125 4 1-004 1962-01-04 00:00:00 2 20/07/2009 Cathet… 80 162 1 139 5 1-005 1974-06-02 00:00:00 1 17/08/2009 Antiar… 93 175 NA 122 6 1-006 1964-06-22 00:00:00 1 21/09/2009 Cathet… 115 185 1 120 # … with 2 more variables: dbp &lt;dbl&gt;, hr &lt;dbl&gt; In the previous output, the header of each data frame column includes the variable name and the type of data is indicated with abbreviations: &lt;chr&gt; for character, &lt;dttm&gt;for datetime, and &lt;dbl&gt; for double (which is a type of numeric). The following is to note: birth_dt is a datetime object, but time values are 00:00:00, and we want it to be a Date object anyway. rand_dt is character, but we want it to be Date as well. weight is character, but we expected a numeric. But let’s verify the object classes of these three variables: class(d$birth_dt) [1] &quot;POSIXct&quot; &quot;POSIXt&quot; class(d$rand_dt) [1] &quot;character&quot; class(d$weight) [1] &quot;character&quot; Yes, these objects classes are as indicated by the abbreviations in the output above. The problem with weightdeserves some investigation. When a variable containing digits is not recognized as numeric, it is likely due to some value(s) containing forbidden characters for numeric data. To verify this, we use mutate() to create a numeric version of weight with as.numeric(), and then filter the rows where the numeric version is missing but the original weight is not. Last, we select() the patient number and the two versions of weight, and we store the resulting dataframe as commas for future reference: commas &lt;- d %&gt;% mutate (weight_numeric = as.numeric(weight)) %&gt;% filter(is.na(weight_numeric) &amp; !is.na(weight)) %&gt;% select(patient, weight, weight_numeric) commas # A tibble: 9 × 3 patient weight weight_numeric &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1-034 67,2 NA 2 1-052 84,5 NA 3 1-053 99,5 NA 4 2-022 92,5 NA 5 6-002 85,5 NA 6 6-013 83,5 NA 7 7-003 85,5 NA 8 7-008 91,5 NA 9 7-014 92,5 NA There are nine patients with decimals in weight that could not be interpreted as numbers because the comma (instead of a dot) was used as decimal separator. Now we need to replace this commas with dots, and this can be done with function sub(). Once the decimal separator problem has been fixed, we can safely convert to numeric. Last, we filter patients in commas to verify the result. d %&gt;% mutate(weight_fixed = sub(pattern = &quot;,&quot;, replacement = &quot;.&quot;, x = weight, fixed=TRUE), weight_numeric = as.numeric(weight_fixed)) %&gt;% select(patient, weight, weight_fixed, weight_numeric) %&gt;% # filter patients filter(patient %in% commas$patient) # A tibble: 9 × 4 patient weight weight_fixed weight_numeric &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1-034 67,2 67.2 67.2 2 1-052 84,5 84.5 84.5 3 1-053 99,5 99.5 99.5 4 2-022 92,5 92.5 92.5 5 6-002 85,5 85.5 85.5 6 6-013 83,5 83.5 83.5 7 7-003 85,5 85.5 85.5 8 7-008 91,5 91.5 91.5 9 7-014 92,5 92.5 92.5 Note in the script above we have defined two new variables in the mutate() function call (weight_fixed and weight_numeric), separated by comma. Moreover, note that the second variable is defined from the first. This means that, in a single call of mutate(): we can define as many new variables as needed (comma separated), and new variables are immediately available once defined We are left with the problem of dates. This can be fixed with function as.Date(): d %&gt;% mutate(birth_dt = as.Date(birth_dt), rand_dt = as.Date(rand_dt, &quot;%d/%m/%Y&quot;)) # A tibble: 152 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 1 2009-05-11 Catheter… 104 188 1 110 70 2 1-002 1970-02-27 1 2009-05-12 Antiarrh… 92 182 1 NA NA 3 1-003 1964-02-21 2 2009-06-11 Catheter… 95 177 1 125 70 4 1-004 1962-01-04 2 2009-07-20 Catheter… 80 162 1 139 63 5 1-005 1974-06-02 1 2009-08-17 Antiarrh… 93 175 NA 122 74 6 1-006 1964-06-22 1 2009-09-21 Catheter… 115 185 1 120 80 7 1-007 1950-06-30 1 2009-10-01 Catheter… 89 177 1 126 102 8 1-008 1955-03-29 1 2009-10-06 Catheter… 77 178 1 NA NA 9 1-009 1941-07-25 2 2009-11-10 Antiarrh… 62 162 1 140 85 10 1-010 1955-04-22 1 2009-11-17 Antiarrh… 86 168 1 122 83 # … with 142 more rows, and 1 more variable: hr &lt;dbl&gt; Finally, we can perform all the operations we have done by chaining them with pipes, so that we do everything in a single code chunk. This is done in the following script. Note that in this case the problem with commas in weight has been fixed in a single step, and the result overwrites the weight variable, to avoid additional variables in the dataframe. library(readxl) library(janitor) library(dplyr) d &lt;- read_excel(&quot;data/SARA_simplified.xlsx&quot;) %&gt;% # reads data clean_names() %&gt;% # for well formed names rename(patient = patient_no, # for shorter names birth_dt = birth_date_yyyymm_dd, rand_dt = date_of_randomization, group = random_allocation_of_treatment, weight = body_weight_at_baseline_kg, height = body_height_at_baseline_cm, nyha = nyha_classification_at_baseline, sbp = sbp_at_baseline_mm_hg, dbp = dbp_at_baseline_mm_hg, hr = heart_rate_at_baseline_bpm) %&gt;% mutate(weight = as.numeric(sub(pattern = &quot;,&quot;, # commas to dots, replacement = &quot;.&quot;, # and convert to x = weight, # ...numeric fixed=TRUE)), birth_dt = as.Date(birth_dt), # convert to Date rand_dt = as.Date(rand_dt, &quot;%d/%m/%Y&quot;)) d # A tibble: 152 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 1 2009-05-11 Catheter… 104 188 1 110 70 2 1-002 1970-02-27 1 2009-05-12 Antiarrh… 92 182 1 NA NA 3 1-003 1964-02-21 2 2009-06-11 Catheter… 95 177 1 125 70 4 1-004 1962-01-04 2 2009-07-20 Catheter… 80 162 1 139 63 5 1-005 1974-06-02 1 2009-08-17 Antiarrh… 93 175 NA 122 74 6 1-006 1964-06-22 1 2009-09-21 Catheter… 115 185 1 120 80 7 1-007 1950-06-30 1 2009-10-01 Catheter… 89 177 1 126 102 8 1-008 1955-03-29 1 2009-10-06 Catheter… 77 178 1 NA NA 9 1-009 1941-07-25 2 2009-11-10 Antiarrh… 62 162 1 140 85 10 1-010 1955-04-22 1 2009-11-17 Antiarrh… 86 168 1 122 83 # … with 142 more rows, and 1 more variable: hr &lt;dbl&gt; And that’s it, at least by now! Resources A comprehensive tutorial on R data import (including some formats we have not mentioned) can be found here. Data Import Cheat Sheet For more information on the use of package janitor, see this tutorial. For more information on the use of package dplyr, see this tutorial. To easily read many different data formats with a single function, see this introduction to the rio package. Exercises Download this Excel file and read it using the Import Dataset button in the Environment pane of RStudio. How many variables are in the resulting dataframe, and what are their names? Get well formed names with function clean_names() of package janitor. Download this Excel file and read it using the Import Dataset button in the Environment pane of RStudio. How many variables are in the resulting dataframe, and what are their names? Give appropriate names to the dataframe variables. Read this text data file. Why do you think the Weight variable is read as character vector? Look at the character used for missings. Can you imagine a way to read this correctly, so that Weight is a numeric vector? (Hint:explore changing the NA Import Optionto NA, and look at the Code Preview; maybe if you copy this code to the editor you can adapt it to indicate that the NA symbol is…). Read this text file. Are the names of the countries properly written? Try to use the Locale Import Option to set the Encoding to “ISO-8859-1”. Download this SPSS file and read it using the Import Dataset button in the Environment pane of RStudio. What is the class of the Sex variable in the resulting demo dataframe?. Now run this code d &lt;- as_factor(demo). What is the class of Sex in d? Read the Nobel Price Laureates dataset with RStudio and list the variable names of the resulting laureate dataframe. Then, use janitor::clean_names() to get a dataframe with better names, and save it as d. With d, do the following: Look at the class of variables born and died. If not Date, convert this variables to standard dates. Nobel Laureates are awarded the Nobel prize in a ceremony held the 10 December every year, on the anniversary of Alfred Nobel’s death. Use paste() to build a character vector containing the dates the laureates were awarded the prize. Convert the previous result to a Date object and use it to compute the age each laureate was awarded the Nobel Prize. Filter laureates for the “medicine” category and save the resulting dataframe as medicine. Use count(medicine, born_country) to get the number of medicine laureates per country of birth; look at the help of count() to see how to get the result sorted by frequency. Use the same function to get the number of medicine laureates per name, and then filter institutions with more than one laureate. Filter medicine to see the laureates that, when awarded the prize, where working in a country different from their born_country. How many of them were in a different country? "],["data-preparation.html", "4 Data preparation 4.1 Steps in data preparation 4.2 Reading raw data 4.3 Reviewing data 4.4 Modifying data 4.5 Computing new variables 4.6 Selecting valid cases 4.7 Saving the R script Resources Exercises", " 4 Data preparation Data preparation is the process by which we generate tidy data from raw data. Raw data is the original version of the data as stored during collection, whatever its source, format, structure and state. It is very common that raw data need some tweaks to make them ready for statistical analysis. These tweaks may involve simplifying the data structure, providing appropriate labels for coded variables, detecting and amending errors, recovering missing data, or computing new variables from those available. Tidy data is what we get after these tweaks: a version of the data ready for analysis. For a set of data to be considered tidy it should have the following structure: Each variable is a column in a dataframe Each observation is a row in a dataframe Each type of observational unit is a dataframe For instance, suppose we conducted a study on 100 patients and collected data on demographic and anthopometric variables age, sex, and body height and weight. The observational unit is then the patient, and these data can be arranged in a dataframe, having columns for patient, age, sex, height and weight, so that all data for one patient fits in a single row (see table 4.1). Table 4.1: Demographic and anthropometic data patient age sex height weight 1 67 male 180 91 2 42 male 168 75 3 39 female 176 69 4 … … … … 5 … … … … Now consider data on vital signs such as pulse, systolic and diastolic blood pressure, taken in three visits for the same set of patients. In this case, the observational unit is not the patient, but the visit of a patient, and data should be stored in a dataframe having patient, visit, temperature, pulse, systolic and diastolic blood pressure as columns, so that all data of one visit (of a given patient) fits in one row (see table 4.2). Table 4.2: Vital signs patient visit pulse sbp dbp 1 1 77 120 85 1 2 85 145 90 1 3 69 110 65 2 1 70 120 85 2 2 80 135 85 2 3 … … … 3 1 … … … Last, consider data on adverse events collected during a clinical trial. For each event, some characteristics are recorded, such as the duration(days), severity, actions taken, and outcome. In this case, the observational unit is the adverse event, and the data should be stored in a dataframe with columns patient, event, start date and stop date, severity, actions taken, and outcome. Again in this case, all data for an event fits in a single row (see table 4.3. Table 4.3: Adverse events patient visit event duration actions outcome 1 1 Headache 1 none recovered 2 1 Nausea 2 none recovered 2 2 Vomiting 1 none recovered 2 3 Abdominal cramps 1 none recovered 3 1 Hip fracture 1 surgery recovered 3 2 … … … … 4 1 … … … … You will note that the dataframe of figure 4.1 will have as many rows as patients in the study, that is, one-hundred. However, the dataframe of figure 4.2 will have three rows per patient, therefore totaling three-hundred rows (assuming all patients were visited three times). Last, the number of rows in the dataframe of figure 4.3 cannot be known in advance, because a patient can experience no AE at all, one AE or many AE. It should be clear that the structure of these three dataframes is different, and that is why we need three dataframes to accommodate all the data. Trying to put all of it in a single dataframe would result in undesirable things, such as having different columns containing the same type of data (such as AE1, AE2, AE3, …), and structural missings (AE2, AE3, … will be missing for patients experiencing just one AE, but this missings do not reflect lack of data, and are only due to an inappropriate data structure). For the sake of simplicity, in this chapter we will be concerned with the case where all the available data can be stored in a single dataframe having the simplest possible structure, just like the one in figure 4.1. If this is the case, each row will correspond to a patient, and each column to a variable. This should be enough for many simple observational studies collecting a limited amount of data on a single time point. But even in this simple case, it is important to have a unique case identifier; this is always convenient, and it is critical when the data has to be split in different dataframes, to allow record linkage. In any case, it is good practice to create dataframes with the following additional characteristics: Variable names should be short but meaningful, and should contain neither spaces, nor non-standard characters (like slashes, or accents in Spanish words). Variable names may follow different styles, like CAPITALIZED, camelCase, or snake_case, but whatever the style, consistency is a plus. In these book we use the snake_case style. Categorical variables should have meaningful levels (e.g., “male” or “female” rather than 1 or 2, or “m”, or “f”), and should be factors rather than character vectors. Quantitative variables should have the units of measurement documented somewhere, though not necessarily in the variable name (in fact this is not recommended, to keep variable names as short as possible, and free of special characters such as brackets, slashes or Greek letters); the study protocol is a reasonable place to document variable units. A minimal information on the study design, or on how the data were collected, should be available somewhere (e.g., in the study protocol). It is very important that the data preparation process is traceable and reproducible. To ensure traceability and reproducibility, this process should be done programmatically rather than manually. Manual editions of the data are not reproducible (unless very well documented, which takes a lot of time and is prone to errors), and should be avoided. In this chapter we will see what are the most common tasks in data preparation and how to accomplish them following this principle by writing R scripts. 4.1 Steps in data preparation It is convenient to approach data preparation in a systematic way. The following is a reasonable order of the most common data preparations tasks: Reading raw data Reviewing data (looking for problems: missings and errors) Modifying data (to fix problems) Computing new variables Selecting valid cases Saving the R script performing steps 1 to 5 above All the data preparation tasks in steps 2 to 5 above, no matter how complex, can be accomplished using base R. However, some of them are easier using the dplyr package introduced in the previous chapter, and this is the approach we will follow, with few exceptions. 4.2 Reading raw data Reading external data files was addressed in the previous chapter, where we read the SARA data with the following script: library(readxl) library(janitor) library(dplyr) d &lt;- read_excel(&quot;data/SARA_simplified.xlsx&quot;) %&gt;% clean_names() %&gt;% rename(patient = patient_no, birth_dt = birth_date_yyyymm_dd, rand_dt = date_of_randomization, group = random_allocation_of_treatment, weight = body_weight_at_baseline_kg, height = body_height_at_baseline_cm, nyha = nyha_classification_at_baseline, sbp = sbp_at_baseline_mm_hg, dbp = dbp_at_baseline_mm_hg, hr = heart_rate_at_baseline_bpm) %&gt;% mutate(weight = as.numeric(sub(pattern = &quot;,&quot;, replacement = &quot;.&quot;, x = weight, fixed=TRUE)), birth_dt = as.Date(birth_dt), rand_dt = as.Date(rand_dt, &quot;%d/%m/%Y&quot;)) head(d) # A tibble: 6 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 1 2009-05-11 Catheter … 104 188 1 110 70 2 1-002 1970-02-27 1 2009-05-12 Antiarrhy… 92 182 1 NA NA 3 1-003 1964-02-21 2 2009-06-11 Catheter … 95 177 1 125 70 4 1-004 1962-01-04 2 2009-07-20 Catheter … 80 162 1 139 63 5 1-005 1974-06-02 1 2009-08-17 Antiarrhy… 93 175 NA 122 74 6 1-006 1964-06-22 1 2009-09-21 Catheter … 115 185 1 120 80 # … with 1 more variable: hr &lt;dbl&gt; We will use this d dataframe to illustrate the data preparation tasks covered in this chapter. 4.3 Reviewing data Data review is a must with any real data set. You cannot assume your data is free of errors. Rather, you should assume that data has errors unless you prove otherwise. The very first test of a dataset should be to verify that the unique case identifier is really unique, i.e., that all patients have a different value in this variable. The unique() function applied to a vector returns all distinct values with no repetitions (if any). Then, the length() of the resulting vector will be the number of distinct values, and this can be compared to the number of patients (rows) in the dataset: length(unique(d$patient)) # number of distinct values in d$patient [1] 152 length(unique(d$patient)) == nrow(d) # is it equal to the number of rows in d? [1] TRUE Now we are sure that there are no two patients with the same case identifier (otherwise the previous comparison would have produced a FALSE). 4.3.1 Missings Because missing data is a very common problem in real life datasets, we should always start by looking at the number of missings in the data. There are several functions for this purpose, but the simplest one is the base R function is.na(). In previous chapters we used this function on a vector, and summed the result to get the number of missings: sum(is.na(d$weight)) # missings in weight [1] 19 Interestingly, we can use is.na() with dataframes as well. In this case, the result will be a matrix of logical values, the columns (and rows) of which correspond to the columns (and rows) of the dataframe. The colSums() function applied to this matrix will result in the number of missings for each column in the original dataframe. By nesting both functions, a single line of code is enough to get the number of missings in each variable of a dataframe, as shown below (you can do it in two steps if you want to see the matrix produced by is.na(d)) colSums(is.na(d)) # missings in columns of d patient birth_dt sex rand_dt group weight height nyha 0 0 0 0 0 19 22 2 sbp dbp hr 14 14 0 Another option is provided by the plot_missing() function in package DataExplorer, showing the number (and percentage) of missings for each variable in a graphic: DataExplorer::plot_missing(d) 4.3.2 Data errors Errors in the data are very common, and we should attempt to detect at least gross errors before proceeding with the analysis. Error detection may be easy or very difficult depending on the type of variable and the type of error. Gross errors in numeric variables and dates can be easily detected by looking at their extreme values (minimum and maximum). These (among other statistics) are provided by summary() for each numeric variable in a dataframe: summary(d) patient birth_dt sex rand_dt Length:152 Min. :1939-06-24 Min. : 1.000 Min. :2009-05-07 Class :character 1st Qu.:1949-11-09 1st Qu.: 1.000 1st Qu.:2010-01-17 Mode :character Median :1954-03-30 Median : 1.000 Median :2010-08-25 Mean :1955-11-06 Mean : 1.362 Mean :2010-08-27 3rd Qu.:1962-01-01 3rd Qu.: 1.000 3rd Qu.:2011-03-22 Max. :1983-06-22 Max. :22.000 Max. :2011-11-24 group weight height nyha Length:152 Min. : 50.00 Min. : 83.0 Min. :1.00 Class :character 1st Qu.: 74.00 1st Qu.:165.0 1st Qu.:1.00 Mode :character Median : 84.50 Median :174.0 Median :1.00 Mean : 85.75 Mean :172.0 Mean :1.26 3rd Qu.: 95.00 3rd Qu.:179.8 3rd Qu.:1.00 Max. :167.00 Max. :193.0 Max. :3.00 NA&#39;s :19 NA&#39;s :22 NA&#39;s :2 sbp dbp hr Min. :100.0 Min. : 60.00 Min. : 4.00 1st Qu.:119.2 1st Qu.: 70.00 1st Qu.: 57.00 Median :127.0 Median : 80.00 Median : 66.00 Mean :127.4 Mean : 80.42 Mean : 71.14 3rd Qu.:137.0 3rd Qu.: 87.00 3rd Qu.: 82.00 Max. :197.0 Max. :110.00 Max. :150.00 NA&#39;s :14 NA&#39;s :14 In the previous output we see the maximum value of sex is 22, which is an obvious error. There is also a suspicious maximum of 167 (kg) in weight, a very suspicious minimum of 83 (cm) in height, and an impossible minimum heart rate (hr) of 4 (bpm). However, the previous output is not useful for the group variable, and this is because it’s a character vector. The result would be useful if group was a factor (we would then see each possible value and its frequency), but we have not defined factors yet. Note that summary() also provides information on the number of missings (NA's). We can inspect the values of a categorical variable stored in a character vector using unique(). This will print all distinct values appearing in the vector, without repetitions: unique(d$group) [1] &quot;Catheter ablation&quot; &quot;Antiarrhythmic drug treatment&quot; unique(d$sex) # useful for coded categorical variables as well [1] 1 2 22 unique(d$nyha) [1] 1 NA 2 3 Sometimes it is worth looking at a graphic combining two variables whose values are related. For instance, we may look at the join distribution of weightand height as done in figure 4.1 (these and other graphics will be presented in detail in the next chapter). Figure 4.1: Weight and height By looking at this figure we confirm the weight value 167 kg is an error, since it corresponds to the patient with the minimum height value of 83 cm. This error is likely due to a permutation of heigh and weight values for this patient, which is a common data entry error. Interestingly, there is another patient showing an unusual combination of weight (125 kg) and height (about 160 cm). Though these values have nothing strange when we consider them separately, their combination is unlikely. For this reason, this error was not detected when we looked at these variables separately by inspecting their extreme values, but are easily detected in the plot above. Once we have detected problematic values in our data, we need to investigate what are the patients affected by these errors. # Patients with errors d %&gt;% filter(sex == 22) %&gt;% select(patient, sex) # A tibble: 1 × 2 patient sex &lt;chr&gt; &lt;dbl&gt; 1 3-001 22 d %&gt;% filter(hr &lt; 40) %&gt;% select(patient, hr) # A tibble: 1 × 2 patient hr &lt;chr&gt; &lt;dbl&gt; 1 6-030 4 d %&gt;% filter(weight &gt; 124) %&gt;% select(patient, weight, height) # A tibble: 2 × 3 patient weight height &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4-001 167 83 2 5-007 125 158 # Patients with missings d %&gt;% filter (is.na(weight) | is.na(height) | is.na(nyha)| is.na(sbp) | is.na(dbp)) %&gt;% select(patient, weight: dbp ) # A tibble: 30 × 6 patient weight height nyha sbp dbp &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-002 92 182 1 NA NA 2 1-005 93 175 NA 122 74 3 1-008 77 178 1 NA NA 4 2-002 93 180 1 NA NA 5 2-003 93 179 1 NA NA 6 2-005 95 NA 2 120 70 7 2-016 65 158 1 NA NA 8 2-017 120 192 1 NA NA 9 3-004 70 175 NA 130 70 10 5-005 NA NA 1 135 75 # … with 20 more rows Now we should appeal to source documents (such as the clinical records) and try to recover the missing or erroneous data for these patients. 4.4 Modifying data Suppose we have recovered the following data from clinical records, and the remaining missing data could not be recovered: patient variable correct_value 1-005 nyha 1 3-004 nyha 1 3-001 sex 1 4-001 weight 83 4-001 height 167 6-030 hr NA To set these data points to their correct values it is very practical to use vector subsetting. When we subset a vector by a condition which is either TRUE or FALSE for each patient, the returned values are those for whom the condition is TRUE. Thus, if we subset a variable using a condition that identifies a single patient, we refer to the value of this variable for this patient. Note that we need to prefix the variable names with the dataframe names, since this is base R (not dplyr): d$sex[d$patient == &quot;3-001&quot;] # sex for patient 3-001 [1] 22 Then, we can use this to set a new value for this data point: d$sex[d$patient == &quot;3-001&quot;] &lt;- 1 # sex for patient 3-001 d$sex[d$patient == &quot;3-001&quot;] # verify [1] 1 In this manner, we can set all the remaining correct values: d$nyha[d$patient == &quot;1-005&quot;] &lt;- 1 d$nyha[d$patient == &quot;3-004&quot;] &lt;- 1 d$weight[d$patient == &quot;4-001&quot;] &lt;- 83 d$height[d$patient == &quot;4-001&quot;] &lt;- 167 d$hr[d$patient == &quot;6-030&quot;] &lt;- NA Finally, we can verify if everything went as expected by printing data for the patients we set new (correct) values. To filter these patients we use %in% operator, so that d rows will be filtered if the patient is one of those specified in the character vector after %in%. d %&gt;% filter(patient %in% c(&quot;1-005&quot;, &quot;3-001&quot;, &quot;3-004&quot;, &quot;4-001&quot;, &quot;6-030&quot;)) %&gt;% select(patient, sex, weight, height, nyha, hr) # A tibble: 5 × 6 patient sex weight height nyha hr &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-005 1 93 175 1 70 2 3-001 1 101 187 1 75 3 3-004 1 70 175 1 60 4 4-001 1 83 167 1 51 5 6-030 1 NA NA 1 NA It seems that everything is fine now! 4.5 Computing new variables Once all data problems are fixed, the next step is to define factors for all categorical variables, and to compute new variables derived from those in the data. Computing new variables may involve different type of operations, such as using computational formulas, making conditional assignments, categorizing a quantitative variable, recoding categorical variables or manipulating character strings. All these cases are very common, and are illustrated in the following sections. 4.5.1 Defining factors Dataframe d includes the following categorical variables: sex is a categorical variable coded as 1 (for males), or 2 (for females). nyha is a categorical variable coded with numbers 1 to 3 to represent NYHA classes I, II and III respectively. group is a a categorical variable stored as a character vector, with self-explanatory values but a bit too long to be practical (for example, when producing graphics, long strings are usually a problem); we can take the chance of defining a factor to shorten the descriptors as “CA” for Catheter ablation and “ADT” for Antiarrhythmic drug treatment. The following script defines factors for these three variables and overrides3 d with the result. d &lt;- d %&gt;% mutate(sex = factor(sex, levels = 1:2, labels = c(&quot;male&quot;, &quot;female&quot;)), nyha = factor(nyha, levels = 1:3, labels = c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;)), group = factor(group, levels = c(&quot;Catheter ablation&quot;, &quot;Antiarrhythmic drug treatment&quot;), labels = c(&quot;CA&quot;, &quot;ADT&quot;))) head(d) # A tibble: 6 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;fct&gt; &lt;date&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 male 2009-05-11 CA 104 188 I 110 70 2 1-002 1970-02-27 male 2009-05-12 ADT 92 182 I NA NA 3 1-003 1964-02-21 female 2009-06-11 CA 95 177 I 125 70 4 1-004 1962-01-04 female 2009-07-20 CA 80 162 I 139 63 5 1-005 1974-06-02 male 2009-08-17 ADT 93 175 I 122 74 6 1-006 1964-06-22 male 2009-09-21 CA 115 185 I 120 80 # … with 1 more variable: hr &lt;dbl&gt; 4.5.2 Formulas Many derived variables are obtained using a computational formula involving arithmetic operations, mathematical functions, or both. For instance, the age of patients at randomization can be computed from the birth and randomization dates; the body mas index is computed from the body height and weight. These computations are easily implemented via mutate(), as in the following script, where we finish by selecting relevant variables to verify the result of the computations: d %&gt;% mutate(age = floor(as.numeric(rand_dt - birth_dt)/365.25), bmi = round(weight / (height/100)^2,1)) %&gt;% select(patient, rand_dt, birth_dt, age, weight, height, bmi) # A tibble: 152 × 7 patient rand_dt birth_dt age weight height bmi &lt;chr&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 2009-05-11 1974-11-15 34 104 188 29.4 2 1-002 2009-05-12 1970-02-27 39 92 182 27.8 3 1-003 2009-06-11 1964-02-21 45 95 177 30.3 4 1-004 2009-07-20 1962-01-04 47 80 162 30.5 5 1-005 2009-08-17 1974-06-02 35 93 175 30.4 6 1-006 2009-09-21 1964-06-22 45 115 185 33.6 7 1-007 2009-10-01 1950-06-30 59 89 177 28.4 8 1-008 2009-10-06 1955-03-29 54 77 178 24.3 9 1-009 2009-11-10 1941-07-25 68 62 162 23.6 10 1-010 2009-11-17 1955-04-22 54 86 168 30.5 # … with 142 more rows We computed the age of patients as the difference of the randomization and birth dates (which results in a number of days) divided by 365.25 to take into account leap years, and then used floor() to round down so as to get completed years. The body weigh (kg/m^2) is defined as weight in kilograms over the square of height in meters. Because height is expressed in centimeters, we divided by 100 to convert it to meters before squaring. 4.5.3 Conditional assignments Computational formulas are applied exactly the same way for all rows in a dataframe. However, in some instances we want to assign different values to a new variable depending on a condition. For instance, we may want to create an indicator of obesity, which is defined as a body mass index of 30 kg/m^2 or more. Thus, a new variable obesity should take the value “no” for patients having bmi &lt; 30, or “yes” otherwise. This is called a conditional assignment, and can be done with function iflese(), which takes three arguments specified in this order: a condition that can be evaluated as either TRUE or FALSE, the value we want to assign when the condition is TRUE, and the value to be assigned when the condition is FALSE. This is done in the following script after computing bmi: d %&gt;% mutate(bmi = round(weight / (height/100)^2,1), obesity = ifelse(bmi &lt; 30, &quot;no&quot;, &quot;yes&quot;)) %&gt;% select(patient, bmi, obesity) # A tibble: 152 × 3 patient bmi obesity &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1-001 29.4 no 2 1-002 27.8 no 3 1-003 30.3 yes 4 1-004 30.5 yes 5 1-005 30.4 yes 6 1-006 33.6 yes 7 1-007 28.4 no 8 1-008 24.3 no 9 1-009 23.6 no 10 1-010 30.5 yes # … with 142 more rows For more complex conditional assignments, several iflese()functions may be nested as in the following example: d %&gt;% mutate(bmi = round(weight / (height/100)^2,1), nutritional_status = ifelse(bmi &lt; 18.5, &quot;underweight&quot;, ifelse(bmi &lt; 25, &quot;normal weight&quot;, ifelse(bmi &lt;30, &quot;overweight&quot;, &quot;obesity&quot;)))) %&gt;% select(patient, bmi, nutritional_status) # A tibble: 152 × 3 patient bmi nutritional_status &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1-001 29.4 overweight 2 1-002 27.8 overweight 3 1-003 30.3 obesity 4 1-004 30.5 obesity 5 1-005 30.4 obesity 6 1-006 33.6 obesity 7 1-007 28.4 overweight 8 1-008 24.3 normal weight 9 1-009 23.6 normal weight 10 1-010 30.5 obesity # … with 142 more rows Nesting ifelse() functions is very flexible, and there is no limit to the number of nested levels, but it becomes difficult to follow if more than two or three nesting levels are necessary. If the condition depend on a single quantitative variable (as in this case, bmi) it is easier to use a different function allowing to define cutpoints, which is presented in the next section. 4.5.4 Categorization of quantitative variables A common type of derived variable is what results from the categorization of a numeric variable according to one or more cutpoints. For instance, suppose we want to create age groups by decades, i.e., group patients in bins defined by cutpoints 20, 30, 40, … and so on. This can be easily done using function cut(), that takes two arguments: the numeric variable we want to categorize, and the breaks or cutpoints we want to use, passed as a numeric vector such as c(20, 30, 40, ...). When the cutpoints are equidistant, it is practical to create this numeric vector with seq(), as done in the following script: d %&gt;% mutate(age = floor(as.numeric(rand_dt - birth_dt)/365.25), age_group = cut(age, breaks = seq(20, 70, 10))) %&gt;% select(patient, age, age_group) -&gt; foo foo # A tibble: 152 × 3 patient age age_group &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; 1 1-001 34 (30,40] 2 1-002 39 (30,40] 3 1-003 45 (40,50] 4 1-004 47 (40,50] 5 1-005 35 (30,40] 6 1-006 45 (40,50] 7 1-007 59 (50,60] 8 1-008 54 (50,60] 9 1-009 68 (60,70] 10 1-010 54 (50,60] # … with 142 more rows The result is a factor with the following levels: levels(foo$age_group) [1] &quot;(20,30]&quot; &quot;(30,40]&quot; &quot;(40,50]&quot; &quot;(50,60]&quot; &quot;(60,70]&quot; By default, intervals are defined from cutpoints as left-open and right-closed, using standard symbols ( and ] respectively. This means that the lower bound is not included in the interval, and the upper bound is included. For instance, a patient 40 years old is included in interval (30, 40] , and excluded from interval (40, 50]). Sometimes we may need left-closed and right-open intervals instead. For instance, the WHO nutritional status defines intervals in this way (note there is an error in the definition of the interval for Obesity class III, which is defined as Above 40; a patient with a BMI of exactly 40 does not fit in any of the classes! Therefore, the last class should be defined as 40 or more). We can produce this classification with cut(), using the argument right = FALSE, as done below. Note the use of count() to get the number of cases in each of the who_nsintervals. d %&gt;% mutate(bmi = round(weight / (height/100)^2,1), who_ns = cut(bmi, breaks = c(0, 18.5, 25, 30, 35, 40, Inf), right = FALSE)) %&gt;% count(who_ns) # A tibble: 6 × 2 who_ns n &lt;fct&gt; &lt;int&gt; 1 [18.5,25) 25 2 [25,30) 61 3 [30,35) 38 4 [35,40) 5 5 [40,Inf) 1 6 &lt;NA&gt; 22 We could also define custom labels for the resulting intervals as in the following: d %&gt;% mutate(bmi = round(weight / (height/100)^2,1), who_ns = cut(bmi, breaks = c(0, 18.5, 25, 30, 35, 40, Inf), labels = c(&quot;Underweight&quot;, &quot;Normal weight&quot;, &quot;Overweight&quot;, &quot;Obesity class I&quot;, &quot;Obesity class II&quot;, &quot;Obesity class III&quot;), right = FALSE)) %&gt;% count(who_ns) # A tibble: 6 × 2 who_ns n &lt;fct&gt; &lt;int&gt; 1 Normal weight 25 2 Overweight 61 3 Obesity class I 38 4 Obesity class II 5 5 Obesity class III 1 6 &lt;NA&gt; 22 4.5.5 Grouping factor levels Sometimes we want to re-classify observations by pooling some of the levels of a factor. For instance, suppose we want a simpler classification of the nutritional status with a single obesity class obtained by merging the three obesity classes I, II and III. This can be done in with function recode() from the dplyr package as shown below. Note that we only need to specify the levels to recode, plus .default = levels(who_ns) to keep all other levels unchanged. d %&gt;% mutate(bmi = round(weight / (height/100)^2,1), who_ns = cut(bmi, breaks = c(0, 18.5, 25, 30, 35, 40, Inf), labels = c(&quot;Underweight&quot;, &quot;Normal weight&quot;, &quot;Overweight&quot;, &quot;Obesity class I&quot;, &quot;Obesity class II&quot;, &quot;Obesity class III&quot;), right = FALSE), simpler_ns = recode(who_ns, &quot;Obesity class I&quot; = &quot;Obesity&quot;, &quot;Obesity class II&quot; = &quot;Obesity&quot;, &quot;Obesity class III&quot; = &quot;Obesity&quot;, .default = levels(who_ns))) %&gt;% count(simpler_ns) # A tibble: 4 × 2 simpler_ns n &lt;fct&gt; &lt;int&gt; 1 Normal weight 25 2 Overweight 61 3 Obesity 44 4 &lt;NA&gt; 22 4.5.6 Character strings We sometimes need to use strings stored in a character variable to derive a new variable. For instance, variable patient is a character vector containing a code for the study center (hospital), a hyphen, and a patient number within the center. Suppose we want to have a variable with the center code. This can be done with substr(). Similarly, we could extract the number of patient in each center. d %&gt;% mutate(site = substr(patient, 1, 1), site_patient = as.numeric(substr(patient, 3, 5))) %&gt;% select(patient, site, site_patient) # A tibble: 152 × 3 patient site site_patient &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1-001 1 1 2 1-002 1 2 3 1-003 1 3 4 1-004 1 4 5 1-005 1 5 6 1-006 1 6 7 1-007 1 7 8 1-008 1 8 9 1-009 1 9 10 1-010 1 10 # … with 142 more rows Function substr() always returns a character vector, as is the case of site above. However, we can easily get a numeric vector by wrapping substr() within as.numeric(), as we did for site_patient. Another common need is exactly the opposite of what we did: to combine two variables to form a new one. As an example, we recompose the patient variable from the two pieces obtained above, using paste(): d %&gt;% mutate(site = substr(patient, 1, 1), site_patient = substr(patient, 3, 5), recompose_patient = paste(site, site_patient, sep = &quot;-&quot;)) %&gt;% select(patient, site, site_patient, recompose_patient) # A tibble: 152 × 4 patient site site_patient recompose_patient &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1-001 1 001 1-001 2 1-002 1 002 1-002 3 1-003 1 003 1-003 4 1-004 1 004 1-004 5 1-005 1 005 1-005 6 1-006 1 006 1-006 7 1-007 1 007 1-007 8 1-008 1 008 1-008 9 1-009 1 009 1-009 10 1-010 1 010 1-010 # … with 142 more rows Working with strings is always difficult, and we often face problems we did not cover in the previous examples. However, you should be aware that there is much more power in R for working with strings, including detection and substitution of complex patterns, fuzzy string matching, and translations from, or to other languages. In the resources section we provide some links which are relevant for working with strings. 4.6 Selecting valid cases In virtually all real studies, some of the patients included in the study database are not valid for analysis. Common causes for this are inappropriate enrollment of patients that do not meet all predefined selection criteria, and lack of critical data. Then, we need to get rid of these patients, and keep only those who are valid. Validity criteria are always functions of the variables in our data, so that we should be able to write an expression to retain valid patients only. For instance suppose we want to declare invalid only those patients having a missing in hr. The result is a dataframe with 151 rows, after removing the only case with missing hr, as shown below: d %&gt;% filter(!is.na(hr)) %&gt;% nrow() [1] 151 Note the use of the not operator ! in front of is.na() in the filter() function. This operator reverses the meaning of what follows. Therefore, if is.na(hr) means hr is missing, !is.na(hr) means hr is NOT missing. Sometimes you will need to drop all cases having a missing somewhere, so as to keep only cases with complete data. This is easily achieved with function na.omit(). d %&gt;% na.omit() -&gt; complete nrow(complete) [1] 124 colSums(is.na(complete)) patient birth_dt sex rand_dt group weight height nyha 0 0 0 0 0 0 0 0 sbp dbp hr 0 0 0 In a more realistic case, you may need to investigate different aspects of the data, including compliance with all selection criteria (collected in several variables) and availability of important variables. This may take deriving a new variable that summarizes several validity criteria (such as valid: yes or no), and then use this variable for selection of valid patients. In the SARA study, only patients that did not comply with the study selection criteria were declared invalid and excluded form all analyses. These patients were: 1-013, 1-038, 1-054, 2-012, 5-002, and 6-021. In the following script we define a vector invalid, and then use the %in% operator to identify rows in d where patient is one of those in invalid: invalid &lt;- c(&quot;1-013&quot;, &quot;1-038&quot;, &quot;1-054&quot;, &quot;2-012&quot;, &quot;5-002&quot;, &quot;6-021&quot;) d %&gt;% filter(patient %in% invalid) # A tibble: 6 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;fct&gt; &lt;date&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-013 1955-01-01 male 2010-01-22 ADT 100 176 II 143 100 2 1-038 1953-05-03 male 2011-03-14 CA 106 180 II 142 107 3 1-054 1943-05-16 female 2011-11-07 CA 67 162 I 130 84 4 2-012 1951-12-05 male 2010-06-25 CA 110 180 I 120 70 5 5-002 1946-03-12 male 2009-09-16 CA 85 179 I 133 87 6 6-021 1969-12-22 male 2011-01-31 ADT NA NA I 140 90 # … with 1 more variable: hr &lt;dbl&gt; To subset dretaining all the remaining patients, we just use the not operator in front of the same expression to get just the opposite result, i.e., patients not in the invalid vector. d %&gt;% filter(!(patient %in% invalid)) # A tibble: 146 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;fct&gt; &lt;date&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 male 2009-05-11 CA 104 188 I 110 70 2 1-002 1970-02-27 male 2009-05-12 ADT 92 182 I NA NA 3 1-003 1964-02-21 female 2009-06-11 CA 95 177 I 125 70 4 1-004 1962-01-04 female 2009-07-20 CA 80 162 I 139 63 5 1-005 1974-06-02 male 2009-08-17 ADT 93 175 I 122 74 6 1-006 1964-06-22 male 2009-09-21 CA 115 185 I 120 80 7 1-007 1950-06-30 male 2009-10-01 CA 89 177 I 126 102 8 1-008 1955-03-29 male 2009-10-06 CA 77 178 I NA NA 9 1-009 1941-07-25 female 2009-11-10 ADT 62 162 I 140 85 10 1-010 1955-04-22 male 2009-11-17 ADT 86 168 I 122 83 # … with 136 more rows, and 1 more variable: hr &lt;dbl&gt; 4.7 Saving the R script Saving the R script that performs all the needed data preparation tasks is critical if we want this process to be reproducible. This is why we should always save the data preparation script, rather than its result, the tidy dataframe. There is no need to save the tidy data if we can reproduce it at no cost4, and as we will see, there is a very easy way to run the data preparation script once it has been saved to a file. The following script accumulates what we have done in previous sections, from the reading of raw data to the selection of valid cases: # reading raw data, cleaning and renaming vars d &lt;- read_excel(&quot;data/SARA_simplified.xlsx&quot;) %&gt;% clean_names() %&gt;% rename(patient = patient_no, birth_dt = birth_date_yyyymm_dd, rand_dt = date_of_randomization, group = random_allocation_of_treatment, weight = body_weight_at_baseline_kg, height = body_height_at_baseline_cm, nyha = nyha_classification_at_baseline, sbp = sbp_at_baseline_mm_hg, dbp = dbp_at_baseline_mm_hg, hr = heart_rate_at_baseline_bpm) %&gt;% # fixing problems in weight (commas instead of points!), and dates mutate(weight = as.numeric(sub(pattern = &quot;,&quot;, replacement = &quot;.&quot;, x = weight, fixed=TRUE)), birth_dt = as.Date(birth_dt), rand_dt = as.Date(rand_dt, &quot;%d/%m/%Y&quot;)) # data changes (after verification in hospital records) d$sex[d$patient == &quot;3-001&quot;] &lt;- 1 d$nyha[d$patient == &quot;1-005&quot;] &lt;- 1 d$nyha[d$patient == &quot;3-004&quot;] &lt;- 1 d$weight[d$patient == &quot;4-001&quot;] &lt;- 83 d$height[d$patient == &quot;4-001&quot;] &lt;- 167 d$hr[d$patient == &quot;6-030&quot;] &lt;- NA # invalid patients invalid &lt;- c(&quot;1-013&quot;, &quot;1-038&quot;, &quot;1-054&quot;, &quot;2-012&quot;, &quot;5-002&quot;, &quot;6-021&quot;) # derived vars sara &lt;- d %&gt;% # define factors mutate(sex = factor(sex, levels = 1:2, labels = c(&quot;male&quot;, &quot;female&quot;)), nyha = factor(nyha, levels = 1:3, labels = c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;)), group = factor(group, levels = c(&quot;Catheter ablation&quot;, &quot;Antiarrhythmic drug treatment&quot;), labels = c(&quot;CA&quot;, &quot;ADT&quot;))) %&gt;% # define new variables mutate(age = floor(as.numeric(rand_dt - birth_dt)/365.25), age_group = cut(age, breaks = seq(20, 70, 10)), bmi = round(weight / (height/100)^2,1), obesity = factor(ifelse(bmi &lt; 30, &quot;no&quot;, &quot;yes&quot;)), who_ns = cut(bmi, breaks = c(0, 18.5, 25, 30, 35, 40, Inf), right = FALSE), simpler_ns = recode(who_ns, &quot;Obesity class I&quot; = &quot;Obesity&quot;, &quot;Obesity class II&quot; = &quot;Obesity&quot;, &quot;Obesity class III&quot; = &quot;Obesity&quot;, .default = levels(who_ns)), site = substr(patient, 1, 1)) %&gt;% filter(!(patient %in% invalid)) %&gt;% # sort by patient arrange(patient) %&gt;% # define order of variables in dataframe select(patient, site, birth_dt:group, age, age_group, height, weight, bmi, simpler_ns, everything(), -who_ns) rm(invalid, d) After filtering valid cases, we used arrange() to ensure that rows will be sorted by patient, and select() to sort the variables as desired. Finally, we removed intermediate objects we no longer need with rm(). In the select() statement, note some useful possibilities: colons to indicate groups of adjacent variables (as in birth_dt:group). everything() to indicate all the remaining, non-mentioned variables. negative sign preceding a variable to drop it (-who_ns). We now print the first rows of sara to verify the result: sara # A tibble: 146 × 17 patient site birth_dt sex rand_dt group age age_group height weight &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;fct&gt; &lt;date&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1 1974-11-15 male 2009-05-11 CA 34 (30,40] 188 104 2 1-002 1 1970-02-27 male 2009-05-12 ADT 39 (30,40] 182 92 3 1-003 1 1964-02-21 fema… 2009-06-11 CA 45 (40,50] 177 95 4 1-004 1 1962-01-04 fema… 2009-07-20 CA 47 (40,50] 162 80 5 1-005 1 1974-06-02 male 2009-08-17 ADT 35 (30,40] 175 93 6 1-006 1 1964-06-22 male 2009-09-21 CA 45 (40,50] 185 115 7 1-007 1 1950-06-30 male 2009-10-01 CA 59 (50,60] 177 89 8 1-008 1 1955-03-29 male 2009-10-06 CA 54 (50,60] 178 77 9 1-009 1 1941-07-25 fema… 2009-11-10 ADT 68 (60,70] 162 62 10 1-010 1 1955-04-22 male 2009-11-17 ADT 54 (50,60] 168 86 # … with 136 more rows, and 7 more variables: bmi &lt;dbl&gt;, simpler_ns &lt;fct&gt;, # nyha &lt;fct&gt;, sbp &lt;dbl&gt;, dbp &lt;dbl&gt;, hr &lt;dbl&gt;, obesity &lt;fct&gt; names(sara) # to see variable order [1] &quot;patient&quot; &quot;site&quot; &quot;birth_dt&quot; &quot;sex&quot; &quot;rand_dt&quot; [6] &quot;group&quot; &quot;age&quot; &quot;age_group&quot; &quot;height&quot; &quot;weight&quot; [11] &quot;bmi&quot; &quot;simpler_ns&quot; &quot;nyha&quot; &quot;sbp&quot; &quot;dbp&quot; [16] &quot;hr&quot; &quot;obesity&quot; The R script above should be saved to file, with an appropriate name such as “data_preparation.R”, in the same folder where the raw_data is located (e.g., the study folder). Once this is done, you can start a fresh new R session and run this script using function source() with the complete file name as argument (don’t forget the quotes and the .R file extension!). As long as your working directory is the study folder, this will run the script, and the sara dataframe will appear in your workspace (i.e., Environment pane): source(&quot;data_preparation.R&quot;) You are now ready for statistical analysis! Resources The forecats package provides useful functions to work with factors (and there is a Factors with forcats Cheat Sheet. The lubridate package provides useful functions to work with dates (and there is a Dates and Times Cheat Sheet. The stringr package provides useful functions to work with strings (and there is a Work with Strings Cheat Sheet. For exact matching of strings, see the help of the grep() function of base R. If you need fuzzy matching (approximate matching) of strings, see the help of the agrep() function of base for starters; if you want more, see packages fuzzywuzzyR; if you want to get mad, see package stringdist. The googleLanguageR package uses Google Cloud Translation API for language detection and translation of strings to a different language (e.g., Spanish to English). It comes at a price, though quite affordable if you don’t exceed a billion of characters per month (and free for the first 500000 characters). Exercises The following script generates a dataframe with a hospitalcode, and a within-hospital patient number. set.seed(1); d &lt;- data.frame(hospital = c(rep(1:5, each=20)), patient = rep(1:20, 5) - rpois(n = 100, 0.05), sex = sample(c(&quot;male&quot;, &quot;female&quot;), replace = TRUE, size = 100), age = floor(rnorm(100, mean=45, sd=15))) head(d) hospital patient sex age 1 1 1 female 35 2 1 2 female 45 3 1 3 female 31 4 1 4 male 47 5 1 5 male 35 6 1 6 male 71 Create a unique case identifier by combining hospital and patient into a single variable case_id. Is case_id really unique? If it’s not, investigate what is/are repeated case_id value(s), and what are their positions (rows) in the dataframe Fix the problem by appending “a” or “b” to the case_id, so that it is unique. Amend the following data errors: The true age of the 16 year old patient is 26. Patient 1 in hospital 1 is not a female, but a male. The top age for this study was 75 years, so that any patient older than 75 is invalid. Eliminate these patients from the tidy dataframe. How many patients are left? Read this data of an observational study on pre-hypertension (previous stage to arterial hypertension) conducted in relatives of patients with arterial hypertension (the units of quantitative variables are whole years for age, cm for height and abdominal circumference, kg for weight, and mmHg for SBP and DBP). Ensure that all variable have appropriate names in snake_case, and that the case identifier is unique. Investigate the number of missings in this dataset. What are the variables containing missings? How many missings are there in each variable? Look at the minimum and maximum values of numeric variables. Are there gross errors, like impossible or very suspicious values? Define factors for all categorical variables (see the coding here). Compute the following derived variables: age_group: defined as working age (15-64 years) or elderly (65 years or older). bmi: the body mass index (BMI) (kg/^2), rounded to 1 decimal. nutritional status: defined as underweight (BMI &lt; 18.5), normal weight (18.5–24.9), overweight (25.0–29.9), or obesity (30 or above). Keep only the patients having complete data (no missings). Reorder variables so that derived variables come immediately after those they depend on. Ensure rows are ordered by subject number. Save your R script to a file with the name “pre_hta_data_preparation.R”, start a fresh R session, and verify your working directory is the folder where you saved the file (check this by running getwd()). Without opening the script in the RStudio editor, run it with source(\"pre_hta_data_preparation.R\"). Answer these questions: How many patients are left? Are there any missings? What is the mean of bmi? How many patients are there in each nutritional status category? Overriding the dataframe you start with (d in this case) is not a good idea unless you have tested the code and are pretty sure it works well. For testing, use a different name, so that you do not loose d if something goes wrong.↩︎ In studies with lots of data and/or very complex data preparation, the R script may be very long and take too much time to be executed. In these cases, it is reasonable to save not only the data preparation script, but also the resulting tidy dataframe.↩︎ "],["exploratory-data-analysis.html", "5 Exploratory Data Analysis 5.1 Number and type of variables 5.2 Univariate description 5.3 Bivariate description 5.4 Adding infromation from additional variables 5.5 Graphic refinement Resources Exercises", " 5 Exploratory Data Analysis Data exploration and description are first steps in data analysis. Graphics are the best tool for exploring data, while summary statistics are best suited for description under space restrictions, as is the case of published papers. Of course, papers include graphics as well, but these are often restricted to the most relevant results. Exploratory graphics differ from publication ready graphics. Graphics for publication must have a title, appropriate labels in axes and legends, and must be stored in a high quality image, all of which takes time, and it is usually done only for a selection of a few graphics. Conversely, data exploration requires lots of graphics, and we do not care about nice labeling or super-high image quality. So, exploratory graphics are fast and dirty. There are several graphics functions in base R., such as plot(), hist(), boxplot(), and barplot(). There are also several packages specialized in graphics, such as ggplot2, plotly, lattice and ggformula. With few exceptions, in this chapter we use the latter because of its simple, consistent, user-friendly syntax, that makes it ideal for fast (though not so dirty!) graphics. Similarly, there are several functions in base R to compute summary statistics, but we will use functions from the mosaic package. You should install the ggformula and mosaic packages now (using the Install button in the Packages pane of RStudio), and then load them with library(). library(ggformula) library(mosaic) All the graphic functions in the ggformula package start with gf_ (e.g., gf_histogram()), so you will recognize them very easily. To illustrate the exploratory methods in this chapter, we use data from the Predimed trial that compared two Mediterranean Diets (MD) to a Control diet. The compareGroups package includes dataframe predimed containing a simplified version of the trial data. The following script loads this package, uses function data(predimed) to make this dataset available, redefines the levels of the group variable (to have shorter labels for graphics), and shows the first 6 lines of (a subset of columns of) the predimed dataset. For a brief description of this dataset run ?predimed. # Remember to install the compareGroups package first, # using the `Install` button in the Packages pane of RStudio library(compareGroups) data(predimed) d &lt;- predimed levels(d$group) &lt;- c(&quot;Control&quot;, &quot;MD+nuts&quot;, &quot;MD+VOO&quot;) head(d) group sex age smoke bmi waist wth htn diab hyperchol famhist 1 Control Male 58 Former 33.53 122 0.7530864 No No Yes No 2 Control Male 77 Current 31.05 119 0.7300614 Yes Yes No No 4 MD+VOO Female 72 Former 30.86 106 0.6543210 No Yes No Yes 5 MD+nuts Male 71 Former 27.68 118 0.6941177 Yes No Yes No 6 MD+VOO Female 79 Never 35.94 129 0.8062500 Yes No Yes No 8 Control Male 63 Former 41.66 143 0.8033708 Yes Yes Yes No hormo p14 toevent event 1 No 10 5.374401 Yes 2 No 10 6.097194 No 4 No 8 5.946612 No 5 No 8 2.907598 Yes 6 No 9 4.761123 No 8 &lt;NA&gt; 9 3.148528 Yes 5.1 Number and type of variables The descriptive methods presented in this chapter are classified according to the number and type of variables they deal with. Concerning the number of variables, statistical methods are usually characterized as univariate, bivariate, or multivariate, when they deal with one, two, or more than two variables, respectively. While univariate methods are used to summarize or display the distribution5 of a single variable, bivariate and multivariate methods deal with the relationship between variables. There are two main types of variables: Categorical variables: nominal, or ordinal Quantitative variables: continuous, or discrete Categorical variables are classifications, like gender (male or female) or the New York Heart Association (NYHA) Classification of heart failure (I: No symptoms, II: Mild symptoms, III: Marked limitation in activity due to symptoms, or IV: Severe limitations). When the classes or categories of a categorical variable have a natural ordering, as is the case of the NYHA classification, the variable is said to be ordinal. Otherwise, it is said to be nominal, and this is the case of gender or ethnicity. Categorical variables with only two categories (like sex) are sometimes called dichotomous, binary or indicator variables, while those with more than two categories are called polytomous variables. Quantitative variables express a quantity and are the result of a counting or measurement procedure. Examples of quantitative variables are the number of seizures experienced by patient in a period of time, or his/her body weight. If the variable can only take a finite number of values in a given interval, it is said to be discrete. This is the case of the number of seizures: given any interval of values (like 2 and 8), there is only a finite number of possible values between them (3, 4, 5, 6, or 7). Otherwise, the variable is said to be continuous (there are infinite possible values of weight between 60 and 61 Kg: e.g., 60.5, 60.05, etc.). 5.2 Univariate description Table 5.1 shows the most common summary statistics and graphics used for univariate description. All methods listed in this table are presented in this section. Table 5.1: Univariate description Variable Statistics Graphics Categorical frequencies (counts, proportions, percentages) barchart Quantitative mean, median, standard deviation, variance, interquartile range, extremes, quantiles histogram, density, boxplot 5.2.1 Categorical variable The tools for the univariate description of a categorical variable are frequency tables and barcharts. They both inform on the frequency distribution of the variable, that is, how frequent are the different values the variable can take. We can compute either absolute frequencies (counts), or relative frequencies (proportions or percentages). 5.2.1.1 Summary statistics The tally() function from the mosaic package can be used to produce frequency tables. Below we have used this function to get counts and percentages for the event variable of the d dataframe. tally( ~ event, data = d) # counts event No Yes 6072 252 tally( ~ event, data = d, format = &quot;percent&quot;) # percentages event No Yes 96.01518 3.98482 From these results we see that 252 subjects experienced a cardiovascular event during the study, which is roughly a 4 % of the total number of subjects. Note the tilde (~) preceding the variable name in the tally() function calls. This symbol is needed, and omitting it will produce an error. 5.2.1.2 Graphics A barchart is nothing but the graphical expression of a frequency table: for each value of the variable, we draw a bar extending from zero to the value indicated in a frequency axis. Bar charts are easily produced with functions gf_bar() and gf_percents() from the ggformula package. In the following script we save the plots produced by these functions as plot1 and plot2, and then use package patchwork to print them at once, side by side. library(ggformula) plot1 &lt;- gf_bar( ~ event, data = d) # counts plot2 &lt;- gf_percents(~ event, data = d) # percentages library(patchwork) plot1 + plot2 # print both in a row Figure 5.1: Barcharts As you can see in figure 5.1, barcharts of absolute or relative frequencies display exactly the same image, but for the scale shown in the vertical axis (counts or percentages). This implies we can use either to get an idea of how common are the different values of the variable. 5.2.2 Quantitative variable 5.2.2.1 Sumary statistics The summary statistics used to describe the distribution of a quantitative variable can be classified in three types: centrality measures, spread or variability measures, and position measures: Centrality measures provide a typical or “central” value, informing on where the distribution is centered; the most common ones are the mean and the median (see table 5.2). Spread measures inform on the variability of values, that is, how scattered the values are around the center of the distribution; the most common ones are the variance, the standard deviation (SD), and the interquartile range (IQR)(see table 5.3). Position measures inform on the relative position of specific values in the distribution. There are two types of position measures: extremes (minimum, maximum) and quantiles(quartiles, quintiles, deciles, percentiles)(see table 5.4). In tables 5.2 to 5.4 we provide the definitions of the statistics listed above. In these tables we denote \\(x_i\\) the \\(i-th\\) value of variable X in a collection of \\(n\\) observations (so that \\(i = 1, 2, ... n\\)), and \\(\\sum\\) denotes summation over \\(i\\): Table 5.2: Centrality measures Name Definition: symbol = formula R function mean Sum of all values divided by the number of values: \\(\\bar{x} = \\frac{\\sum x_i}{n}\\) mean() median Value that occupies the central place in the ordered collection of the \\(n\\) values, thus dividing it in two parts having the same number of values. If \\(n\\) is even, there is no single central place, but two; in this case, these two central values are averaged. median() Table 5.3: Variability measures Name Definition: symbol = formula R function Variance Sum of the squared deviations from the mean, divided the number of values: \\(\\sigma^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n}\\). For estimation purposes (i.e., when the collection of values is a sample rather than a population) we use \\(s^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n-1}\\) var(x) Standard deviation (SD) Square root of the variance: \\(\\sigma = \\sqrt{\\sigma^2}\\), or \\(s = \\sqrt{s^2}\\) sd(x) Interquartile range (IQR) Difference between the third and the first quartiles (see position measures): \\(IQR = Q_3 -Q_1\\). IQR(x) Table 5.4: Position measures Name Definition: symbol = formula R function Extremes: minimum and maximum Lowest and highest value in the collection, respectively min(x) max(x) Quantiles Values that occupy particular positions in the ordered collection of values, dividing the collection in a number of parts having the same number of values. They are named according to the number of parts they generate: Quartiles Divide the distribution in four parts: \\(Q_1, Q_2, Q_3\\) quantile(x) Quintiles Divide the distribution in five parts: \\(Q_1, Q_2, Q_3\\) quantile(x, probs = c(.2, .4, .6, .8)) Deciles Divide the distribution in ten parts: \\(D_1, D_2,…, D_9\\) quantile(x, probs = seq(.1, .9, .1)) Percentiles Divide the distribution in 100 parts: \\(P_1, P_2,…, P_{99}\\) quantile(x, probs = seq(.1, .99, .1)) A couple of comments on quantiles: Note that we need K-1 cuts to divide the distribution in K parts; therefore, there are three quartiles, four quintiles, nine deciles, and ninety-nine percentiles. It should be clear that: the median is the same as the second quartile and the percentile 50 (median = \\(Q_2 = P_{50}\\)) the first quartile is the same as percentile 25 (\\(Q_1 = P_{25}\\)), and the third quartile is the same as percentile 75 \\(Q_3 = P_{75}\\). The R functions listed in tables 5.2 to 5.4 are useful when we want to compute just one of these statistics. However, there are other functions providing several statistics at once. This is the case of function favstats() from package mosaic, which is used in the script below to get summary statistics for the age variable (as you see, we use the pipe to round() to the first decimal the results obtained with favstats()). library(mosaic) favstats( ~ age, data=d) %&gt;% round(1) min Q1 median Q3 max mean sd n missing 49 62 67 72 87 67 6.2 6324 0 In the resuls above we see that the age of subjects ranged from 49 to 87 years, with mean and median values of 67 years. The fact that they match is an indication that the distribution is pretty symmetric (in asymmetric distributions the median and the mean are different, the extent of the difference depending on the degree of asymmetry). The standard deviation of 6.2 can be loosely interpreted as the average deviation from the mean of age values. Quantiles can be obtained with the quantile() function. By default, the quartiles (and extremes) are provided, but other quantiles can be obtained by providing appropriate probabilities, as a numeric vector, in argument probs. In some of the example below we use seq() to generate this vector of probabilities. quantile( ~ age, data=d) # quartiles (default) 0% 25% 50% 75% 100% 49 62 67 72 87 quantile( ~ age, data=d, probs = seq(.2, .8, .2)) # quintiles 20% 40% 60% 80% 61 65 69 73 quantile( ~ age, data=d, probs = seq(.1, .9, .1)) # deciles 10% 20% 30% 40% 50% 60% 70% 80% 90% 59 61 63 65 67 69 71 73 76 quantile( ~ age, data=d, probs = c(.05, .95)) # percentiles 5 and 95 5% 95% 58 77 From the last result above, we see that only 5% of the cases are 58 years or younger, and an additional 5% are 77 years or older. Therefore, 90% of the cases have an age between 58 and 77 years. 5.2.2.2 Graphics The most classical graphic to show the distribution of a quantitative variable is the histogram. Histograms are built by defining bins in a variable axis, and drawing a bar for each bin, the area of which is proportional to the frequency of cases in that bin. You can produce histograms with functions gf_dhistogram() or gf_histogram(): the former draws a density axis (which is the relative frequency divided by the bin width), and the later a count axis. They both produce the same skyline, as shown in figure 5.2. plot1 &lt;- gf_dhistogram( ~ age, data=d) plot2 &lt;- gf_histogram( ~ age, data=d) plot1 + plot2 Figure 5.2: Histograms A limitation of histograms is that the resulting shape depends on how the bins are defined, which is an arbitrary decision. In the two histograms of figure 5.3, we get quite different shapes for the age distribution by just changing the number of bins (and therefore the bin width), and the bin boundaries. Clearly, this is not desirable for a graphic aimed at characterizing the shape of a distribution. plot1 &lt;- gf_histogram( ~ age, data=d, bins = 10) plot2 &lt;- gf_histogram( ~ age, data=d, bins = 10, boundary = 50) plot1 + plot2 Figure 5.3: Histograms changing bin width and boundaries A better approach to see the shape of a distribution is the density plot, which is a smoothed version of a histogram. Figure5.4 shows the density plots produced by functions gf_dens() and gf_density(): the former shows the curve and the later fills the area under it. plot1 &lt;- gf_dens( ~ age, data=d, bins = 10, boundary = 50) plot2 &lt;- gf_density( ~ age, data=d, bins = 10, boundary = 50) plot1 + plot2 Figure 5.4: Density plots The density plots in figure 5.4 display a bell-shaped distribution, though not perfectly symmetric. In a density plot, the area below the curve for any given interval of values is proportional to the frequency of observations in it. Thus, age values in the 60-75 interval are far more common than below 60 or above 75, just because the corresponding areas under the curve are markedly different. A simple but very useful graphic is the box-and-whiskers plot or boxplot for short (see figure 5.5). In a boxplot, a box is drawn from the first to the third quartile, and is divided by a line at the median. In addition, two lines (the whiskers) are drawn from the box limits to either the extremes, if there are no outliers, or to the last non-outlier observation. Outliers are values lying farther away than 1.5 times the IQR from the box edges, and are depicted individually using dots, if any (there is none in figure 5.5, but there are several in the distribution of bmi shown in figure 5.6). Boxplots can be produced with function gf_boxplot() of the ggformula package. gf_boxplot( ~ age, data=d) Figure 5.5: Boxplot In figure 5.5 it’s easy to see that the age distribution is pretty symmetric about its median of 67 years (there is just a slight asymmetry identified by an upper whisker a bit lengthier than the lower one), and that no outliers are detected. Because there are no outliers, the whisker limits are the minimum and maximum values (49 and 87, respectively). Also, the box limits are the first and third quartiles (62 and 72, respectively), and thus the box contains the central half of the age values. Last, note that the width of the box does not convey any information and is irrelevant, as is the unlabeled vertical axis. Boxplots are very good at detecting asymmetry and outliers. For instance, in figure 5.6, a clear asymmetry is apparent in the toevent variable, outliers are seen in the upper tail of the distribution of the bmi, and both asymmetry and outliers are present in the case of p14. plot1 &lt;- gf_boxplot( ~ toevent, data=d) plot2 &lt;- gf_boxplot( ~ bmi, data=d) plot3 &lt;- gf_boxplot( ~ p14, data=d) plot1 / plot2 / plot3 # print them stacked Figure 5.6: Boxplots showing assymetry, outliers, or both 5.2.3 Important remarks When summarizing the distribution of a variable using only summary statistics (as is the case in published papers), it is extremely important to select the appropriate ones. This is not a problem for categorical variables, because a frequency table summarizes the distribution with no information loss. Consider for instance this frequency table: sex n Percentage female 4 40 male 6 60 The information in the frequency table above is enough to reproduce the whole distribution of sex values: female, female, female, female, male, male, male, male, male, male This means that, when we summarize the distribution of a categorical variable by providing a frequency table, no information is lost. However, when we summarize the distribution of a quantitative variable by its mean and SD, we loose a lot of information, and as a result we are unable to reproduce the collection of raw values. For this reason, it is especially important to choose the summary statistics wisely. To summarize the distribution of a quantitative variable, we need to provide at least a central tendency measure and a variability measure. The mean and SD are good choices provided the distribution has no marked asymmetry, and no influential outliers. Otherwise, the median and IQR (or the three quartiles) are much better choices. The reason for this is that the mean and SD are affected by asymmetry or influential outliers. Both asymmetry and outliers are easily detected in a boxplot, making it ideal to decide which summary statistics to use. Outliers pull the mean and inflate the SD, but these effects depend on how far they are, how many they are, and how large is the overall number of observations. For instance, it is unlikely that a single outlier in 200 observations is influential, because its effect will be diluted. Conversely, it might have a dramatic effect in a collection of 20 observations. All in all, the main messages for the description of a quantitative variable are: Start always by looking at a graphic, ideally a boxplot Based on what you see in the boxplot, decide how to summarize the distribution: Use mean and SD if neither marked asymmetry nor influential outliers are seen Use the median and IQR otherwise 5.3 Bivariate description Bivariate descriptions involve two variables. The following table 5.5 shows the most common statistics and graphics used in bivariate descriptions according to the type of the two variables. Table 5.5: Bivariate description Variables Statistics Graphics Both categorical frequencies in contingency tables, Pearson’s chi-square, risk difference, relative risk, odds ratio, number needed to treat stacked barchart, mosaic plot Both quantitative covariance, correlation coefficients scatterplot, contour plot One of each type mean, median, standard deviation, variance, interquartile range, extremes, quantiles, mean difference, effect size histogram, density, jitter, violin plot, boxplot In this section, all the graphics and most of the statistics mentioned in table 5.5 will be illustrated, but we will defer the discussion of some statistics - those in italics in the table above - until the chapters devoted to statistical inference. 5.3.1 Two categorical variables The most basic methods to describe the join distribution of two categorical variables are contingency tables and stacked barcharts. A contingency table is a table showing all possible combinations of values of the two variables, and the frequencies of these combinations. In the script below, the tally() function of the mosaic package is used to build contingency tables for the event and group variables. By default, counts are computed. If the format = \"percent\"argument is used, percentages are computed. Note that, in this last case, column percentages are computed, and columns correspond to the variable after the ~). The optional margins = TRUE argument is used to print the column totals to make it clear that the percentages are column percentages, since they add up to 100% in each column. tally(event ~ group, data = d) # counts group event Control MD+nuts MD+VOO No 1945 2030 2097 Yes 97 70 85 tally(event ~ group, data = d, format = &quot;percent&quot;, # percentages margins = TRUE) # print col totals group event Control MD+nuts MD+VOO No 95.249755 96.666667 96.104491 Yes 4.750245 3.333333 3.895509 Total 100.000000 100.000000 100.000000 The results above show that the frequency of events is very low in all three groups, but the percentage is slightly higher in the control group than in any MD group. A stacked barchart is a barchart showing a bar for each category of one of the variables, with each bar split (using different colors) according to the distribution of the other variable. Stacked barcharts can be produced with functions gf_bar() and gf_props(), that show an axis of absolute or relative frequencies, respectively. In the first barchart of figure 5.7 (left), the length of each bar is determined by the absolute frequency (count) of the corresponding diet group. This is not very useful to compare the splits of different bars. This is not very problematic in this case, because the number of patients randomized to each diet group is similar. Even so, it is better to display a vertical axis of proportions, so that all bars have the same length and their splits can be compared fairly. This is the case of the second barchart in figure 5.7 (right), where the higher proportion of events in the control group is apparent. plot1 &lt;- gf_bar( ~ group , fill = ~ event, data = d) # counts plot2 &lt;- gf_props(~ group , fill = ~ event, data = d, position = &quot;fill&quot;) # percentages plot1 + plot2 Figure 5.7: Staked barcharts An interesting alternative to stacked barcharts is the mosaic plot. Such a plot is composed of tiles corresponding to the cells of a contingency table, created by recursive vertical and horizontal splits of a square. The area of each tile is proportional to the corresponding cell frequency, given the dimensions of previous splits. A mosaic plot can be produced with the mosaic() function of package vcd. library(vcd) mosaic(event ~ group, data=d) Figure 5.8: Mosaic plot Mosaic plots are particularly useful when we want to represent higher dimensional tables involving more than two variables. For instance, because diabetes is a known risk factor of cardiovascular events, we may want to take it into account when comparing the frequency of events among the tree treatment groups. Here is a mosaic plot on the three-dimensional contingency table generated by the event, group and diab variables: library(vcd) mosaic(event ~ group + diab, data=d) Figure 5.9: Mosaic plot with three variables Several things are immediately apparent in this plot. First, the number of diabetics is higher in the MD+VOO group, followed by the control group and by the MD+nuts group. Second, for both diabetic and non-diabetic subjects, the frequency of events is higher in the control group than in any of the MD groups. 5.3.2 Quantitative and categorical variable When one of the variables is quantitative and the other is categorical, we can apply the methods for the description of a quantitative variable for each level of the categorical variable. For instance, suppose we want to compare the body mass index (BMI) in the three treatment groups of the predimed trial. We can compute summary statistics for each group, using the favstats() function, indicating the quantitative variable before the tilde, and the categorical variable after it as shown below: favstats(bmi ~ group, data = d) group min Q1 median Q3 max mean sd n missing 1 Control 19.71 27.53 30.000 32.7675 51.94 30.28044 3.963947 2042 0 2 MD+nuts 19.64 26.95 29.455 32.1800 51.83 29.68725 3.766372 2100 0 3 MD+VOO 20.04 27.22 29.735 32.4200 49.12 29.94050 3.706966 2182 0 In terms of graphics, we can plot the density of the BMI for each group, using colors to differentiate groups. This can be done with function gf_density() by assigning different fill colors depending on the group: note the use of the fill argument, and the ~ preceding variable group. In this case, the distribution of the BMI is very similar in all three groups, since the densities are almost perfectly overlapped. gf_density( ~ bmi, data = d, fill = ~ group) Figure 5.10: Density by group We can also use boxplots, violin plots, or jitter plots. These are illustrated in in figure 5.11. A violin plot (figure 5.11, center) is just a mirrored density for each diet group. A jitter plot (figure 5.11, right) displays all individual values as points, randomly scattered in the direction perpendicular to the variable axis to prevent excessive overlapping of points. In the jitter plot below we have used the argument alpha to decrease the opacity of the points, making them transparent. Transparency is important when plotting very many observations, so that regions with higher density of cases are distinguished (if a solid color was used, the result would be too pasted to appreciate density variations). In all three plots, we have used different colors for fancy: they do not convey any information, since groups are already distinguished by the horizontal axis. Later in this chapter we will show a wiser way to use colors. p1 &lt;- gf_boxplot(bmi ~ group, data = d, color = ~ group) p2 &lt;- gf_violin(bmi ~ group, data = d, color = ~ group) p3 &lt;- gf_jitter(bmi ~ group, data = d, color = ~ group, alpha=0.2) p1 + p2 + p3 Figure 5.11: Boxplot, violin and jitter plots, by group All three plots above allow to compare the distribution of the BMI among treatment groups, which are very similar as expected by the random allocation of the three diets. 5.3.3 Two quantitative variables When we want to look at the bivariate distribution of two quantitative variables, the scatterplot is the graphic of choice. In a scatterplot, individual observations are depicted as points positioned according to the value of each variable. The points collectively form a cloud of points whose shape characterizes the relationship between the two variables. Figure 5.12 shows the scatterplot of waist and bmi obtained with function gf_poin(). gf_point(waist ~ bmi, data=d, alpha=0.1) %&gt;% # scatterplot gf_lims(x = c(20, 55), y = c(40, 180)) # sets axis limits Figure 5.12: Scatterplot Despite using a low value in the alpha argument, the cloud of points in figure 5.12 is quite pasted due to the high number of observations (n = 6324), making it difficult to appreciate density variations in the inner part of the cloud. An alternative to the scatterplot when the number of observations is very high is the contour plot, which is a sliced two-dimensional density: points of the plane having the same density are connected with lines. Figure 5.13 shows the contour plot for the bmi and waist variables, obtained with function gf_density2d(). It is easy to see that the density increases progressively towards the center of the cloud from any outside point. gf_density2d(waist ~ bmi, data=d) %&gt;% # contour plot gf_lims(x = c(20, 55), y = c(40, 180)) # set axis limits Figure 5.13: Contour plot The scatterplot and the contour plot above were built using the same scale for easy comparison, setting the axis limits with gf_lims() after a pipe (%&gt;%). In both graphics, bmiand waistappear positively related, since low (or high) values of one variable tend to be associated with low (or high) values in the other. 5.4 Adding infromation from additional variables The graphic functions in the ggformula package have several arguments we can use to create more informative graphics, by adding information on additional variables. In fact, we have already used one of these arguments, the fill argument, in figure 5.7 to split the bars according to the event variable, or in figure 5.10 to distinguish the densities in each treatment group. The fill argument is useful to color areas. Similarly, there is a color argument to color lines and symbols. This was used in figure 5.14 to map the color of points to variable sex, making it clear that, for any given value of waist, the waist to height ratio (wth) tends to be higher in females than in males. Note that this plot was built from a random sample of 300 subjects of the d dataset, to improve the visibility of individual points. set.seed(1) # for reproducibility rand_sample &lt;- sample(d,150) # random sample of 150 cases gf_point(wth ~ waist, col= ~ sex, data=rand_sample, alpha=0.5) Figure 5.14: Scatterplot with color maped to sex Also, the size argument can be used to map the size of symbols to a variable, as done for bmi in the bubble plot of figure 5.15. In the resulting plot, the size of the bubbles tends to increase as the waist and wth increase. gf_point(wth ~ waist, size= ~ bmi, data=rand_sample, alpha=0.1) Figure 5.15: Bubble plot A very useful way to add information on a categorical variable, that can be used with any type of graphic, is to use facets. These are obtained by adding the variable to the formula after a bar symbol (|), as done in the script below: the resulting expression can be read BMI as a function of group, given sex. As you see in figure 5.16, this results in a boxplot of bmi by group, but the two levels of sex are shown in two panels or facets that share the same scale for bmi to facilitate the comparison. It is quite clear that females have slightly higher BMI values than males, but no clear differences are see between treatment groups in either males or females. gf_boxplot(bmi ~ group | sex, data=d, alpha=0.5) Figure 5.16: Facets Facets can be defined according to two variables as well, as shown in the following script (note the use of the + to add smoke after sex). In the resulting figure 5.17, facets are arranged in a matrix, the rows of which define the sex, and the columns the smoking status. gf_boxplot(bmi ~ group | sex + smoke, data=d, alpha=0.5) Figure 5.17: Facets for sex and smoking habit Last, we can combine some of the previous options as in the following boxplots, where we can inspect the distribution of bmi according to four variables (group, sex, smoke and diab). gf_boxplot(bmi ~ group | sex + smoke, color= ~diab, data=d, alpha=0.5) Figure 5.18: Facets for sex and smoking habit, and color for diabetes There are still other arguments that can be used in some functions of the gg_formula package we did not cover here. You should be aware that not all arguments can be used in all functions (for instance, gf_contour() has no size argument). To know which arguments are available in a particular function of gg_formula, look at its fast help , by calling the function with no arguments, as shown below for gf_point(): gf_point() gf_point() uses * a formula with shape y ~ x. * geom: point * key attributes: alpha, color, size, shape, fill, group, stroke For more information, try ?gf_point 5.5 Graphic refinement We started this chapter saying that EDA graphics are fast and dirty. This does not mean you cannot produce “proper” graphics with ggformula functions. For instance, all functions in this package have optional arguments like title, subtitle, xlab, ylab allowing to define titles and axis labels within the function call. In some cases, overlaying two graphics produces a more informative result. You can overlay graphics using the %&gt;%, as in the following examples. In figure 5.19 contours are overlaid on the scatterplot of more than six-thousand points, to make visible the density gradient. When calling two functions chained by %&gt;%, there is no need to repeat identical arguments in the second function. gf_point(waist ~ bmi, data=rand_sample, alpha=0.1) %&gt;% gf_density2d() # no need to repeat formula and data Figure 5.19: Countour overlaid on a scatterplot As a second example, in figure 5.20 jitter points are overlaid on a boxplot. The with argument in the gf_jitter() function controls the amount of horizontal jitter. Note the outlier.alpha = 0 to avoid double plotting of outliers. gf_boxplot( bmi ~ group | sex, data=rand_sample, fill = NULL, color = ~ group, outlier.alpha = 0) %&gt;% gf_jitter(width = 0.1, alpha = 0.5) Figure 5.20: Jittered points overlaid on a boxplot Overlaying can be used to add information on summary statistics computed by function df_stats, which is very similar to the favstats() function presented in section 5.2.2.1. In the following script, we compute the summary statistics for bmi, save them in dataframe bmi_stats, and use this dataframe to overlay mean values to a boxplot. This is very useful to assess the influence of outliers, by comparing the mean and the median. bmi_stats &lt;- df_stats(bmi ~ group, data = d) # compute stats gf_boxplot(bmi ~ group, data = d) %&gt;% gf_point(mean ~ group, color = &quot;red&quot;, data = bmi_stats) %&gt;% # use them here gf_refine(coord_flip()) # flips axes (to get horizontal boxplots) Figure 5.21: Boxplot with means overlaid (red dot) In figure 5.21 the means and the medians of each group are practically identical, reflecting no or little influence of outliers on the mean, despite being quite far away (some values above 50!). Last, functions gf_text() and gf_label() are very handy to overlay annotations on a graphic. For instance, the lack of influence of outliers on the mean of bmi commented above is likely due to dilution because the number of observations, but this number is not reflected in the boxplot. Thus, it is a good idea to print this number for each group. This has been done in figure 5.22, using variable n from bmi_stats in function gf_label(). Note the use of paste() to prefix n with the text N =. gf_boxplot(bmi ~ group, data = d) %&gt;% gf_point(mean ~ group, color = &quot;red&quot;, data = bmi_stats) %&gt;% gf_label(15 ~ group, label = ~ paste(&quot;N =&quot;, n), data = bmi_stats) %&gt;% gf_lims(y = c(11, 54)) %&gt;% # to get space for labels gf_refine(coord_flip()) Figure 5.22: Boxplot with means and annotation of N Resources Plotting with formulas is a tutorial on the ggformula package you will find it in the Tutorial pane of RStudio (once you have installed this package). We strongly advise you follow this tutorial. If you enjoy it, you may want to learn how to refine your plots by following the second tutorial Refining ggformula plots. DataExplorer is a package for EDA you might want to know about. The summarytools package provides a few functions to neatly summarize your data. Venn diagrams and upset plots are useful when we want to visualize the combination for multiple dichotomous variables (like combinations of COVID19 symptoms). The ggplotgui package is graphical user interface (GUI) to build ggplot2 and Plotly graphics. No need to write R code, but you can save the code for reproducibility. The esquisse package is another GUI for ggplot2 graphics. Exercises Look at the help of the gf_bar() function and experiment with the arguments to describe the smoke variable of the d dataframe used in this chapter. We suggest the following: color = \"darkblue\" fill = \"darkblue\" xlab = \"Treatment group\" title = \"Smoking habits in the Predimed trial In the previous exercise, the ordering of the smoking categories in the plots (Never, Current, Former) is not very nice. Reproduce some of these plots by redefining the levels of smoke so that the ordering is Never, Former, Current. This text data file contains data on people involved in a well known historical event. Read the data and answer the following questions: How any people were involved in this event? Explore the univariate distribution of the four variables with bar plots. Did most of these people survive? Was Sex evenly distributed? What about Age, and what about Class? What are the levels of the Class variable? Produce stacked barcharts to explore Survival by Class, by Age, and by Sex. Did children survive more than adults? Did females survive more than males? What were the Classes surviving the most and the least? Produce contingency tables to explore Age by Class and Sex by Class. Are Age and Sex evenly distributed among Classes? Do you guess what is this historical event? Install package gapminder. Then load it with library(gapminder) and look at the help of the gapminder dataset included in this package. Subset the gapminder dataset for year 2007 and answer the following questions: How many countries there are in each continent? What is the range of life expectancy (LE) values overall? and by continent? Look at the distribution of life expectancy in all countries by producing a density plot. Does it look symmetric? Inspection an appropriate graphic to investigate if life expectancy is similar in all continents. What was the life expectancy in Spain? Is there a relationship between life expectancy and GDP per capita? If yes, is it linear? Is it similar in all continents? Now subset the gapminder dataset for Spain, France, Portugal, and Italy, keeping all years (not only 2007!), and use function gf_lines() to plot the evolution of life expectancy in these countries (you will need to specify the group argument). How does Spain compare to France, Portugal, and Italy? Here, the term distribution refers to the collection of observed values of a variable in a set of individuals, which is sometimes called, more precisely, empirical distribution of that variable. In other contexts, notably in probability theory, it is used to refer to theoretical models characterizing the behavior of random variables, such as the normal distribution, the binomial distribution, etc.↩︎ "],["statistical-inference.html", "6 Statistical inference 6.1 Population and sample 6.2 Inference problems 6.3 Probability distributions 6.4 Random samples 6.5 Example data 6.6 Estimation 6.7 Sample size and CI’s 6.8 Significance tests 6.9 Sample size and p values 6.10 Types of tests Resources Exercises", " 6 Statistical inference Statistical inference refers to the attempt to learn something about a population through the analysis of a sample. Let’s start by clarifying the meaning of these two terms. 6.1 Population and sample In statistics, a sample is a finite number (n) of observations, no matter how large, and population refers to a much wider set of individuals from which the sample was drawn. Actually, in statistical theory populations are often assumed to be infinite. In practical applications however, they may be finite, but much, much larger than the sample. For instance, suppose we want to investigate how common is low weight at birth (less than 2.5 kg) among singleton births. To this end, the weight of newborns in the 189 singleton births attended in hospital H is recorded during a year. This is just a sample of newborns, and our interest is not restricted to these 100 newborns in particular. Rather, we would like to learn something about the population of all newborns, or at least all newborns in a geographical area. Figure 6.1 illustrates the conceptual framework for statistical inference, were a quantity of interest in the population -like the proportion of low weight newborns- is denoted by \\(\\pi\\). A sample of n = 100 observations is drawn from the population, and the sample proportion, denoted by p, is computed by dividing the number of low weight newborns over the sample size (p = 23/100 = 0.23). It is crucial to realize that the value of p will not necessarily be equal to that of \\(\\pi\\). Moreover, if we draw another sample, it is very likely that the value of p in this new sample is different from that in the first sample, due to sampling variation. The pattern of variation of a statistic (like p) across samples is called its sampling distribution, and plays a central role in inferential statistics. In some circumstances the sampling distribution of a statistic is well known, and this knowledge is used to devise methods allowing to solve common questions about population quantities (like \\(\\pi\\)). However, for this to be the case, samples need to be drawn at random. Later in this chapter we will explain what random samples are, and how to draw a random sample from a population when this is possible. Figure 6.1: Conceptual framework for statistical inference Population quantities such as \\(\\pi\\) are called population parameters, or simply parameters, while quantities computed from sample data, like p, are called statistics. When a statistic is used to estimate a parameter, it is said to be an estimator of this parameter. Because it is fundamental not to confuse parameters and estimators, we usually denote the former by Greek letters and the later by Latin letters. The following table shows the usual notation for some very common parameters and corresponding estimators: Parameters Estimators Mean \\(\\mu\\) \\(\\bar{x}\\) Standard deviation \\(\\sigma\\) \\(s\\) Variance \\(\\sigma^2\\) \\(s^2\\) Proportion \\(\\pi\\) \\(p\\) Correlation coefficient \\(\\rho\\) \\(r\\) … (greek letters) (latin letters) 6.2 Inference problems There are two main types of inference problems: estimation and significance testing. Estimation is just a nice name for guessing quantities or, more specifically, population parameters. For instance, in the example of the previous section we were interested in estimating \\(\\pi\\), the proportion of low weight newborns in the population. Other examples of research questions involving estimation could be: What is the mean weight of newborns? What is the median survival time of breast cancer patients? What is the length of the COVID-19 incubation period, on average? How long does the protection of a COVID-19 vaccine last, on average? Significance testing is a procedure to assess the plausibility of scientific hypotheses. A scientific hypothesis is formalized into a statistical hypothesis, which is a statement about one or more parameters6. For instance, according to the National Center for Health Statistics, 8.31% of newborns in the US are low birthweight. Thus, we could hypothesize that this is the value of \\(\\pi\\) in the geographic area of hospital H, that is \\(\\pi = 0.0831\\). In this case, the statistical hypothesis proposes a particular value for a parameter, but in other cases it will involve the comparison of two (or more) parameters; for example, we might be interested in testing whether the frequency of low birth weight is the same in smoking and non-smoking mothers. Other examples of questions that could be addressed by testing appropriate statistical hypotheses are: Is a new diagnostic test more specific than the test currently used ? Is a specific genotype associated with coronary heart disease ? Does vitamin D reduce the risk of COVID-19? In this and subsequent chapters we will cover estimation and significance testing methods, all of which are based on the assumption that we have a random sample of observations. 6.3 Probability distributions Probability distributions are mathemtical models useful to describe the behaviour of random variables (i.e., variables taking values depending on chance). In random samples, statistics computed from sample data are random variables. There are many different probability distributions, for both discrete and continuous variables. Some important probability distributions models for discrete variables are the Bernoulli, the binomial, and the Poisson distributions. Important probability distributions models for continuous variables are the normal, chi-squared, Student’s t, and F distributions. Despite probability distributions are at the heart of inferential methods, their study can be omitted in a purely instrumental approach to data analysis, such as that adopted in this book. However, we will present some basic ideas about the normal distribution for two reasons: first, because it is the most important distribution model for continuous variables; and second, because some of the inferential methods that will be introduced in later chapters will require to judge whether the normal model is adequate to represent empirical the empirical data. A couple of words will be said also on two additional distribution models, the Student’s t and the chi-squared distributions, because these are used in some methods of inferential analysis that will be introduced later in this book. 6.3.1 The normal distribution The normal or Gaussian distribution is a bell-shaped curve symmetric about the mean, such as those shown in figures 6.2 and 6.3. A normal distribution is defined by two parameters, the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)). The mean is where the distribution is centered, and the standard deviation determines its spread. Figure 6.2 displays three normal distributions with means \\(\\mu = 0\\) (black), \\(\\mu = 5\\) (red), and \\(\\mu = 20\\) (blue), all having the same standard deviation \\(\\sigma = 1\\). Conversely, figure 6.3 shows three other normal distributions, all having the same mean \\(\\mu = 10\\), but differing in spread (black: \\(\\sigma = 1\\); red: \\(\\sigma = 2\\); and blue: \\(\\sigma = 4\\)). The vertical scale of figures 6.2 and 6.3 shows probability densities, which have no simple interpretation. In particular, they cannot be interpreted as probabilities. Rather, probabilities are given by areas under the curve. Figure 6.2: Normal distributions differing in mean (0, 5, and 20), and standard deviation 1 Figure 6.3: Normal distributions with mean = 10, differing in standard deviation (1, 2 and 4) There is an infinite number of normal distributions that differ by the value of the parameters, which jointly determine a single normal distribution. Among all members of the normal distribution family, the standard normal distribution is of particular interest. This is a normal distribution with parameters \\(\\mu = 0\\) and \\(\\sigma = 1\\). Variables follwing a standard normal distribution are often denoted with letter \\(Z\\). In a standard normal distribution, the \\(Z\\) values -1.96 and +1.96 limit a central probability interval of 95% (leaving 2.5% probability regions below -1.96 and above +1.96). Figure 6.4: Standard normal distribution with central 95% probability interval Any normal distribution can be mapped to the standard normal distribution with a transformation called standardization. If a variable \\(X\\) follows a normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\), then the following transformation will map \\(X\\) values to values of a standard normal distribution, in such a way that the relative positions of values, and the probability intervals they may define, are preserved: \\[Z \\quad = \\quad \\frac{X - \\mu}{\\sigma}\\] For instance, suppose that a particular variable \\(X\\) follows a normal distribution with \\(\\mu = 100\\) and \\(\\sigma = 10\\), which is often written as \\(X \\sim N(100, 10)\\). Solving for \\(X\\) the stardardization formula above gives, \\[ X \\quad = \\quad \\mu + Z \\sigma\\] which in this case is \\[ X \\quad = \\quad 100 + 10 \\ Z\\] If we know that the standard normal values \\(Z_1 = -1.96\\) and \\(Z_2 =+1.96\\) limit a central interval of 95% probability, then the corresponding \\(X\\) values will also limit such an interval (see figure 6.5): \\(X_1 \\quad = \\qquad \\quad 100 + (-1.96) \\ 10 \\quad = \\quad 100 -19.6 \\quad = \\quad 80.4\\) \\(X_2 \\quad = \\qquad \\quad 100 + (+1.96) \\ 10 \\quad = \\quad 100 +19.6 \\quad = \\quad 119.6\\) Figure 6.5: N(100,10) distribution with central 95% probability interval 6.3.2 The chi-squared distribution The chi-squared distribution is a probability distribution model for continuous variables taking positive values only. This distribution depends on a single parameter called degrees of freedom, usually denoted by the greek letter \\(\\nu\\). Figure 6.6 displays several chi-squared distributions, with \\(\\nu = 1\\) (red), \\(\\nu = 3\\) (blue), and \\(\\nu = 5\\) (black) degrees of freedom. Figure 6.6: Chi-squared distributions with 1, 3, and 5 degrees of freedom Some statistics that are functions of table counts follow a chi-squared distribution, which provides the basis for some methods of inferential analysis we will introduce in chapter 7. 6.3.3 The Student’s t distribution The Student’s t distribution, or simply t dsitribution, is a probability distribution model for continuous variables, very similar in shape to the standard normal distribution. However, this distribution depends on one parameter only, called its degrees of freedom. Figure 6.7 displays a t distributions with 5 and 10 degrees of freedom (blue), and a standard normal distribution (black). Though very similar, the t distribution has slightly hevier tails than the standard normal. This difference decreases as the number of degrees of freedom increases, so that for 30 degrees of freedom and above, the t distribution is virtualy identical to the standard normal. Figure 6.7: t distribution with 10 degrees of freedom (blue), and standard normal distribution (black dashed line) Under certain circumstances, some statistics that are functions of sample means follow a t-distribution, which provides the basis for some methods of inferential analysis on means we will introduce in chapter 8. 6.4 Random samples Many different procedures can be used to get a sample of observations from a population. When a sampling procedure is such that the probability of selecting an individual is known for all population individuals, we say this is a random or probabilistic sampling procedure, and the resulting sample a random (or probabilistic) sample. In any other case, the sampling procedure, and the resulting sample, are non-random. Examples of non-random samples are: Patients with surname starting with letters A to L. Patients visited on Monday, Tuesday or Wednesday. Consecutive patients. There are several types of random sampling procedures. In simple random sampling, all population individuals have the same probability of being selected. In order to guarantee this, random number generating functions should be used, such as the sample() function in R. Suppose we want to select a simple random sample (SRS) of patients from the population of 90,000 patients visited in an emergency service during the last year. For simplicity, assume that these patients are uniquely identified by numbers 1 to 90,000. To get a SRS of say 10 patients, we can use the sample() function. The first argument to this function should be either a vector of unique identifiers of all population individuals, or the size of the population (in which case identifiers are assumed to be the natural numbers from 1 up to the population size); the second argument (size) should be the required sample size: sample(1:90000, 10) # vector of identifiers as 1st argument [1] 8465 6563 62126 20205 62480 87817 76380 68581 55968 8615 sample(90000, size = 10) # population size as 1st argument [1] 41381 45196 9346 13824 8696 61798 61140 39557 7131 61256 The results of the two functions calls above are different, and if you run them twice you will get different results once again, so that the random sampling result is not reproducible. If we want our random selection to be reproducible (as we should), then we need to set a seed for the random number generator with set.seed(), before calling sample(): set.seed(123) # setting the seed to 123 (arbitrary number) sample(90000, 10) [1] 51663 57870 2986 29925 68293 62555 45404 65161 46435 9642 Now, the re-execution of this code (both lines!) will produce the same results. See? set.seed(123) # second execution sample(90000, 10) [1] 51663 57870 2986 29925 68293 62555 45404 65161 46435 9642 It should be noted that to get a SRS we do need the list of individuals in the population (a population census) and their unique identifiers (like 1, 2, …90000); otherwise, it is just impossible to get a SRS. There are other, more complex random sampling procedures, like stratified sampling, cluster sampling, and multi-stage sampling. These are frequently used in community health surveys. However, we will not discuss them because the inferential methods presented in this book, and in many other books and courses on statistics, assume the data come from a SRS. The inferential analysis of data obtained through random sampling procedures other than SRS is more complex, and is out of the scope of this book. Many people, including some researchers, have a wrong idea of what is random sampling or a random sample. It is not just an apparently harmless selection procedure like, for instance, consecutive patients. Rather, it requires the probability of selection to be known for all population individuals and, in the case of SRS, this probability should be the same for all of them. If this is not the case, the selection `procedure should not be called random, and the resulting sample should not be described as a random sample. Depending on what is the population of interest, obtaining a SRS may be just impossible (i.e., if there is no census available), and then a convenience sample (any sample we can possibly get) is better than nothing. However, if SRS is possible, there is no excuse to use a non-random sample. In any case, to use inferential methods with a convenience, non-random sample, requires to assume that the sampling process behaves as a random sampling procedure, and that the sample composition resembles that of a random sample. This is actually the case in many research studies, based on convenience samples. 6.5 Example data To illustrate the concepts introduced in the remaining of this chapter we will use the birthwt dataset in package MASS, containing data on 189 births collected at the Baystate Medical Center, Springfield, Massachusetts (US). We will be concerned with the following variables (see ?MASS::birthwt for a description of all the variables in the dataset): low: an indicator of low birth weight (less than 2500 grams). race: mother’s race (1 = white, 2 = black, 3 = other). smoke: mother’s smoking status during pregnancy (0 = no, 1 = yes). ht: mother’s history of arterial hypertension (AHT) (0 = no, 1 = yes). bwt: birth weight in grams. Here are the first six rows of the dataset: head(MASS::birthwt) low age lwt race smoke ptl ht ui ftv bwt 85 0 19 182 2 0 0 0 1 0 2523 86 0 33 155 3 0 0 0 0 3 2551 87 0 20 105 1 1 0 0 0 1 2557 88 0 21 108 1 1 0 0 1 2 2594 89 0 18 107 1 1 0 0 1 0 2600 91 0 21 124 3 0 0 0 0 0 2622 The following script defines factors for categorical variables, renames low to bw (with levels labeled as low or normal) and prints the first six rows of the resulting dataframe d: library(dplyr) d &lt;- MASS::birthwt %&gt;% mutate(low = factor(low, levels = 1:0, labels = c(&quot;low&quot;, &quot;normal&quot;)), race = factor(race, levels = 1:3, labels = c(&quot;white&quot;, &quot;black&quot;, &quot;other&quot;)), smoke = factor(smoke, levels = 1:0, labels = c(&quot;smoker&quot;, &quot;non-smoker&quot;)), ui = factor(ui, levels = 1:0, labels = c(&quot;UI&quot;, &quot;no-UI&quot;)), ht = factor(ht, levels = 1:0, labels = c(&quot;AHT&quot;, &quot;no-AHT&quot;))) %&gt;% rename(bw = low) head(d) bw age lwt race smoke ptl ht ui ftv bwt 85 normal 19 182 black non-smoker 0 no-AHT UI 0 2523 86 normal 33 155 other non-smoker 0 no-AHT no-UI 3 2551 87 normal 20 105 white smoker 0 no-AHT no-UI 1 2557 88 normal 21 108 white smoker 0 no-AHT UI 2 2594 89 normal 18 107 white smoker 0 no-AHT UI 0 2600 91 normal 21 124 other non-smoker 0 no-AHT no-UI 0 2622 6.6 Estimation Proportions and means are common parameters we may want to estimate. Proportions are relevant when dealing with categorical variables (such as bw), while means are relevant when dealing with quantitative variables (such as bwt). For example, we may want to answer these two questions: What is the proportion of low weight newborns (\\(\\pi\\))? What is the mean weight of newborns (\\(\\mu\\))? The simplest way to estimate a parameter is to compute the corresponding estimator: the sample proportion (p) in the case of \\(\\pi\\), or the sample mean (\\(\\bar{x}\\)) in the case of \\(\\mu\\). The result provided by an estimator when computed in a particular sample is called a point estimate of the corresponding parameter. To get a point estimate for \\(\\pi\\), we simply compute the frequency table of bw with function tally() of the mosaic package, using argument format to get the proportions: library(mosaic) tally(~bw, data = d, format = &quot;proportion&quot;) bw low normal 0.3121693 0.6878307 To get a point estimate for \\(\\mu\\), we can use function mean() to compute the sample mean of bwt: mean(~bwt, data = d) [1] 2944.587 So, the point estimates are 31.2 % for \\(\\pi\\), and 2945 grams for \\(\\mu\\) (rounded to unit). The problem with point estimates is that, because of (random) sampling variation, we have no idea of how similar or different from the parameter value they may be. To solve this uncertainty, confidence intervals are computed. A confidence interval (CI) is an interval around the point estimate, such that we have a given confidence level that it will contain the true value of the parameter, somewhere within the interval. Although it is quite common to compute CI’s with 95% confidence, we could use other confidence levels, such as 90%, or 99%. To compute the 95% confidence interval for \\(\\pi\\), it is convenient to chain functions binom.test(), confint() and round(). The computation of the CI is made by function binom.test(), but this function produces other results as well we are not interested in right now, and function confint() extracts just the CI. Then, round() will round the results to the desired number of decimals. This will provide the 95% CI for the category of bw indicated in argument success in the binom.test() function (which is low): binom.test(~ bw, data = d, success = &quot;low&quot;) %&gt;% confint() %&gt;% round(3) probability of success lower upper level 1 0.312 0.247 0.383 0.95 The result provided includes the point estimate (labeled as probability of success), the lower and upper bounds of the CI, and the confidence level (labeled as level). This result means that we are 95% confident that the value of \\(\\pi\\) is somewhere between 24.7% and 38.3%. To compute a CI with a different confidence level, use the conf.level argument in binom.test() to indicate the desired level (as a proportion, not as a percentage!): binom.test(~ bw, data = d, success = &quot;low&quot;, conf.level = .99) %&gt;% confint() %&gt;% round(3) probability of success lower upper level 1 0.312 0.228 0.406 0.99 Thus, with 99% confidence, the value of \\(\\pi\\) is somewhere between 22.8% and 40.6%. Note that, the higher the confidence level, the wider the resulting CI. There are several methods available to compute the CI for a proportion (see ?binom.test and references therein). Most books and courses on statistics present the so called Wald method, which is an approximated method based on the normal distribution. Though perfectly valid in many instances, in some cases it should be avoided, at the risk of getting unreliable or even absurd results (e.g., values lower than zero, or higher than one). For this reason, the default method used by biniom.test() (the so called Clopper-Pearson method) is safer, can be applied always, and never produces values out of the 0 to 1 range. However, if you ever need to compute the Wald CI, you can get it using the ci.method argument: binom.test(~ bw, data = d, success = &quot;low&quot;, ci.method = &quot;Wald&quot;) %&gt;% confint() %&gt;% round(3) probability of success lower upper level 1 0.312 0.246 0.378 0.95 As you see, this CI is slightly different to what we got when we did not specify the ci.method, and the default (Clopper-Pearson) was used. To compute a CI for \\(\\mu\\) we can chain functions t.test() and confint(). Again, the actual CI computation is made by t.test() along with other results, and confint() extracts just the CI. t.test(~ bwt, data = d) %&gt;% confint() mean of x lower upper level 1 2944.587 2839.952 3049.222 0.95 Thus, we are 95% confident that the population mean \\(\\mu\\) is somewhere between 2840 and 3049 grams (rounding to unit). For a different confidence level, the conf.level argument of t.test() can be used: t.test(~ bwt, data = d, conf.level = 0.99) %&gt;% confint() mean of x lower upper level 1 2944.587 2806.558 3082.616 0.99 Again, we see that a higher confidence level results in a wider CI. The method used by t.test() to compute the CI is based on the assumption that the quantitative variable (bwt) is about normally distributed in the population. In practice, it will be sufficient to verify that the distribution is approximately symmetric, and there are no influential outliers. This can be easily verified by inspecting a boxplot: library(ggformula) gf_boxplot(~ bwt, data = d) Figure 6.8: Boxplot of birthweights In this case, the distribution is fairly symmetric. There is a single outlier, pretty close to the edge of the lower whisker, so it can hardly affect the mean, given the sample size (n = 189). Therefore, we can rely on the CI computed by t.test(). If this was not the case, alternative methods are available, such as the bootstrap method that will be presented in subsequent chapters. We have seen how to compute CI’s for a proportion and for a mean, but CI’s can be computed for many other parameters as well, such as: A difference of proportions, \\(\\pi_1 - \\pi_2\\) A difference of means, \\(\\mu_1 - \\mu_2\\) A relative risk An odds ratio … and many others. To compute a CI for these and other parameters, we will need to use different functions. However, the result will be always an interval (defined by a lower and an upper bound), and its interpretation is always the same: we have the specified confidence that the CI contains the value of the population parameter we are estimating. When we compute a CI with a given confidence level, there is some risk that it does not contain the value of the parameter. This risk is the complementary of the confidence level, so that: 95% CI’s will not contain the parameter about 5% of the times 99% CI’s will not contain the parameter about 1% of the times Therefore, the higher the confidence level used to compute a CI, the lower the risk. So, why not to compute CI’s always with a very high confidence level, such as 0.999999? Well, we have seen that increasing the confidence level results in wider, less informative CI’s. The width of the CI reflects uncertainty (since we can’t know where the parameter value is inside the CI). Therefore, the wider the CI the less informative it is. Fortunately, the width of the CI depends also on the sample size, and this makes it possible to get high confidence, as well as narrow, informative CI’s. 6.7 Sample size and CI’s Figure 6.9 shows the 95% CI’s for a proportion \\(\\pi\\) resulting from several hypothetical studies, all of them having the same sample proportion p = 0.312 (or 31.2%), but different sample size (n). It i s clear that the CI shrinks as n increases. This means that, for a given value of p, we can choose a sample size that will produce a 95% CI of desired width. Of course, this is relevant when designing a study (once the study has been conducted, the sample size is fixed). For instance, if we want the CI width to be no larger than 0.10 (or 10%), we need to collect n = 350 subjects. Figure 6.9: Effect of sample size on the CI for a proportion (p = 0.312) When designing a study whose objective is to estimate a parameter with a given confidence level (say 95%), an important question is to determine the sample size needed, so that the resulting CI width is no greater than a desired value. However, because the CI width depends also on the value of the point estimate (p), figure 6.9 cannot be used as guidance (but in the case of expecting p to be 0.312). Function prec_prop() in package presize allows to compute the required sample size from the expected value of p, the desired width of the CI, and the confidence level. Several methods are available, depending on how the CI will be computed. Here we use method = exact. By default, 0.95 (or 95%) confidence level is assumed. The printed result documents the arguments used in the computation, and the required sample size (n). library(presize) prec_prop(p = 0.312, conf.width = 0.10, method = &quot;exact&quot;) sample size for a proportion with exact confidence interval. p padj n conf.width conf.level lwr upr 1 0.312 NA 347.2988 0.1 0.95 0.2636203 0.3636203 NOTE: padj is the adjusted proportion, from which the ci is calculated. Thus, assuming we expect p = 0.312, to get a 95% CI of width = 0.10, the required sample size is 348 (obviously, the sample size has to be an integer since it is the number of subjects, so that the result should be rounded up always). If the CI is to be computed with a different confidence level (say 0.99) the conf.level argument should be set to this value as done below. Because increasing the confidence level results in a wider CI, the sample size required to keep the CI width = 0.10 is now n = 586, much larger than it was for a 95% CI. prec_prop(p = 0.312, conf.width = 0.10, method = &quot;exact&quot;, conf.level = 0.99) sample size for a proportion with exact confidence interval. p padj n conf.width conf.level lwr upr 1 0.312 NA 585.9679 0.1 0.99 0.2635561 0.3635561 NOTE: padj is the adjusted proportion, from which the ci is calculated. We have seen that the sample size has an effect on the width of the CI for a proportion, and how to use this fact to determine the sample size when designing a study. Similar arguments hold for many other parameters (i.e., increasing the sample size reduces their CI width), and in particular for means. Suppose we are designing a new study to estimate the mean weight of newborns, and we want the resulting 95% CI to have a width of 200 grams. We can use function proc_mean() from the presize package to determine the required sample size, but in this case we need to provide an estimate of the standard deviation (SD) of the weight of newborns, and we assume this to be 730 grams. Though it is not necessary for the calculation, this function expects an argument specifying the expected mean (mu). prec_mean(mean = 3000, sd = 730, conf.width = 200) sample size for mean mean sd n conf.width conf.level lwr upr 1 3000 730 207.1364 200 0.95 2900 3100 Thus, assuming that SD = 730 grams, we require a sample of size n = 208 to get a 95 CI of width = 200 grams. If the CI is to be computed with a different confidence level (e.g., 0.99), the conf.level argument is used: prec_mean(mean = 3000, sd = 730, conf.width = 200, conf.level = 0.99) sample size for mean mean sd n conf.width conf.level lwr upr 1 3000 730 357.3903 200 0.99 2900 3100 In this case, we would require a sample size of n = 358. 6.8 Significance tests Significance tests are tools for evaluating scientific hypotheses. To explain the basic concepts involved in a significant test, let’s consider the following example. According to a National Vital Statistics Report, 8.31% of all births in the US in 2019 were low weight births (&lt;2500 grams). Suppose we suspect that this could be higher in the population of deliveries assisted in the Baystate Medical Center, and use the sample of births in the birthwt dataset to verify it. To conduct a significance test, we set a pair of conflicting formal statistical hypotheses called null and alternative, denoted \\(H_0\\) and \\(H_1\\) respectively. In this case, these hypotheses involve a single parameter \\(\\pi\\), that represents the proportion of low birthweight in the population of deliveries assisted in the Baystate Medical Center: \\(H_0: \\qquad \\pi = 0.0831\\) \\(H_1: \\qquad \\pi &gt; 0.0831\\) Note that the working hypothesis stated in the first `paragraph of this section (we suspect that this could be higher in the population of deliveries assisted in the Baystate Medical Center) is represented by the alternative hypothesis \\(H_1\\), the null hypothesis \\(H_0\\) being the opposite (i,e., the proportion of low birthweights in the population of deliveries assisted in the Baystate Medical Center is as in the whole US); so that \\(H_0\\) and \\(H_1\\) are mutually exclusive. In the birthwt dataset, the sample proportion of low birthweigh is 0.312. Now we compute the probability of getting a random sample of 189 births in which this proportion is 0.312 or more extreme in the direction of \\(H_1\\), if \\(H_0\\) was true. This probability is called p value, and in this case is &lt;0.0000001 (that is, less than one in ten million). Now, there are two possible explanations for this result (assuming the sample is a random sample): \\(H_0\\) is true, and something very unlikely happened when drawing the sample, or \\(H_0\\) is false, in which case, the computed p value does not reflect the actual probability of the sample, since it was computed assuming \\(H_0\\) is true). Clearly, the second explanation seems better, since it does not require to believe that something very unlikely happened. Therefore, a very unlikely result under \\(H_0\\), is taken as evidence against \\(H_0\\), and supports its rejection in favor of \\(H_1\\). Thus, in this case we conclude that the proportion of low birthweight in the population of deliveries assisted in the Baystate Medical Center is higher than that in the whole US. A difficulty with this argument is to decide what probability values should be considered very unlikely. It is usual to take values below 0.05 as unlikely enough to reject \\(H_0\\), but this is a choice we make as investigators, and there is nothing wrong with choosing a different, perhaps more strict value, like 0.01. In any case, the probability value below which we will reject a null hypothesis in a test is called the significance level of the test, is denoted by the Greek letter \\(\\alpha\\), and should be established before getting the sample data. In this book, we will use \\(\\alpha\\) = 0.05 unless otherwise stated. But whatever the value of \\(\\alpha\\) chosen a priori by an investigator, a p value &lt; \\(\\alpha\\) is used as argument to reject \\(H_0\\) and conclude \\(H_1\\). To compute the p value reported above, we used function binom.test(). This function is available both in base R and in the mosaic package, but here we use the later (by loading mosaic previously). The arguments we need to pass to binom.test() are the factor bw preceded by a tilde and the dataframe containing this factor, the value proposed by the null hypothesis for \\(\\pi\\) in argument p, the category for which we want to estimate the proportion in argument success, and the form of alternative hypothesis in argument alternative): binom.test(~ bw, data=d, p = 0.0831, # Null hypothesis success = &quot;low&quot;, # Category of bw we are interested in alternative = &quot;greater&quot;) # Alternative hypothesis data: d$bw [with success = low] number of successes = 59, number of trials = 189, p-value &lt; 2.2e-16 alternative hypothesis: true probability of success is greater than 0.0831 95 percent confidence interval: 0.2565953 1.0000000 sample estimates: probability of success 0.3121693 The results offered by binom.test() will always refer to the category indicated in its argument success (or to the first level of the factor if argument success is not used), and are identified in the output with the word “success”. The first line of the results includes the number of low cases (labeled as “number of successes”), the sample size (“number of trials”), and the p value in scientific notation. In a second line, the alternative hypothesis of the test is stated in words. In addition, the 95% CI for \\(\\pi\\) is provided, as well as the sample proportion (“probability of success”). Note that the CI excludes the value proposed by \\(H_0\\), which is consistent with the rejection of \\(H_0\\) implied by the extremely low p value. In summary, when we conduct a significance test we compute a p value, which is a measure of how consistent the data are with \\(H_0\\). A very low p value (p &lt; \\(\\alpha\\)) indicates that the data are very inconsistent with \\(H_0\\), leading to the rejection of \\(H_0\\). Conversely, a high p value reflects consistency of the data and \\(H_0\\), but by no means this can be taken as proof that \\(H_0\\) is true. Therefore, when p \\(\\ge \\alpha\\) we can neither reject \\(H_0\\), nor conclude it is true: this is an inconclusive result. Current guidelines recommend to report p values with two or three decimal places at most, so that the p value we computed previously should be reported as &lt;0.001. However, when a p value is greater than that, its actual value should be reported (e.g. p = 0.021, rather than p &lt; 0.05). 6.9 Sample size and p values The p value of a test depends on how different the point estimate is from the parameter value proposed by \\(H_0\\): the more different they are, the lower the p value. For instance, suppose that we conduct the binomial test of the previous section on a sample of 100 births, 10 of which are low weight. The sample proportion is then p = 0.1 (or 10%). In this case, the p value would be 0.319, implying that the result is not very inconsistent with \\(H_0\\). Indeed, a sample proportion of 0.1 is not very different from the population value proposed by \\(H_0: \\pi = 0.0831\\). Conversely, if the sample proportion was 0.2 (or 20%), which is quite different from 0.0831, the p value would be &lt;0.001. However, p values depend on the sample size as well. Table 6.1 shows the p values obtained in the binomial test for a series of hypothetical studies, all having a sample proportion of 0.10, but differing in sample size. Although all studies have the same sample proportion, p values decrease as the sample size increases. In studies with a sample size of 800 births or less, the p value does not allow to reject \\(H_0\\), but studies based on 850 or more births would lead to the its rejection, providing evidence that the population proportion is greater than 0.0831. Table 6.1: Effect of the sample size on the p value of a binomial test with sample proportion 0.1 (or 10%) n x p p-value 100 10 0.1 0.319 150 15 0.1 0.265 200 20 0.1 0.225 250 25 0.1 0.194 300 30 0.1 0.169 350 35 0.1 0.148 400 40 0.1 0.130 450 45 0.1 0.114 500 50 0.1 0.101 550 55 0.1 0.090 600 60 0.1 0.080 650 65 0.1 0.071 700 70 0.1 0.063 750 75 0.1 0.057 800 80 0.1 0.051 850 85 0.1 0.045 900 90 0.1 0.041 950 95 0.1 0.036 1000 100 0.1 0.033 The dependency of the p values on the sample size has two important implications. First, the p value (or, its “significance”), is not particularly informative in general. Rather, the point and CI estimates are much more informative. Sometimes it is said that different studies have conflicting results only because some of them report a “statistically significant” result (p &lt; 0.05) while others do not (p &gt; 0.05). However, if the point estimates are similar there is no conflict at all, and a different sample size could be enough to explain the apparent conflict. The second implication is that, when designing a study, we can choose a sample size that will produce a statistically significant result with high probability. This is called sample size determination, and is an important aspect of study design. The computation of the sample size for a particular study will depend on some study design characteristics, and on the test planned for the inferential analysis of its primary objective. 6.10 Types of tests There are lots of statistical tests available, and choosing the appropriate one for a particular problem may be challenging. Tests can be classified according to several criteria, such as the form of the null and alternative hypotheses, the parameter(s) involved, or the assumptions they are based on. Depending on the form of the null hypothesis, we distinguish one-sample, two-samples or many-samples tests. The general form of the null hypothesis for these three types of test is shown below, where \\(\\theta\\) is some parameter of interest, and \\(k\\) some specific value: One-sample tests \\(H_0: \\theta = k\\) Two-sample tests \\(H_0: \\theta_1 = \\theta_2\\) Many samples \\(H_0: \\theta_1 = \\theta_2 = ...= \\theta_S\\) In one-sample tests, a particular value is proposed for a parameter. This was the case of the binomial test addressed in the previous section, where the parameter was a population proportion (\\(H_0: \\quad \\pi = 0.0831\\)). Two-sample tests compare a parameter in two populations. For example, we could compare the proportion of low birthweight cases in the populations of smoker and non-smoker mothers, setting the null hypothesis \\(H_0: \\quad \\pi_{smoker} = \\pi_{non-smoker}\\). Many-samples tests compare a parameter in more than two populations. For example, we could compare the proportion of low birthweight cases in the populations defined by race, setting the null hypothesis \\(H_0: \\quad \\pi_{white} = \\pi_{black} = \\pi_{other}\\). Correspondingly similar examples would result by changing the parameter to a population mean (\\(\\mu\\)) of birth weights in grams. According to the form of the alternative hypothesis, we can distinguish two-sided or one-sided tests. In a two-sided test, \\(H_1\\) is a strict inequality. In a one-sided test, only one of the two directions (greater or lower) is of interest: Two-sided \\(H_1: \\theta \\ne k\\) One-sided \\(H_1: \\theta \\gt k \\qquad \\text{or} \\qquad H_1: \\theta \\lt k\\) The test conducted in the previous section was a one-sided test, since the alternative hypothesis was \\(H_1: \\quad \\pi &gt; 0.0831\\). We decided to set this alternative hypothesis because the working hypothesis was “we suspect that the proportion of low birthweight could be higher than in the whole US”. In general, one- and two-sided tests will produce a different p-value. For this reason it is important to be careful when setting \\(H_1\\). Unless the working hypothesis clearly states an expected direction and the other direction is of no interest at all, two-sided tests should be used. Some tests are based on assumptions about the data. In particular, many classic tests on means assume that the variable follows a normal distribution in the population(s). These are generically called parametric tests. Conversely, some other tests do not require such an assumption, and are called non-parametric or distribution-free tests. Last, tests are usually described by the type of parameter involved. When analyzing categorical variables, tests on proportions are usually performed. When analyzing quantitative variables, tests on means are most used. As stated at the beginning of this section, the number of significance tests available is endless (every day new tests are published in statistical journals), and any attempt to present a “complete” collection of tests covering all possible data analysis scenarios would be naive. Rather, in the following chapters, we will cover a selection of few tests commonly used in clinical research. Among them: Test for categorical variables: Binomial test and one-sample proportion z-test Independence Chi-square test McNemar’s test Parametric tests for quantitative variables: One-sample (or paired) Student’s t-test Two-sample (or unpaired) Student’s t-test Welch test Distribution-free tests for quantitative variables: Wilcoxon’s rank sum test or Mann-Whitney’s test Wilcoxon’s signed rank test Linear independence test on the Pearson’s correlation coefficient Linear independence test on the Spearman’s correlation coefficient Intercept and slope test of a regression line Resources An interesting read about common missinterpretations of p-values and confidence intervals. Journal collections devoted to statistical methods: Nature’s Points of Significance. BMJ’s Statistics notes. StatQuest is a youtube channel with several lists of videos, one of them devoted to Statistics fundamentals. Package presize includes functions to compute the required sample size to estimate many parameters other than a mean or a proportion. In addition, a point-and-click web app implementing these functions is available here. For a more detailed explanation of inferential statistics (covering both hypothesis testing, confidence intervals and how they relate to each other) see this R-bloggers post. Exercises What is the term used in inferential statistics to refer to population quantities of interest? What a are the two parameters of a normal distribution, and how they relate to its graphical representation? What type of sample is assumed by common inferential methods (such as those in these book and in many other books and courses on statistics)? What property of a sampling procedure allows to claim that the resulting sample is a simple random sample? What is the interpretation of a 95% confidence interval? What is a p value? What is the difference between two-sided and one-sided tests? What is the difference between parametric and non-parametric tests? A dataframe contains the hospital record numbers (HRN), and other variables, for a population of 8000 patients. Run the following code to simulate such a dataframe. Then look at the help of function sample_n() from package dplyr, and use it to randomly select 20 patients from dataframe d. In a study newborns conducted in a hospital of a different geographical area, only 2 out of 150 newborns were low weight. Look at the help of the binom.test() function in the mosaicpackage, and use it to compute a 95% CI using the Wald method. Then repeat using the default Clopper-Pearson method. You are designing a study to estimate the proportion of COVID-19 cases seen in the emergency department of a tertiary hospital who present dyspnea on admission. Determine the required sample size to get 95% CI of width 0.10 in three different scenarios: if you guess that this proportion is about 0.2 (or 50%). if you guess that this proportion is about 0.5 (or 50%). if you guess that this proportion is about 0.8 (or 50%). What value would you choose to determine the sample size if you have no idea what this value can be? Why? What is the sample size required to estimate the mean weight of newborns with 95% confidence, so that the CI width is 100 grams, assuming a SD of 730 grams and a mean of 3000 grams? What is the result if you assume a mean of 2000 grams? An what if you assume an SD = 800 grams? in some cases, statistical hypotheses refer to probabilistic models rather than simple parameters. However, most of the statistical hypotheses we are dealing with in this book do test hypotheses on parameters.↩︎ "],["analysis-of-categorical-data.html", "7 Analysis of categorical data 7.1 One-sample tests 7.2 Independent samples 7.3 Paired samples Resources Exercises", " 7 Analysis of categorical data Categorical data is extremely common in medical research, particularly dichotomous variables like cured/not-cured, success/failure, or presence/absence of symptoms. Consequently, the methods for the analysis of categorical data are used in almost all clinical research projects. In this chapter we present classical methods for the analysis of categorical variables, classifying them according to the type of statistical problem they solve: one-sample problems, where we focus on the observed distribution of a single variable, and compare it to some theoretical distribution. two-sample problems, where two variables are involved, and the purpose is to compare distributions observed in either: independent samples, when the distributions to be compared come from different groups of subjects, or paired samples, when the distributions to be compared come from the same group of subjects. Although the primary objective and the design of a study both determine the type of statistical problem posed by the primary objective, we can find all three cases listed above in the same study (possibly related to secondary objectives). Figure 7.1 illustrates the three types of problem in terms of the data involved, for a hypothetical clinical trial in which patients are randomized to receive one of two treatments, A or B, for seven days, and coughing is recorded as yes or no at baseline (day0), and at the end of treatment (day7). The estimation of the coughing prevalence at baseline is a one-sample problem (left); the comparison of treatments A and B in terms of the coughing frequency at day 7 is a two-sample problem with independent samples; and the assessment of change in the coughing frequency from baseline to end of treatment is a two-sample problem with paired samples. Table 7.1: One-sample (left), two independent samples (center), and two paired samples (right) group day0 day7 A yes yes A yes yes A yes no A no no A yes yes A no no A no no A yes no B yes no B yes no B yes no B yes no B yes yes B yes no B no no B yes no group day0 day7 A yes yes A yes yes A yes no A no no A yes yes A no no A no no A yes no B yes no B yes no B yes no B yes no B yes yes B yes no B no no B yes no group day0 day7 A yes yes A yes yes A yes no A no no A yes yes A no no A no no A yes no B yes no B yes no B yes no B yes no B yes yes B yes no B no no B yes no 7.1 One-sample tests In the previous chapter we introduced the binomial test, which is an exact test for a proportion that makes no assumptions about the data. But there is another test commonly used and explained in many books and courses, known as the one-sample proportion z-test. This is an approximate test based on the normal distribution, simpler to compute, particularly i large samples. However, the approximation is not always reliable, and should not be used for extreme proportion values (i.e., close to zero or one). A rule of thumb to determine the applicability of this test is to verify that the products \\(n \\ \\pi\\) and \\(n \\ (1-\\pi)\\) are both at least 5. If this is the case, the test is reliable, but otherwise it is not. The one-sample proportion z-test can be used to test the same null hypothesis we tested in the previous chapter by a binomial test, i.e., \\(H_0: \\quad \\pi = 0.0831\\), becasue both \\(n \\ \\pi = 189 \\times 0.0831 = 15.7\\) and \\(n \\ (1-\\pi) = 189 \\times (1-0.0831) = 173.3\\) are well above five. This test can be done with function prop.test(), available in both base R and in the mosaic package. Here we use the later, by explicitly prefixing the function with the package name and a double colon. The arguments of this function are very similar to those in binom.test(): mosaic::prop.test(~ bw, success = &quot;low&quot;, p=0.0831, data=d) 1-sample proportions test with continuity correction data: d$bw [with success = low] X-squared = 127.17, df = 1, p-value &lt; 2.2e-16 alternative hypothesis: true p is not equal to 0.0831 95 percent confidence interval: 0.2479596 0.3841585 sample estimates: p 0.3121693 The output, and in particular the p-value, is similar to what we obtained when we used the binomial test (see previous chapter), leading to the same conclusion: the rejection of the null hypotesis on teh basis of an extremely low p value. 7.1.1 Chi-square goodness-of-fit (GOF) test The chi-square GOF test is a one-sample test that compares the observed distribution of a categorical variable with \\(k \\ge\\) 2 levels to a theoretical distribution for this variable. The theoretical distribution is specified by the probabilities of the \\(k\\) levels, and may come from conceptual models or from external data, like national statistics. For instance, according to 2019 data from the National Vital Statistics Reports, the split of all births in the US by mother race was 51.1% white, 14.6% black and 34.3% other. We may want to test the null hypothesis that this split is the same in the population of births assisted in the Baystate Medical Center: \\[H_0: \\quad \\pi_{white} = 0.511, \\quad \\pi_{black} = 0.146, \\quad \\pi_{other} = 0.343\\] According to this null hypothesis, the expected number of births for each race in a sample of 189 births would be: white: 0.511 times 189 = 96.58 black: 0.146 times 189 = 27.59 other: 0.343 times 189 = 64.83 These expected numbers look quite similar to those actually observed: race_ft &lt;- mosaic::tally(~race, data = d) race_ft race white black other 96 26 67 To judge how different are the observed counts from those expected under the null hypothesis, we could compute their difference for each race category and add them up, but unfortunately this will be always zero due to the sign of the differences. To avoid this problem, the differences are squared and divided by the expected count before adding up, and this is called the Pearson’s chi-square statistic: \\[\\chi^2 \\quad = \\quad \\sum\\limits_{i=1}^{k} \\ \\frac{(O_i - E_i)^2}{E_i}\\] where \\(O_i\\) and \\(E_i\\) are the observed and expected counts in the \\(i\\)-th category, and the summation is taken over all categories (\\(i = 1, 2, ..., k\\)). For the race of mothers in the births data, the computation is as follows: \\[\\chi^2=\\frac{(96-96.58)^2}{96.58} + \\frac{(26-27.59)^2}{27.59} + \\frac{(67-64.83)^2}{64.83} = 0.168\\] Perhaps you wonder why the (squared) difference in each table cell is divided by the expected count. Well, this is to judge the deviation per expected case, since a given difference may be modest if many cases are expected in that category (which depends on the sample size), but huge if few cases are expected. In any case, this statistic is basically a function of the difference between observed and expected counts, and is interpreted as a measure of distance between the observed distribution and the distribution expected according to \\(H_0\\). The lowest possible value is 0, if the observed and expected counts are exactly equal, and it can only grow as the observed counts diverge from those expected. In the example above, the value of this statistic is very close to zero, indicating that the observed distribution of race is very consistent with the null hypothesis. The chisq.test() function in base R computes the Pearson’s chi-square statistic and a p-value for the GOF test. This function needs at least two arguments: a frequency table as first argument, and a further argument p, a numeric vector containing the theoretical probabilities for all the categories ordered as in the frequency table (white, black, other). chisq.test(race_ft, p = c(0.511, 0.146, 0.343)) # probs for white, black, other Chi-squared test for given probabilities data: race_ft X-squared = 0.16839, df = 2, p-value = 0.9193 The output informs this is a chi-square test on given probabilities (those we specified in argument p), and provides the value of the chi-square statistic, its degrees of freedom (\\(k-1\\) for a variable with \\(k\\) categories), and the p-value. In this case, the chi-square value is very low, reflecting the similarity of the counts observed and expected if \\(H_0\\) was true; and the p-value is very high, indicating that the observed distribution of race is very consistent with the theoretical probabilities. The mosaic package provides a similar function, xchisq.test(), that can be passed the frequency table as first argument, just as in chisq.test(), but also accepts a formula, which is handy, since there is no need to compute the frequency table first. In addition, the output is more verbose, and shows the expected counts: mosaic::xchisq.test(~ race, p = c(0.511, 0.146, 0.343), data = d) Chi-squared test for given probabilities data: x X-squared = 0.16839, df = 2, p-value = 0.9193 96 26 67 (96.58) (27.59) (64.83) [0.0035] [0.0921] [0.0728] &lt;-0.059&gt; &lt;-0.303&gt; &lt; 0.270&gt; key: observed (expected) [contribution to X-squared] &lt;Pearson residual&gt; For the chi-square GOF test to be valid, the expected counts should not be too low. It is generally considered that when some expected count is less than 5, the test may be unreliable. If this is the case, a warning is printed in the output stating “the chi-squared approximation may be incorrect”. 7.2 Independent samples Independent samples (or independent distributions, or independent data) arise in many study designs, like parallel group clinical trials, concurrent or historical cohorts, case-control studies or even in single cohort studies when cohort subgroups defined by some variable (like gender) have to be compared. Suppose we want to investigate if the probability of low birth weight depends on smoking during pregnancy, and we look at the contingency table of bw and smoking to this end. Here we produce the contingency table, and the table of percentages (rounded to one decimal) with argument margins=TRUE to get column totals and make it clear that percentages are column percentages: bw_smoke &lt;- mosaic::tally(bw ~ smoke, data = d) bw_smoke smoke bw smoker non-smoker low 30 29 normal 44 86 mosaic::tally(bw ~ smoke, data = d, format = &quot;percent&quot;, margins = TRUE) %&gt;% round(1) smoke bw smoker non-smoker low 40.5 25.2 normal 59.5 74.8 Total 100.0 100.0 In the study sample, low birth weight is more frequent in smoker mothers (40.5%) than in non-smokers (25.2%). To see how compatible is this result with the possibility that these proportions are equal in the population, we can use one of the following methods: Association measures (or effect measures): Risk difference (RD) Relative risk (RR) Odds ratio (OR) Significance tests: Two-sample test for proportions Chi-square independence test Fisher’s test Association measures approach the problem from an estimation perspective: we devise a measure of how different is the proportion of low birth weight in smokers and non-smokers, and we estimate this measure in the population by computing a confidence interval (e.g., the 95% CI). In significance tests, we compute a p-value to evaluate the null hypothesis that the population proportion of low weight birth (\\(\\pi_{low}\\)) is the same for smokers and non-smokers: \\[H_0: \\pi_{low|smoker} = \\pi_{low|non-smokers}\\] While all three tests mentioned above evaluate this same null hypothesis, the way in which the p-value is computed differs (though as we will see, the first two provide identical p-value). 7.2.1 Association measures The association measures listed in the previous section are statistics that summarize the results of a 2x2 contingency table generated by two dichotomous variables. These measures are simple, easy to interpret, and can be used to assess the relationship of the two variables involved. For example, consider the results of birth weight according to smoking status: smoker non-smoker low 30 29 normal 44 86 Total 74 115 Because bw is a dichotomous variable, we can summarize its distribution in smokers and non-smokers with the proportion of just one of its levels (low, say) since the proportion of the other level (normal) is complementary to one. So, let’s concentrate on the proportions of low shown in table 7.2: Table 7.2: Proportion of low weight smoker non-smoker P(low) 0.405 0.252 To compare the results in smokers and non-smokers, we can compute the proportion difference or Risk Difference \\[RD \\quad = \\quad P(low|smoker) - P(low|non\\mbox{-}smoker) \\quad = \\quad 0.405 - 0.252 \\quad = \\quad 0.153\\] Clearly, the RD would be zero if both proportions were equal, but in this case RD = 0.153, and the interpretation of this value is straightforward: the proportion of successes is 0.153 higher in smokers than in nonsmokers. It is quite common to express the results in percentage form, e.g., 40.5% and 25.2%, and also the risk difference (15.3%). Alternatively, the results in the two groups can be compared by their ratio, which is called Relative Risk (RR): \\[RR \\quad = \\quad \\frac{P(\\text(low|smoker)}{P(low|non\\mbox{-}smoker)} \\quad = \\quad \\frac{0.405}{0.252} \\quad = \\quad 1.607\\] The RR would be one if both proportions were equal, but in this case RR = 1.607, implying that the proportion of low weight births in smokers is 1.607 times that of non-smokers (a 60.7% increase in relative terms). The RD and the RR are measures derived from the proportions of low weight births shown in table 7.2, and proportions (or percentages) are the most common way to summarize results of dichotomous variables. However, such results can be summarized just as well using odds rather than proportions. The odds (O) of an event is defined as the ratio of its probability (\\(\\pi\\)) to the complementary probability, that is \\[O \\quad = \\quad \\frac{\\pi}{1-\\pi} \\qquad\\] For any smoking status, the odds of low weight can be computed as the ratio of its corresponding proportion over the complementary proportion, or equivalently, as the ratio of the low vs normal weight counts (since the denominators of the proportions are the same and cancel out). For instance, in the case of smokers, the odds of low weight is: \\(O(low|smoker) \\quad = \\quad \\frac{P(\\text(low|smoker)}{1-P(low|smoker)} \\quad = \\quad \\frac{0.405}{1-0.405} \\quad = \\quad \\frac{\\frac{30}{74}}{\\frac{44}{74}} \\quad = \\quad \\frac{30}{44} \\quad = \\quad 0.681\\) The odds of low weight in smokers and non-smokers are show in table 7.3. Table 7.3: Odds of low weight births smoker non-smoker O(low) 0.681 0.337 These two odds can be compared by computing their ratio, obtaining an odds ratio (OR): \\[OR \\quad = \\quad \\frac{O_{low|smoker}}{O_{low|non\\mbox{-}smoker}} \\quad = \\quad \\frac{0.681}{0.337} \\quad = \\quad 2.02\\] The OR would be one if the odds were equal in smokers and non-smokers, but in this case OR = 2.02, implying that the odds of low weight is twice as high in smokers as in non-smokers. It is quite common to say that the OR is the cross product of counts in a contingency table. Indeed, this crossproduct produces the same result as the definition provided above: \\[ OR \\quad = \\quad \\frac{O_{low|smoker}}{O_{low|non\\mbox{-}smoker}} \\quad = \\quad \\frac{\\frac{30}{44}}{\\frac{29}{86}} \\quad = \\quad \\frac{30 \\times 86}{29 \\times 44} \\quad = \\quad 2.02\\] However, although the cross product is an easy way to calculate an OR, it does not reflect its definition of an OR, which is nothing but the ratio of two odds, and serves the purpose of comparing them. The following table summarizes the four measures presented above, and their fundamental properties: the theoretical range and the value that implies equality of results in the two groups compared. Measure Formula Equality Theoretical range RD \\(P_E - P_C\\) 0 -1 to +1 RR \\(P_E / P_C\\) 1 0 to \\(\\infty\\) OR \\(O_E / O_C\\) 1 0 to \\(\\infty\\) All these measures can be computed form a 2x2 contingency table, but it is important to be clear about how we compute them. First, what is the outcome variable, and which of its levels we want to compute the measures for (above we did it for the low level of the bw variable, but we could have done it for the normal level). Second, how we define the difference (RD) or the ratio (RR and OR) for the comparison. In the examples above, we defined the RD as smoker minus non-smoker, and the RR and OR as smoker over non-smoker. You should be aware that the value obtained, and its interpretation, will depend on how we computed it. For this reason, when reporting such measures we need to be explicit about their meaning, such as in the following sentences: The RD of low birth weight (smokers minus non-smokers) was 0.153 (or 15.3%). The RR of low birth weight (smokers over non-smokers) was 1.07. The OR of low birth weight (smokers over non-smokers) was 2.02. Several functions/packages in R compute some or all of these measures and their corresponding CI: prop.test() computes the CI for the RD (here called proportion difference), though does not compute the RD itself; the fisher.test() function computes the OR and its CI; the mosaic package has functions oddsRatio() and relrisk() to compute the OR and the RR, respectively; and function twoby2() from the Epi package computes the RD, RR, and OR at once. The use of these functions is shown below. Note that all of them take a 2x2 contingency table as argument, and you need to be aware of how they compute the different association measures from this table. For instance Epi::twoby2() will assume that the outcome variable is in the columns of the table, the category of interest is the first column, and the comparison will be done by computing the first row minus (or over) the second row. However, the contingency table we computed previously is not formed like this: bw_smoke smoke bw smoker non-smoker low 30 29 normal 44 86 Therefore, before using the twoby2() function, we need to transpose the table, so that columns become rows and viceversa. This can be done with function t() (for transpose) that can be chained with the pipe: bw_smoke %&gt;% t() bw smoke low normal smoker 30 44 non-smoker 29 86 This is the table needed for Epi::twoby2(), that can be chained further: res &lt;- bw_smoke %&gt;% t() %&gt;% Epi::twoby2() 2 by 2 table analysis: ------------------------------------------------------ Outcome : low Comparing : smoker vs. non-smoker low normal P(low) 95% conf. interval smoker 30 44 0.4054 0.3001 0.5203 non-smoker 29 86 0.2522 0.1812 0.3394 95% conf. interval Relative Risk: 1.6076 1.0578 2.4433 Sample Odds Ratio: 2.0219 1.0807 3.7831 Conditional MLE Odds Ratio: 2.0141 1.0288 3.9649 Probability difference: 0.1532 0.0176 0.2871 Exact P-value: 0.0362 Asymptotic P-value: 0.0276 ------------------------------------------------------ The first part of the output is very useful to review if the calculations were done as intended, because it shows the contingency table and then the proportions of the first column, P(low) (and its 95% CI) for every row. Then the RR, the OR (two versions, the first one is the one we computed above), and the RD labeled as Probability difference, are shown along with their 95% CI. The result of this function is a list, and we can extract the association measures from the measures element of the list. Because this element is matrix, we can subset it to drop the third row (using the subsetting operator[-3,]), and round the results to 3 decimals for better readability: res$measures # get the measures element (a matrix) 95% conf. interval Relative Risk: 1.6076421 1.05781242 2.4432623 Sample Odds Ratio: 2.0219436 1.08065960 3.7831115 Conditional MLE Odds Ratio: 2.0141372 1.02878038 3.9649039 Probability difference: 0.1532315 0.01757833 0.2871171 res$measures[-3,] %&gt;% # drop third row x round(3) # and round to 3rd decimal 95% conf. interval Relative Risk: 1.608 1.058 2.443 Sample Odds Ratio: 2.022 1.081 3.783 Probability difference: 0.153 0.018 0.287 Finally, note that all the CI’s exclude the value of the corresponding measure that implies an equal proportion of low weight births in smoker and non-smoker mothers: the IC`s for both the RR and the OR exclude the value 1, and the CI for the RD excludes the value 0. Any of these could be used to realize that the data are not compatible with the hypothesis of an equal population proportion of low weight births in smoker and non-smoker mothers. 7.2.2 Two-sample test for proportions This test can be computed using function prop.test() from the mosaicpackage, but now we need to specify the outcome variable before the tilde, and the grouping variable after it. The remaining arguments are as in previous uses of this function. Beware that there is also a prop.test() function in base R, but it does not accept a formula. To make clear we are using the one from mosaic, we prefix the function with the package name. mosaic::prop.test(bw ~ smoke, success = &quot;low&quot;, data=d) 2-sample test for equality of proportions with continuity correction data: tally(bw ~ smoke) X-squared = 4.2359, df = 1, p-value = 0.03958 alternative hypothesis: two.sided 95 percent confidence interval: 0.004967192 0.301495793 sample estimates: prop 1 prop 2 0.4054054 0.2521739 The output title indicates that these are results for a 2-sample test for the equality of proportions, with continuity correction7. Then, the value of a test statistic8 (X-squared), its degrees of freedom (df) and the resulting p-value are provided, and we are informed that the test was conducted with a two-sided alternative hypothesis. Then the 95%CI for the difference in proportions is given, as well as the sample proportions (though their difference is not shown). The small p-value (0.03958) indicates that the observed difference of proportions of low weight births in smoker and non-smoker mothers (0.4054054 - 0.2521739 = 0.1532315) is inconsistent with the null hypothesis stating that these proportions are equal in the population. In fact, the 95%CI for this difference is [0.0049672, 0.3014958], ruling out the possibility that the difference is zero in the population. 7.2.3 Chi-square independence test The chi-square test of independence allows to test a null hypothesis of independence in a contingency table. We say that two variables are independent when the distribution of one of them is the same for all values of the other. For instance, if bw is independent from smoke, then the relative frequency of low weight and normal births should be the same in smoking and in non-smoking mothers. When the contingency table has four cells (i.e., the two variables generating the table are dichotomous), the chisquare independence test is equivalent to the two-sample test for proportions discussed in the previous section, producing exactly the same p value. This test can be done with function chisq.test(), by passing it the contingency table as done below (bw_smoke defined in a previous script above). Alternatively, the two factors generating the table could be passed as arguments (see ?chis.test for details): chisq.test(bw_smoke) Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: bw_smoke X-squared = 4.2359, df = 1, p-value = 0.03958 The output informs that this is a Pearson’s chi-square test with Yates’ continuity correction (just as in the two-sample proportions test), and provides the value of the chi-square statistic, its degrees of freedom, and the p-value. Note that these results are exactly the same we obtained in the two-sample proportions test of the previous section. However, the chi-square test is more general, since it can be used to test the independence of two categorical variables with any number of levels each, that is to contingency tables with any number of rows and columns. Thus this test can be used to: compare the distribution of a dichotomous variable in \\(g \\ge\\) 2 groups, compare the distribution of a categorical variable with \\(k \\ge\\) 2 levels in 2 groups, or compare the distribution of a categorical variable with \\(k \\ge\\) 2 levels in \\(g \\ge\\) 2 groups For instance, let`s consider the relationship of low birth weight and race. Figure 7.1 suggests that low weight is less common among white mothers than among black mothers or other races. ggformula::gf_props(~ race, fill = ~ bw, data = d, position=&quot;fill&quot;) + coord_flip() Figure 7.1: Birth weight by race The contingency table of birth weight and race is shown below, extended with the marginal distributions: race bw white black other Total low 23 11 25 59 normal 73 15 42 130 Total 96 26 67 189 Now, let’s see what are the counts expected according to the null hypothesis of independence of birth weight and race. These can be computed form the marginal distributions shown in table 7.4: Table 7.4: Marginal distributions white black other Total low 59 normal 130 Total 96 26 67 189 From the margins in table 7.4, the overall proportion of low weight births is 59/189 = 0.312. Since an independence pattern should have this same overall proportion in all three race groups, we can apply it to the number of cases in each race group to compute the counts expected by the null hypothesis of independence: white: 96 \\(\\times\\) 59/189 = 29.97 black: 26 \\(\\times\\) 59/189 = 8.12 other: 67 \\(\\times\\) 59/189 = 20.91 And similarly for normal weight births, whose overall proportion is 130/189 = 0.688: white: 96 \\(\\times\\) 130/189 = 66.03 black: 26 \\(\\times\\) 130/189 = 17.88 other: 67 \\(\\times\\) 130/189 = 46.08 Now let’s compare the observed counts to those expected in case of independence: Table 7.5: Observed (left) and expected (right) counts white black other low 23 11 25 normal 73 15 42 white black other low 29.97 8.12 20.92 normal 66.03 17.88 46.08 The Pearson’s chi-square statistic measures how distant are the observed counts from those expected in case of independence. Here is the general formula, for a contingency table of \\(r\\) rows and \\(c\\) columns:9: \\[\\chi^2=\\sum\\limits_{i=1}^{r \\times c} \\frac{(O_i - E_i)^2}{E_i}\\] where \\(O_i\\) and \\(E_i\\) are the observed and expected counts in the \\(i\\)-th table cell, respectively, and the summation is taken over all table cells (\\(i = 1, 2, ..., r \\times c\\), in a table with \\(r\\) rows and \\(c\\) columns). From its formula, its is clear that the lowest possible value of the Pearson’s statistic is 0 (when the observed and expected counts are exactly equal), and it grows as observed counts move away from those expected. For the contingency tableof bw and race, the value of the Pearson’s chi-square statistic is: \\[\\chi^2=\\frac{(23 - 29.97)^2}{29.97} + \\frac{(73 - 66.03)^2}{66.03} + \\frac{(11 - 8.12)^2}{8.12} + \\qquad \\qquad\\\\ \\frac{(15 - 17.88)^2}{17.88} + \\frac{(25 - 20.92)^2}{20.92} + \\frac{(42 - 46.08)^2}{46.08} = 5.00\\] To judge this value as low or high, we need to take into account the dimension of the table, since a table with very many cells will tend to produce higher values than a table with few cells, just because there are more terms in the sum. However, it is not the number of cells what is used as reference, but the degrees of freedom (\\(df\\)), which is the number of rows minus 1, times the number of columns minus one, that is: \\[df = (r-1) \\times (c-1)\\] In this case, the table has two rows and three columns, so that \\(df = (2-1) \\times (3-1) = 2\\). Roughly, a \\(\\chi^2\\) value above twice its degrees of freedom can be considered too high. In this case \\(\\chi^2 = 5.00\\) , which is a bit high for two degrees of freedom, but not that much. All these computations can be made with the same function we used for the chi-square GOF test in section 7.1.1, with the only difference that now we pass a contingency table as argument instead of a frequency table: mosaic::xchisq.test(bw_race) Pearson&#39;s Chi-squared test data: x X-squared = 5.0048, df = 2, p-value = 0.08189 23 11 25 (29.97) ( 8.12) (20.92) [1.62] [1.02] [0.80] &lt;-1.27&gt; &lt; 1.01&gt; &lt; 0.89&gt; 73 15 42 (66.03) (17.88) (46.08) [0.74] [0.46] [0.36] &lt; 0.86&gt; &lt;-0.68&gt; &lt;-0.60&gt; key: observed (expected) [contribution to X-squared] &lt;Pearson residual&gt; The p-value is low, but not low enough to provide definitive evidence that the frequency of low weight births differs across races. As in the case of the chi-square GOF test, the chi-square test of independence may not be valid if there are low expected counts (below five, say). Expected counts are shown in the output above (within brackets, inmediately below the observed), and are all greater than five, so that in this case we can be confident on the validity of the test. But let’s see what happens when using this function to investigate the relationship of bw and ht. We first have a look to the contingency table: bw_ht &lt;- mosaic::tally(bw ~ ht, data = d) bw_ht ht bw AHT no-AHT low 7 52 normal 5 125 The limited number of patients with AHT (7+5 = 12) makes us anticipate low expected frequencies for this table cells, but let’s try a chi-square test: mosaic::xchisq.test(bw_ht) Warning in chisq.test(x = x, y = y, correct = correct, p = p, rescale.p = rescale.p, : Chi-squared approximation may be incorrect Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: x X-squared = 3.1431, df = 1, p-value = 0.07625 7 52 ( 3.75) ( 55.25) [2.025] [0.137] &lt; 1.68&gt; &lt;-0.44&gt; 5 125 ( 8.25) (121.75) [0.919] [0.062] &lt;-1.13&gt; &lt; 0.29&gt; key: observed (expected) [contribution to X-squared] &lt;Pearson residual&gt; As suspected, we are warned that the test may be incorrect due to a low expected count (3.75) for the top-left table cell. In such a case, a better alternative is the Fisher’s exact test we introduce in the next section. 7.2.4 Fisher’s exact test This test works well when expected counts are low. It is implemented by function fisher.test(), which also needs the contingency table as argument: fisher.test(bw_ht) Fisher&#39;s Exact Test for Count Data data: bw_ht p-value = 0.05161 alternative hypothesis: true odds ratio is not equal to 1 95 percent confidence interval: 0.8679484 13.9894703 sample estimates: odds ratio 3.340866 The output includes the p-value for the null hypothesis of independence, which for 2x2 tables is equivalent to the hypothesis that the odds ratio equals one. This p-value is computed from the probabilities of all possible tables with the same marginal distributions, and no statistic is involved in this computation. Also, the conditional Maximum Likelihood Estimate of the OR is provided, with corresponding 95% CI. 7.3 Paired samples Paired samples (or paired distributions, or paired data) arise when a characteristic is assessed twice in the same set of subjects, maybe at different times (e.g., before and after an intervention), by different methods or observers, or in symmetrical body structures (e.g., eyes, limbs). As an example, consider the following data from the Optimcare study, that compared the physician’s opinion on the need of lipid lowering drug treatment (LLD) in 500 hypercholesterolemic patients to the recommendation made by a clinical decision support system (CDSS). Here are the first six rows of the data: Case CDSS Physician 1 1 No Yes 2 2 Yes Yes 3 3 No Yes 4 4 Yes Yes 5 5 Yes Yes 6 6 No No Download Optimcare data This type of data allows to investigate two different research questions of potential interest: Do the physician and the CDSS have the same tendency to prescribe LLD? To what extent they both agree on the need of LLD treatment? The first of these questions is addressed by the McNemar’s test, and the second by a measure of agreement known as Cohen’s kappa. 7.3.1 McNemar’s test Suppose we are interested in testing the null hypothesis of equal probability of prescribing LLD drugs (denoted by \\(\\pi\\)) by the doctor and the CDSS: \\[H_0: \\qquad \\pi_{physician} \\quad = \\quad \\pi_{CDSS}\\] First, let’s look at the contingency table generated by the two variables in the dataset, Physician and CDSS, extended with the table margins: Table 7.6: Contingency table with margins CDSS Physician No Yes Total No 209 4 213 Yes 57 230 287 Total 266 234 500 The table margins show the frequency of each oppinion (yes or no) by the physician (row totals) and by the CDSS (column totals). The proportion of cases needing LLD can be computed from them, and they are 287/500 and 234/500 for the Physician and the CDSS, respectively. These proportions will be different as long as the margins are different, because the two proportions have the same denominator (500). Moreover, each margin is the sum of counts in two table cells: \\[\\begin{aligned} \\text{Physician:} \\qquad &amp; \\frac{57+230}{500} \\quad = \\quad \\frac{287}{500} \\quad = \\quad 0.574\\\\\\\\ \\text{CDSS:} \\qquad &amp; \\frac{4+230}{500} \\quad = \\quad \\frac{234}{500} \\quad = \\quad 0.468 \\end{aligned}\\] Because the numerator of the two proportions involve 230 cases where both the physician and the CDSS considered that LLD were needed, the resulting proportions will be different only when the discrepant cells (Yes-No and No-Yes) have a different count. Therefore, the null hypothesis above could be tested by verifying if the two types of discrepancy are equally likely: \\[H_O: \\qquad \\pi_{yes-no} \\quad = \\quad \\pi_{no-yes}\\] So, the problem can be reformulated by focusing on discrepancies and ignoring the cases where both the physician and he CDSS gave the same answer, which are not informative on a possible difference between them. The following script creates a new variable phy_cdss with the two opinions when these are discrepant, and then subsets the data retaining discrepant cases only: d %&gt;% mutate(phy_cdss = ifelse(Physician == CDSS, NA, paste(Physician, CDSS, sep=&quot;-&quot;))) %&gt;% na.omit() -&gt; discrepancies head(discrepancies) # A tibble: 6 × 4 Case CDSS Physician phy_cdss &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1 No Yes Yes-No 2 3 No Yes Yes-No 3 33 No Yes Yes-No 4 38 No Yes Yes-No 5 41 No Yes Yes-No 6 46 No Yes Yes-No Let’s look now at the frequency table of the new variable phy_cdss, which is the type of discrepancy: mosaic::tally(~ phy_cdss, data = discrepancies) phy_cdss No-Yes Yes-No 4 57 If the previous (reformulated) null hypothesis was true, we would expect both types of discrepancies to be equally likely. Note that this is a one sample problem that can be solved with a GOF chi-square test with theoretical probabilities equal to 0.5: mosaic::tally(~ phy_cdss, data = discrepancies) %&gt;% chisq.test(p = c(0.5, 0.5)) Chi-squared test for given probabilities data: . X-squared = 46.049, df = 1, p-value = 1.153e-11 So, we have reformulated a paired-samples problem involving variables Phsician and CDSS, into a one-sample problem involving variable phy-cdss. This is actually what the McNemar’s test does: a GOF test on the subset of discrepant results. For convenience, the mcnemar.test() function conducts this test on the contingency table of the two original variables (so that we need neither to compute the new variable with the type of discrepancy, nor to subset the discrepant cases). If the test is done without continuity correction (using option correct = FALSE) the result is identical to the previous GOF test: mcnemar.test(cdss_phys, correct = FALSE) McNemar&#39;s Chi-squared test data: cdss_phys McNemar&#39;s chi-squared = 46.049, df = 1, p-value = 1.153e-11 In the output, the McNemar’s chi-square value is the usual Pearson’s chi-square statistic computed with expected counts equal to half the overall number of discrepancies. In this case, 46.049 is a very high value for a single degree of freedom, and the p-value is extremely low, providing strong evidence of marginal heterogeneity in the contingency table, that is, different probability of prescribing LLD by the Physician and the CDSS. The test does not tell us who prescribes LLD more often, but this is quite obvious looking at the most common type of discrepancy (Physician Yes and CDSS No: 57 cases), or at the margins of the contingency table. In summary, the McNemar’s test compares the marginal distributions of a contingency table (this is why it is sometimes called marginal homogeneity test), and is equivalent to a chi-square GOF test, conducted with discrepant cases only, to test that the two types of discrepancy are equally likely. As is the case with other chi-square based tests, the McNemar’s test is valid provided the expected counts are not too low (below 5, say). Because the expected counts in a McNemar test are always half the overall number of discrepancies, the test will be valid when this overal number is at least 10. 7.3.2 Cohen’s kappa When a variable is evaluated by different methods or observers, we may want to quantify the extent to which they agree. This was the case in the Optimcare study. The simplest and more natural measure of agreement is the observed agreement, which is nothing but the proportion of cases for whom both methods agree. From table 7.6, the observed agreement is computed as: \\[P_o \\quad = \\quad \\frac{209+230}{500} \\quad = \\quad 0.878\\] So, the physician and the CDSS agreed in 87.7% of the cases. However, a certain amount of agreement can be expected just by chance, and this can be quantified using the counts expected in case of independence: res &lt;- chisq.test(cdss_phys) res$expected %&gt;% round(1) CDSS Physician No Yes No 113.3 99.7 Yes 152.7 134.3 From these, the expected agreement (the agreement expected just by chance) is computed as: \\[P_e \\quad = \\quad \\frac{113.3+134.3}{500} \\quad = \\quad 0.495\\] Then, the kappa statistic is computed as the difference between the observed and the expected agreement, over one minus the expected agreement: \\[\\kappa \\quad = \\quad \\frac{P_o - P_e}{1-P_e}\\] Figure 7.2 represents the observed amount of agreement as a vertical bar in a scale ranging from 0 to 1 (the theoretical range for the observed agreement), the agreement beyond chance (\\(P_o-P_e\\)) and the maximum possible agreement beyond chance (\\(1-P_e\\)). The last two are the numerator and denominator of \\(\\kappa\\). Therefore, the kappa value measures the agreement beyond change as a fraction of the maximum possible agreement beyond chance. Thus, \\(\\kappa = 0\\) if the observed agreement is equal to the expected agreement (i.e., when there is no agreement beyond chance), and \\(\\kappa = 1\\) when there is perfect agreement (\\(P_o = 1\\)). Figure 7.2: Graphical expression of observed and expected agreement In the Optimcare study, the value of the kappa statistic is: \\[\\kappa \\quad = \\quad \\frac{0.878 - 0.495}{1-0.495} \\quad = \\quad 0.758\\] Which is a moderate agreement. The kappa statistic may be computed using function epi.kappa() from package epiR, which takes the contingency table as argument. The result is a list including these elements, among others: prop.agree, containing the observed and expected agreement; and kappa, containing the kappa statistic and 95% CI. res &lt;- epiR::epi.kappa(cdss_phys) res$prop.agree obs exp 1 0.878 0.495264 res$kappa est se lower upper 1 0.7582895 0.04372403 0.672592 0.843987 Resources A Student’s guide to R is a book on introductory and intermediate statistics with R. Chapters 4 and 6 are devoted to the analysis of categorical variables. If your categorical variables are ordinal, you may want to know about polychoric correlation. Another correction of the Pearson’s chi-square statistic when expected counts are too low is discussed here Compute many different statistics from a 2-by-2 contingency table with this online calculator. Birth weight info: The primary source of the birthwt data is the book by Hosmer, D and S. Lemeshow (2000), Applied Logistic Regression, Wiley. Births, by birth weight in the United States (table 22). In: National Vital Statistics Reports, Vol. 70, No. 2, March 23, 2021. A very nice example of the Chi-square GOF test in this short post. Exercises The odds of an event is defined as the proportion of cases presenting the event (p) over its complementary: O = p / (1-p). Solve this equation for p and use your result to compute: The proportion of cases with an event whose odds is 3 The proportion of cases with an event whose odds is 1/3 According to the National Vital Statistics Report 2019, 1.38% of all births in the US are very low weight births (&lt;2500 gr). Is this the case in the study population of the MASS::birthwt data? The following table shows the number of deliveries assisted in a Spanish General Hospital during one year, according to the lunar phase. births Ful moon 590 First quarter 572 New moon 542 Second quarter 565 Does this data support the hypothesis that the moon phase has an influence on births? What is the appropriate test? What is the null hypothesis of the test? What is the value of the test statistic? Is this value too high? What is the p-value and what can be concluded from it? Is the test reliable? This file contains data on the genotype (single nucleotide polymorphism, SNP) and the response to clozapine (yes or no) observed in 270 schizophrenic patients. Answer the following questions using these data: What is the percentage of responders in each genotype? Is the response related to the genotype? What is the RR of response in AA vs GG? What is the RR of response in GA vs GG? What is the RR of response in AA+GA vs GG? Hint: the riskratio() and function in package epitools is very useful to compute several RR when the grouping variable has more than two levels (like genotype); look at its help, and use it to solve the last three questions. Two clinicians assessed independently 200 patients suspected of having a disease. Each patient was assessed by both clinicians (that were blinded from each other) as having (D), or not having (noD) the disease. The results were as follows: # create the contingency table from a vector of four counts xt &lt;- matrix(c(52, 43, 71, 34), nrow = 2, dimnames = list(clinician_1 = c(&quot;D&quot;, &quot;noD&quot;), clinician_2 = c(&quot;D&quot;, &quot;noD&quot;))) %&gt;% as.table() # print the table xt clinician_2 clinician_1 D noD D 52 71 noD 43 34 How much they both agree in the diagnostic of the disease? Are both physicians equally prone to establish the diagnostic of the disease? If not, which one is more prone to diagnose D? Select the appropriate analysis for the following research objectives (assuming that any condition for the validity of the method is met). Is a complication of a disease equally frequent in male and female patients? Is the exposure to a risk factor more frequent in diseased than in healthy subjects? Are two radiologists equally prone to diagnose a disease from an imaging test? Do two radiologists agree on the diagnosis of a disease from an imaging test? Are births equally likely in all seasons of the year? The continuity correction is a small penalty used in the computation of the test statistic (affecting also the p-value), and is used by default; some authors recommend to use the correction only in some circumstances (when expected counts are low), but others recommend to use it always, which is safer.↩︎ Although the classical 2-sample test for the equality of proportions is based on a \\(z\\) statistic (computed as the difference of proportions divided by its standard error), the output of prop.test() shows the square of the \\(z\\) statistic, denoted as X-squared.↩︎ This formula computes the statistic without Yates’ continuity correction; the formula for the statistic with Yates’ continuity correction is similar, but the absolute values of the differences are reduced by 0.5 before squaring.↩︎ "],["analysis-of-quantitative-data.html", "8 Analysis of quantitative data 8.1 One sample tests 8.2 Independent samples 8.3 Paired samples 8.4 One-sided alternatives Resources Exercises", " 8 Analysis of quantitative data In this chapter we present methods for the analysis of quantitative variables. As we did in the previous chapter for categorical variables, we classify these methods as one-sample, and two-sample with independent or paired data. Table 8.1 illustrates the data involved in each case for a hypothetical study in which two groups of patients (A and B) had their diastolic blood pressure measured at two successive visits (DBP_1 and DBP_2). The estimation of the mean DBP at visit 1 is a one-sample problem (left); the comparison of DBP_2 values in groups A and B is a two-sample problem with independent samples (middle); and the comparison of DBP values in visits 1 and 2 is a two-sample problem with paired samples (right). Table 8.1: One-sample (left), two independent samples (middle), and two paired samples (right) group DBP_1 DBP_2 A 72 68 A 89 89 A 68 69 A 117 114 A 92 92 A 69 69 A 95 91 A 100 85 B 97 95 B 79 75 B 115 108 B 93 81 B 73 67 B 41 41 B 107 108 B 84 79 group DBP_1 DBP_2 A 72 68 A 89 89 A 68 69 A 117 114 A 92 92 A 69 69 A 95 91 A 100 85 B 97 95 B 79 75 B 115 108 B 93 81 B 73 67 B 41 41 B 107 108 B 84 79 group DBP_1 DBP_2 A 72 68 A 89 89 A 68 69 A 117 114 A 92 92 A 69 69 A 95 91 A 100 85 B 97 95 B 79 75 B 115 108 B 93 81 B 73 67 B 41 41 B 107 108 B 84 79 8.1 One sample tests It is said that the average weight at birth in western countries is 3500 grams. A one-sample test can be used to test this hypothesis. Since there is no expectation of a deviation from the null value in a particular direction, a two-sided alternative is appropriate, so that we set the following statistical hypotheses refered to the population mean \\(\\mu\\): \\(H_0: \\quad \\mu = 3500\\) \\(H_1: \\quad \\mu \\ne 3500\\) 8.1.1 One-sample t-test The t-test takes its name from the statistic computed and used to judge on the hypotheses stated above: \\[t = \\frac{\\bar{X} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\] where \\(\\bar{X}\\) and \\(s\\) are the sample mean and the standard deviation of the variable analyzed (birthweights) respectively, and \\(\\mu_0\\) is the value proposed by \\(H_0\\) (3500). This statistic is a measure of how different the sample mean \\(\\bar{X}\\) and \\(\\mu_0\\) are (top), scaled in a particularly convenient way (bottom). The resulting quantity is dimensionless, and it sign will depend on the direction in which \\(\\bar{X}\\) deviates from \\(\\mu_0\\). If \\(H_0\\) is true, we expect a relatively small absolute value of this statistic, e.g. below 2. It can be shown that, when \\(H_0\\) is true, the sampling distribution of the \\(t\\) statistic follows a probability distribution model known as Student’s t distribution, and this is used to compute the p-value. The t-distribution depends on a single parameter known as degrees of freedom (df), which in this case is the sample size minus one (\\(n-1\\)). To compute the p-value, a t-distribution with \\(n-1\\) degrees of freedom is used. This test can be done with function t.test() from either base R or mosaic. If we load mosaic (as done below), its t.test() function will take preference over the one in base R. The mu argument specifies the value for \\(\\mu\\) proposed by the \\(H_0\\) (\\(\\mu_0 =\\) 3500): library(mosaic) t.test(~ bwt, mu = 3500, data = d) One Sample t-test data: bwt t = -10.471, df = 188, p-value &lt; 2.2e-16 alternative hypothesis: true mean is not equal to 3500 95 percent confidence interval: 2839.952 3049.222 sample estimates: mean of x 2944.587 The absolute value of the \\(t\\) statistic is very high (10.471), and the p-value is extremely low (2.2e-16), which provides strong evidence against \\(H_0\\). The sample mean reported at the end of the output is 2995 grams (rounding to unit), a much lower value than the one proposed by \\(H_0\\), and its 95%CI is provided inmediately above. These results could be summarized by saying that the population mean of birthweights is well below 3500 (p-value &lt; 0.001), and is estimated as 2995 (95%CI: 2840, 3049) grams. The one sample t-test relies on the assumption that the variable analyzed follows a normal distribution in the population, and in figure 6.8 we saw that this assumption is reasonable. 8.1.2 Bootstrap When the normality assumption seems inappropriate, a good option is to use the bootstrap, a technique based on resampling (i.e., taking many random sub-samples of the sample) that makes no distributional assumptions at all, and therefore can be applied always. In addition, it is very flexible, because it can be applied to any statistic we can compute from sample data, no matter how complex. Although the normality assumption for birtweights is reasonable and the t-test conducted in the previous section is perfectly valid, we use this variable to illustrate the bootstrap method. The easiest way to use this technique is provided by function boot.t.test() in the MKinfer package. The syntax is similar to that of t.test(), but we can indicate the number of resamples we want to evaluate in the optional argument R. Since resamples are random, it is important to set the seed for the random number generator first, for reproduicibility: set.seed(123) MKinfer::boot.t.test(d$bwt, mu = 3500, data = d, R = 5000) # 5000 resamples Bootstrapped One Sample t-test data: d$bwt bootstrapped p-value &lt; 2.2e-16 95 percent bootstrap percentile confidence interval: 2839.655 3048.498 Results without bootstrap: t = -10.471, df = 188, p-value &lt; 2.2e-16 alternative hypothesis: true mean is not equal to 3500 95 percent confidence interval: 2839.952 3049.222 sample estimates: mean of x 2944.587 The output shows a very low p-value (2.2e-16) of the boostrapped t-test, providing strong evidence against \\(H_0\\), and a 95% CI for the mean ranging from 2840 to 3048. The result of the standard one-sample t-test is also provided under Results without bootstrap, for comparison. Here, because the normality assumption is reasonable, the two methods provide almost identical results, but this might not be the case if the normality assumption of the t-test is clearly violated, or when there are influential outliers, particularly in small samples. 8.2 Independent samples In the previous chapter we used the birthwt dataset to compare the proportion of low weight births in smoking and non-smoking mothers, by means of a chi-square test computed from the contingency table of categorical variables bw and smoke. Now we want to compare the birth weights recorded in variable bwt in smoking and non-smoking mothers. As a first step we inspect boxplots with overlaid means: gf_boxplot(bwt ~ smoke, data = d) %&gt;% gf_summary(fun = mean, geom = &quot;point&quot;, color = &quot;red&quot;) %&gt;% gf_refine(coord_flip()) Figure 8.1: Boxplot of birth weight with means overlaid (red dot) Figure 8.1 shows pretty symmetric distributions. In smokers, a single, not too far outlier doesn’t affect the mean (since it coincides with the median). The distribution of bwt in the group of smoking mothers seems to be shifted towards lower values compared to that of non-smoking mothers, and seems to have lower variability. 8.2.1 Unpaired (or two-sample) t-test An unpaired or two-samples t-test compares the means of a quantitative variable in two populations. This is the case if we want to compare the mean birth weights in smoking and non-smoking mothers. For a two-sided test, the formal statistical hypotheses would be: \\(H_0: \\quad \\mu_1 = \\mu_2 \\quad \\text{or} \\quad \\mu_1 - \\mu_2 = 0\\) \\(H_1: \\quad \\mu_1 \\ne \\mu_2 \\quad \\text{or} \\quad \\mu_1 - \\mu_2 \\ne 0\\) where \\(\\mu_1\\) and \\(\\mu_2\\) are the means in populations 1 and 2 (such as smokers and non-smokers).   Then, the following \\(t\\) statistic is computed: \\[t = \\frac{(\\bar{X_1} - \\bar{X_2}) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{s^2}{n_1} + \\frac{s^2}{n_2}}} = \\frac{(\\bar{X_1} - \\bar{X_2})}{\\sqrt{\\frac{s^2}{n_1} + \\frac{s^2}{n_2}}}\\] where \\(\\bar{X_1}\\) and \\(\\bar{X_2}\\) are the sample means in groups 1 and 2, \\(n_1\\) and \\(n_2\\) are the respective sample sizes, \\(s\\) is an pooled estimate of the standard deviation assumed to be the same in both populations, and \\(\\mu_1\\) and \\(\\mu_1\\) are as proposed by \\(H_0\\) (and therefore their difference equals zero). This statistic measures how different the mean difference in the sample is from that expected if \\(H_0\\) was true (zero difference), scaled in a convenient way. It can be shown that when \\(H_0\\) holds, this statistic follows a t-distribution with \\(n_1 + n_2 - 2\\) degrees of freedom, and this is used to compute the p-value. This test can be produced with function t.test() from either base R or mosaic. Option var.equal = TRUE is needed to compute a standard t-test, which assumes equality of variances in both populations (smokers and nonsmokers). We do not need to use the mu argument in this case because it is set to zero by default, since this is the value of the mean difference (\\(\\mu_1 - \\mu_2\\)) specified by the \\(H_0\\) . t.test(bwt ~ smoke, data = d, var.equal = TRUE) Two Sample t-test data: bwt by smoke t = -2.6529, df = 187, p-value = 0.008667 alternative hypothesis: true difference in means between group smoker and group non-smoker is not equal to 0 95 percent confidence interval: -494.79735 -72.75612 sample estimates: mean in group smoker mean in group non-smoker 2771.919 3055.696 The test gives a p-value of 0.009 (rounded to the third decimal), reflecting that the data are very inconsistent with \\(H_0\\), so that we can reject it and conclude that the mean birth weight is not the same for smoker and non-smoker mothers. The sample means in each group are reported at the end of the output under sample estimates. The 95% CI provided immediately above is a CI for the difference of means (2771.919 - 3055.696 = -283.777). We could synthesize these results by saying that the mean birth weigh of newborns is lower in smoking mothers than in non-smoking mothers (p = 0.009), with a mean difference of 284 grams (95% CI: 73, 495). To judge the magnitude of the difference, the Cohen’s d measure of effect size is sometimes used. This is nothing but the standardized mean difference, that is, the mean difference divided by the standard deviation. The main advantage of this measure is that it is dimensionless, and facilitates to judge the magnitude of a mean difference. The Cohen’s d measure of effect size can be computed with function cohens_d() of package rstatix: library(rstatix) cohens_d(bwt ~ smoke, data = d, var.equal = TRUE) # A tibble: 1 × 7 .y. group1 group2 effsize n1 n2 magnitude * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt; 1 bwt smoker non-smoker -0.395 74 115 small The output provides the Cohen’s d measure of effect size (-0.395) and a statement of its magnitude (small), according to a guide for interpretation proposed by Cohen (0.2 is a small effect, 0.5 is a moderate effect, and and 0.8 is a large effect). The t-test assumes that birthweights follow a normal distribution in the two populations compared (inthe example, smoking and non-smoking mothers). In practice, the test works well if both distributions are about symmetrical and there are no influential outliers, as is the case in figure 8.1. A further assumption of the standard t-test is homscedasticity, that is, equality of variances in both populations. This assumption could be inadequate here because variability seems to be lower in smoker mothers (see figure 8.1). When this is the case, a modified version of the t-test called Welch test is more appropriate. 8.2.2 Welch test A Welch two-sample t-test is just a t-test with some penalization in the computation of the p-value to compensate for possible heteroscedasticity (inequality of variances). The null and alternative hypotheses are exactly the same as in the t-test. We can produce this test with the same function t.test(), with argument var.equal set to FALSE, but because this is the default value for this argument (see ?t.test), we do not need to specify it. t.test(bwt ~ smoke, data = d) Welch Two Sample t-test data: bwt by smoke t = -2.7299, df = 170.1, p-value = 0.007003 alternative hypothesis: true difference in means between group smoker and group non-smoker is not equal to 0 95 percent confidence interval: -488.97860 -78.57486 sample estimates: mean in group smoker mean in group non-smoker 2771.919 3055.696 Note that the title of the output is now Welch Two Sample t-test. The p-value is 0.007 and the 95% CI for the mean difference ranges from -489 to -79. The sample means and their difference are just the same as in the standard t-test. Just as the standard t-test, the Welch test assumes a normal distribution of the variable in the two populations compared. 8.2.3 Wicoxon’s rank sum test/ Mann-Whitey’s test Suppose we want to compare the weight of smoking and non-smoking mothers. The weight of mothers at last menstrual period is recorded in variable lwt, expressed in pounds. We first convert lwt to kilograms into a new variable lwt_kg (just because this unit is more familiar to us) and produce boxplots for this variable according to smoking status, with means overlaid: d &lt;- d %&gt;% mutate(lwt_kg = round(lwt * 0.453592)) library(ggformula) gf_boxplot(lwt_kg ~ smoke, data = d) %&gt;% gf_summary(fun = mean, geom = &quot;point&quot;, color = &quot;red&quot;) %&gt;% gf_refine(coord_flip()) Figure 8.2: Weight of mothers with means overlaid (red dot) Figure 8.2 shows a typical pattern of asymmetric distributions, with several outliers in the long (right) tail. However, there is a shift towards lower values in the smokers group. Because in this case it is not reasonable to assume that the weight of mothers follows a normal distribution, the t-test or the Welch t-test would not be appropriate, and a distribution-free test is safer. The Wilcoxon-rank-sum test (WRST) and the Mann-Whitney test (MWT) are classic distribution-free tests that look for a shift in location when comparing two distributions of a quantitative variable in independent samples. Although they use different methods for its computation, they produce exactly the same p-value, and are therefore equivalent. In the WRST, the formal statistical hypotheses refer to a parameter \\(\\delta\\), that represents a shift in location assuming that the distributions are otherwise identical in form (though not necessarily normal, or even symmetric): \\(H_0: \\quad \\delta = 0\\) \\(H_1: \\quad \\delta \\ne 0\\) Therefore, \\(H_0\\) means there is no shift in location (the shift is null), while \\(H_1\\) means there is a (non-null) shift. A WRST can be produced with function Wilcox.test(), that has a syntax similar to t-test(), but its mu argument now represents the location shift proposed by \\(H_0\\), and we do not need to specify it since it is zero by default (see ?wilcox.test): wilcox.test(lwt_kg ~ smoke, data = d) Wilcoxon rank sum test with continuity correction data: lwt_kg by smoke W = 3814.5, p-value = 0.23 alternative hypothesis: true location shift is not equal to 0 The W statistic is computed from the ranks of lwt_kg, and has no easy interpretation. The p-value is 0.23, reflecting that the data are not very inconsistent with \\(H_0\\), and therefore do not provide evidence of a shift in location. The assumption that the form of the distribution of weights is the same in smoker and non-smoker mothers seems reasonable in this case (see figure 8.2). Otherwise, it would be difficult to attach a precise meaning to the shift in location parameter (\\(\\delta\\)). In such a case, the hypotheses of the MWT can be used: \\(H_0: \\quad P(X &gt; Y) = P(X &lt; Y)\\) \\(H_1: \\quad P(X &gt; Y) \\ne P(X &lt; Y)\\) where P stands for probability, and \\(X\\) and \\(Y\\) represent the weights of a (randomly selected) smoker and non-smoker mother, respectively. This \\(H_0\\) means that if we take a random pair of cases, one from each population, the weight of the smoker is just as likely to be greater than that of the non-smoker as the opposite. The same p-value obtained above can be used to evaluate the MWT hypotheses. However, the assumptions are not the same as in the WRST, and this affects how we can interpret the result. Under the assumption of equal distribution forms made in the WRST, a low p-value can be interpreted in terms of a shift in medians (or any other quantile, actually). However, if we are not ready to make this assumption these interpretation is not correct, and we can only say that it is more likely to have a lower (or higher) value of the variable in one group than in the other. 8.2.4 Bootstrap Bootstrap resampling can be used to assess the difference in the means of smoker and non-smoker mothers. The same function boot.t.test() in the MKinfer package that was used in section 8.1.2 can be used here. The syntax is similar to that of t.test(), but we can indicate a specific number of resamples in the optional argument R. Since resamples are random, it is important to set the seed first, for reproducibility: set.seed(123) MKinfer::boot.t.test(lwt_kg ~ smoke, data = d, R = 5000) Bootstrapped Welch Two Sample t-test data: lwt_kg by smoke bootstrapped p-value = 0.582 95 percent bootstrap percentile confidence interval: -5.345035 2.919850 Results without bootstrap: t = -0.59251, df = 136.7, p-value = 0.5545 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -5.528051 2.979050 sample estimates: mean in group smoker mean in group non-smoker 58.10811 59.38261 The result shows a bootstrapped p-value of 0.582 which indicates that the data are not very inconsistent with the null hypothesis (which is the same as for the standard t-test). A 95% bootstrap percentile CI is also provided, ranging from -5.3 to 2.9 Kg, for the mean difference. Below these results, the result of the standard t-test is also provided under Results without bootstrap. 8.2.5 Assessment of normality In this section we have presented several one- and two-samples tests for the analysis of a quantitative variable. Some of them rely on the assumption of normal distributions (standard and Welch t-tests) and some others do not (WRST/MWT and bootstrapped t-test). To choose among them, we only inspected the boxplots for asymmetry or influential outliers, because it is a simple and sufficient procedure. However, this is not the best way to assess normality. The optimal way to assess normality by graphical methods is the so called quantile-quantile plot or QQ plot. The following script produces QQ plots for the weight of smoking and non_smoking mothers using function gf_qq() in ggformula, and overlays a diagonal reference line. gf_qq(~ lwt_kg | smoke, data = d) %&gt;% gf_qqline(col=&quot;red&quot;) Figure 8.3: QQ plot of the weight of mothers In a good approximation to the normal distribution, the points would lie randomly scattered along the diagonal reference line. This is not the case here, where we see a curvilinear pattern clearly parting from the reference line, and therefore assuming normality would be inappropriate. In the case of the weight of births, the QQ plot shows a much better approximation: although there is some deviation from the reference at the ends, this is quite acceptable to conduct a t (or Welch) test, provided sample sizes are not too small (e.g., n &gt; 30). gf_qq(~ bwt | smoke, data = d) %&gt;% gf_qqline(col=&quot;red&quot;) Figure 8.4: QQ plot of birthweights In some books and courses on statistics, a more formal assessment is recommended, using significance tests for normality, such as the Shapiro-Wilk test, or the Kolmogorov test. We do not share this view, and the main reason is that in these (and any other) normality tests, normality is represented by the null hypothesis, and can never be shown (remember: a high p-value only indicates that the data are not inconsistent with the null, but this by no means is a proof of its truth). So, a high p-value does not allow to conclude the distribution is normal, particularly if the samples are small. In addition, a low p-value can be obtained if the samples are very large, even if the deviation from normality is mild. Despite we do not recommend it as a way to assess the normality assumption, we show how to conduct the Shapiro-Wilk test on the birthweights in both smoking and non-smoking mothers. Note the function needs to be passed a vector of values as argument and does not support formula syntax. For this reason, we first extract the vectors of birthweight values for smoking and non-smoking mothers (with pull()), and chain the shapiro.test() thereafter: d %&gt;% filter(smoke == &quot;smoker&quot;) %&gt;% pull(bwt) %&gt;% shapiro.test() Shapiro-Wilk normality test data: . W = 0.98296, p-value = 0.4195 d %&gt;% filter(smoke == &quot;non-smoker&quot;) %&gt;% pull(bwt) %&gt;% shapiro.test() Shapiro-Wilk normality test data: . W = 0.98694, p-value = 0.3337 Since the p-values are not very low, the null hypothesis of normality is not rejected, that is, the test fails to find evidence of non-normality. 8.3 Paired samples To illustrate the methods for the analysis of paired samples, we will use data on the body weight of 25 individuals that gave up smoking. The dataset only has two variables, showing body weights before (pre) and after (pos) smoking cessation, expressed in kilograms. Download smoking cessation data As a first step to investigate the evolution of weight, after reading the data we compute its increment (the difference defined as Pos - Pre), and produce a boxplot with the mean overlaid: d &lt;- read.delim(&quot;data/smoking cessation.txt&quot;) %&gt;% mutate(increment = pos - pre) gf_boxplot(increment ~ &quot;&quot;, data=d) %&gt;% gf_summary(fun = mean, color = &quot;red&quot;) %&gt;% gf_refine(coord_flip()) Figure 8.5: Boxplot of increments in body weight afer smoking cessation (Kg) with mean overlaid (red dot) In the boxplot of figure 8.5 we see that all increments are positive, reflecting a gain in weight, and range from less than one to more than six kilograms. The distribution of increments looks quite symmetric, with an single outlier that does not greatly affect the mean. If we compute the means of all three variables, we will realize that the difference of the means of variables pre and pos is exactly equal to the mean of the increments: mean(d$pre) [1] 59.676 mean(d$pos) [1] 63.592 mean(d$pos)-mean(d$pre) [1] 3.916 mean(d$increment) [1] 3.916 It can be shown that this in necessarily so, and will happen whatever the values of the two variables. Given any two variables \\(X\\) and \\(Y\\), the difference in their means equals the mean of the differences: \\[\\mu_X - \\mu_Y = \\mu_d\\] where \\(d = X - Y\\). 8.3.1 Paired (or one-sample) t-test To test for a difference in the mean of weights before and after smoking cessation, we could set the following formal hypotheses: \\(H_0: \\quad \\mu_{pos} = \\mu_{pre} \\qquad \\text{or} \\qquad \\mu_{pos} - \\mu_{pre} = 0\\) \\(H_0: \\quad \\mu_{pos} \\ne \\mu_{pre} \\qquad \\text{or} \\qquad \\mu_{pos} - \\mu_{pre} \\ne 0\\) Because the difference in means equals the mean of the differences, the previous set of hypotheses can be replaced by: \\(H_0: \\quad \\mu_d = 0\\) \\(H_0: \\quad \\mu_d \\ne 0\\) where \\(d\\) are the increments of weight (weight pos - weight pre). Note that this is the form of a one sample test involving a single variable (the increments \\(d\\)). Therefore, we could use a one-sample t-test, on the assumption that the weight increments are normally distributed (no need to specify the mu argument, since it is zero by default): t.test(d$increment, data=d) One Sample t-test data: d$increment t = 12.553, df = 24, p-value = 4.893e-12 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: 3.272134 4.559866 sample estimates: mean of x 3.916 The value of the t statistic is quite high, and the p-value very low, providing evidence against \\(H_0\\). So, there is evidence of a mean reduction in the body weight after smoking cessation, of 3.9 kg (95%CI: 3.3, 4.6; p &lt; 0.001). For convenience, t.test() can be used with the initial variables pre and pos, so that there is no need to compute them previously, but then the paired = TRUE argument has to be specified. In addition, the order in which the two variables are specified will determine how their difference is computed: it will be always computed as first variable - second variable. Therefore, if we want the difference to be defined as we did for increment, the posvariable should be specified first: t.test(d$pos, d$pre, data=d, paired = TRUE) Paired t-test data: d$pos and d$pre t = 12.553, df = 24, p-value = 4.893e-12 alternative hypothesis: true mean difference is not equal to 0 95 percent confidence interval: 3.272134 4.559866 sample estimates: mean difference 3.916 Note that the only difference between this output and the previous one is the title, now reading Paired t-test, and the way the alternative hypothesis is expressed, now referring to the difference in means of the two paired variables. Therefore, a paired t-test is nothing but a one-sample t-test applied to the differences of the two paired variables. 8.3.2 Wilcoxon’s signed ranks test The Wilcoxon`s signed ranks test (WSRT) is a classic distribution-free test that can be used when the assumption of normality is not tenable. Despite this assumption is quite reasonable in the case of the weight increments after smoking cessation, we will use this variable to illustrate the WSRT. The WSRT test can be performed with function wilcox.test(), specifying paired = TRUE: wilcox.test(d$pre, d$pos, paired = TRUE) Wilcoxon signed rank test with continuity correction data: d$pre and d$pos V = 0, p-value = 1.306e-05 alternative hypothesis: true location shift is not equal to 0 The small p-value implies that the distribution of the weight increments is not symmetric and centered at zero, which is what we would expect in case of no shift in one of the distributions with respect to the other. 8.3.3 Bootstrap Just as for the two independent samples case, the bootstrap can be used with a one sample t-test on the weight increments. The one sample t-test can be bootstrapped using the boot.t.test() function from package MKinfer: set.seed(123) MKinfer::boot.t.test(d$increment, R = 5000) Bootstrapped One Sample t-test data: d$increment bootstrapped p-value &lt; 2.2e-16 95 percent bootstrap percentile confidence interval: 3.340 4.528 Results without bootstrap: t = 12.553, df = 24, p-value = 4.893e-12 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: 3.272134 4.559866 sample estimates: mean of x 3.916 With 5000 bootstrap replicates, the result shows a very small bootstrapped p-value, and a 95% bootstrap percentile CI very similar to the one provided with the standard t.test() function. 8.4 One-sided alternatives All examples of tests presented in this chapter were two sided tests. However, tests with one-sided alternatives are possible as well. To conduct a one-sided test, we use the same functions as for the two-sided test, but we specify the alternative argument as either greater or less. For instance, the following function call performs a one-sample t-test on the smoking cessation data, with the following one sided alternative \\(H_1: \\quad \\mu_{increment} &gt; 0\\). t.test(d$pos, d$pre, data=d, paired = TRUE, alternative = &quot;greater&quot;) Paired t-test data: d$pos and d$pre t = 12.553, df = 24, p-value = 2.447e-12 alternative hypothesis: true mean difference is greater than 0 95 percent confidence interval: 3.382263 Inf sample estimates: mean difference 3.916 Note that the p-value is different (though very small as well), and the 95% CI is a one-sided CI, indicating a lower bound only (the upper bound is Inf for infinity). This means we are 95% confident that, after smoking cessation, the mean increment in weight is at least 3.4 kg (3.4 kg or higher). Resources Wikipedia has a very complete explanation of the Student`s t-distribution, though rather technical. A lighter explanation can be found here. If you want to know the details on how p-values are computed in a significance test, look at this example of a one-sample t-test. Do not use histograms to assess normality: find out why. If you want to know the details on how the Welch t-test is computed, see this wikipaedia entry. An intuitive introduction to the bootstrap can be found here. Not just a book, but the book on the bootstrap, by its creator Bradley Efron. Warning: it is a technical book, not an easy read for non-statisticians! Exercises The Gestation dataset that comes with mosaic contains data on birth weights of 1028 newborns. (see ?Gestation for a description of variables). With this dataset: Filter cases of never-smoking mothers, and current-smoking mothers (levels “never” and “now” of smoke, respectively), and use this subset of cases to conduct the following analyses. What is the size of this subset, and how is the split of current- and never-smokers? Compute a new variable bwt expressing the birth weight in kilograms, by dividing wt by 35.274, and rounding to the third decimal. Inspect a boxplot of bwt according to smoking, with overlai means. Is there a shift in location? Are there outliers? and if yes, are they influential? Which test would you choose, based on the boxplots, to compare birthweighs in the smoking and non-smoking groups? Inspect a QQ plot. Do you think there is a deviation from normality in either smoker and non-smoker mothers? Conduct a normality test for each group. Use the bootstrap to compare birthweighs in smoking and non-smoking mothers, and to estimate the difference in means. Interpret the results. Are they very different from what you get in a standard t-test? The dataset ais in package DAAG contains data on various characteristics of the blood of 202 athletes (see ?DAAG::ais for a description of variables). Conduct appropriate analyses to answer the following questions: How many males and female athletes are there in this dataset? Is the hematocrite different in males and females? By how much, on average? Is the white blood cell count different in males and females? By how much, on average? The following dataset contains health-related quality of life (QoL) data collected in the SARA trial, as measured by the AF-QoL questionnaire. QoL was measured at baseline, 6 and 12 months after randomization. Higher values indicate better QoL. The group variable is the randomized treatment coded as 1 = catheter ablation (CA), or 2 = antiarrhythmic drug treatment (ADT). Download SARA QoL data Read the data, retain the complete cases only (i.e., cases with no missing values), and conduct all analyses with these complete cases. How many patients are there in each treatment group? Is there an improvement in QoL from baseline to 12 months? Is there an improveent in QoL from baseline to 12 months, in both CA and AD treatment groups? Compute the increments from baseline to 12 months. Are these increments similar in both treatment groups? The following dataset has total cholesterol levels in the blood of patients with arterial hypertension. Are cholesterol levels different for males and females? Cholesterol and sex data Inspect a boxplot, and describe what you see. A far outlier is seen in females; could it be a data error? Conduct a t-test and interpret the result. Conduct a Wilcoxon Rank Sum test and interpret the result. Repeat the t-test after excluding the far outlier. A pharmaceutical company produces tablets containing 500 mg of an active principle. The manufacturing process is imperfect, and actual table weights are not exactly 500 mg. Small variations are allowed provided the average weight of the tables produced is 500 mg. The quality assurance staff controls the weight of every batch of tablets by weighting a random sample of 30 tablets, and declares the manufacturing process out of control if its average weight is not 500 mg. For a particular batch, these are the weights of the 30 sampled tablets: 497.2 500.4 502.3 497.8 502.0 508.9 503.5 498.9 496.9 500.8 498.8 500.6 493.7 506.1 500.6 502.5 497.6 494.9 491.6 494.3 507.8 508.6 496.6 501.8 497.2 490.2 494.7 496.4 504.2 506.3 Was the manufacturing process out of control for this batch? "],["assessing-relations.html", "9 Assessing relations 9.1 Independence vs relationship 9.2 Relation between two quantitative variables 9.3 Measures of linear relation 9.4 Linear regression Resources Exercises", " 9 Assessing relations Assessing relations between variables is commonplace in every area of research. It is important to start clarifying that the word relation may refer to different types of relation. For instance, in mathematics, it is often used to refer to functional relations, as in \\(y = f(x)\\), meaning that \\(y\\) is a function of \\(x\\), so that \\(x\\) and \\(y\\) are functionally related. In statistics however, when we say that two variables are related, or associated, we mean that there is a probabilistic relation between them, as opposed to independence. 9.1 Independence vs relationship Two variables X and Y are said to be independent when the probability of one of them (say, Y) taking certain values is the same for all values of the other (X). More formally, \\[Pr(Y|X) = Pr(Y) \\qquad \\text{for all values of} \\: Y \\text{and} \\: X\\] The previous expression states that, the conditional probability of variable Y given variable X (that is, when X takes some particular value), equals the unconditional probability of Y. This implies that the conditional probability \\(Pr(Y|X)\\) is the same for all possible values of X. Conversely, two variables are said to be probabilistically related or associated, when the previous expression does not hold, that is, when \\[Pr(Y|X) \\ne Pr(Y) \\qquad \\text{for some values of} \\: Y \\text{and} \\: X\\] Let’s clarify this with graphical examples. In figure 9.1 we show three graphics displaying the distribution of variable Y (vertical axis) according to the value of variable X (horizontal axis). The difference between the three graphics is the type of the variables involved: In graphic A, both variables are categorical In graphic B, Y is quantitative and X is categorical In graphic C, both variables are quantitative Figure 9.1: Independence patterns In all three graphics, we see that the distribution of Y is the same for all possible X values. In this case, we say that X and Y are independent, because the distribution of Y does not change depending on X. Note that if we had plotted the unconditional distribution of Y (that is, the overall distribution of Y in all cases, no matter what the value of X), the shape of the distribution would have been the same we see for each and all X values. Conversely, in figure 9.2 the distribution of Y is not the same for all values of X. In these case, we say that variables X and Y are (probabilistically) related, because the probability of \\(Y\\) taking certain values, depends on the value of \\(X\\). Figure 9.2: Non-independence or association patterns It is important to understand that independence is a very specific situation (when nothing in the distribution of Y changes with X), while association is just anything different from independence, and there are many different possibilities. First, there are different possible forms of relationship. For instance, the boxplots in figure 9.2 show an increase from A to B, and then a decrease from B to C, but other forms are possible, such as a progressive increase from A to B, and from B to C. In addition, relations can have different strength, (being strong, moderate or weak), depending on how different the distributions of Y are for different X values (e.g., the relative shifts in location of A, B and C could have been greater than they are in figure 9.2). The methods used to assess relations depend on the type of variables involved, and the three possible scenarios represented in figures 9.1 and 9.2 need to be considered. In previous chapters we have covered methods for the first two scenarios: Assessing the relation between two categorical variables is no different from comparing the distribution of a categorical variable across the levels of another categorical variable. This is what we did in section 7.2 when we studied the relation between birthweight (categorized as lowor normal) and smoking during pregnancy, using both tests (chi-square and Fisher’s tests) and association measures (RD, RR, and OR). Assessing the relationship between a quantitative and a categorical variable is no different from comparing the distribution of the quantitative variable among groups defined by the categorical variable, and this was addressed in section 8.2, when we studied the relation between birthweights (in grams) and smoking during pregnancy, using tests (standard and Welch t-tests, and bootstrap t-test) and association or effect measures (mean difference and Cohen’s d effect size). We are left with the third scenario we address in this chapter. To illustrate the relevant methods, we will use this dataset containing anthropometric measures in 248 adult men. The script below reads the data file and prints the first six lines: d &lt;- read.table(&quot;data/anthropometric_measures.txt&quot;, header=TRUE) head(d) Age Weight Height Thorax Waist Wrist Biceps Thigh Knee Ankle 1 23 70.0 172 93.1 85.2 17.1 32.0 59.0 37.3 21.9 2 22 78.6 184 93.6 83.0 18.2 30.5 58.7 37.3 23.4 3 22 69.9 168 95.8 87.9 16.6 28.8 59.6 38.9 24.0 4 26 83.8 184 101.8 86.4 18.2 32.4 60.1 37.3 22.8 5 24 83.6 181 97.3 100.0 17.7 32.2 63.2 42.2 24.0 6 24 95.4 190 104.5 94.4 18.8 35.7 66.0 42.0 25.6 9.2 Relation between two quantitative variables The inspection of a scatterplot should be the first step to investigate the relation between two quantitative variables. A scatterplot can depict a wide variety of patterns characterizing different types of relation. Figure 9.3 illustrates some characteristic patterns/types of relation you should be able to recognize. Figure 9.3: Important patterns in scatterplots Independence may look like a ball, a horizontal band, or a horizontal ellipse. Everything else is a relation pattern. The simplest and most common type of relation is the linear relation. Linear relations appear as bands or ellipses with their main axis having a non-null slope. Ellipses don`t need to be very slim for a relation to be linear. What characterizes a linear relation is that the slope (the rate of change in Y as X increases) is constant, and this may happen as well when bands or ellipses are pretty thick. When the slope is not constant the relation is non-linear, and can be either monotonic or non-monotonic. A monotonic relation is either non-decreasing (increasing or flat) or non-increasing (decreasing or flat). Last, non-monotonic relations have turning points: an increasing phase is followed by a decreasing (after reaching a maximum), or viceversa (after reaching a minimum). Aside from the type of relation, its strength is important as well. For instance, consider the scatterplots of the thorax and waist, and the biceps and wrist circumferences shown in figure 9.4. In both cases, a direct linear relation is apparent, but they differ in strength. Graphically, the strength of a linear relation has to do with both the thickness of the band or ellipse, and the slope of its main axis. Figure 9.4: Two direct, linear relations differing in strength In cases such as those in figure 9.4 it is easy to decide which of the two relations is stronger, but in other cases it may be more difficult, and measures of linear relation may help. 9.3 Measures of linear relation 9.3.1 Covariance The Covariance of two quantitative variables \\(X\\) and \\(Y\\) is a measure of their linear relation. The sample covariance is computed as: \\[Cov(X,Y) = \\frac{1}{(n-1)} \\sum{(X - \\bar{X}) (Y - \\bar{Y})}\\] where the sum is taken over all \\(n\\) observations. Despite we divide by \\(n-1\\) (instead of \\(n\\)), covariance is conceptualized as the average of the products of the differences \\((X - \\bar{X})\\) and \\((Y - \\bar{Y})\\), computed for all observations. To understand why this is a measure of linear relation, consider the scatterplot of thorax (\\(Y\\)) and waist (\\(X\\)) circumferences shown in figure 9.5 (left), where the means of \\(X\\) and \\(Y\\) are indicated by a vertical and a horizontal line respectively. These lines define four quadrants. An observation is highlighted in the upper-right quadrant, and the differences, \\((X-\\bar{X})\\) and \\((Y-\\bar{Y})\\) are represented by dashed line segments. Because both \\(X\\) and \\(Y\\) values for this observation are above the corresponding means, the differences \\((X-\\bar{X})\\) and \\((Y-\\bar{Y})\\) are both positive, so that their product will be positive as well. In fact, this will be the case for any observation in the upper-right quadrant, since all have \\(X\\) and \\(Y\\) values above their means. For observations in the lower-left quadrant, the differences \\((X-\\bar{X})\\) and \\((Y-\\bar{Y})\\) will be both negative, since they all have \\(X\\) and \\(Y\\) values below their means, so that their product will be positive as well (the product of two negatives is positive). However, for observations in the remaining two quadrants, this will happen: Lower-right: \\(X &gt; \\bar{X}\\) and \\(Y &lt; \\bar{Y}\\), so that \\((X-\\bar{X})\\) will be positive and \\((Y-\\bar{Y})\\) will be negative, or Upper-left: \\(X &lt; \\bar{X}\\) and \\(Y &gt; \\bar{Y}\\), so that \\((X-\\bar{X})\\) will be negative and \\((Y-\\bar{Y})\\) will be positive. Therefore, for observations in these two quadrants the product \\((X - \\bar{X}) (Y - \\bar{Y})\\) will be negative. In summary, the product of \\((X - \\bar{X}) (Y - \\bar{Y})\\) will be positive for all observations in the upper-right and lower-left quadrants, and negative for all those in the upper-left and lower-right quadrants, as shown in figure 9.5 (right), using colors. Figure 9.5: X and Y diferences with respect to their means (left) and sign of their product (right) Once the products \\((X - \\bar{X}) (Y - \\bar{Y})\\) have been computed for all observations, they are summed up, and the sum is divided by \\((n-1)\\). The result of the sum (and hence the covariance) will be either (see figure 9.6): zero (or close to zero), when the number of observations in positive and negative quadrants is balanced, since the positive and negative terms in the sum will cancel: this will happen in case of independence (figure 9.6 C and F). positive, when observations predominate in positive quadrants, which will happen in case of direct relations (figure 9.6 A and B). negative, when observations predominate in negative quadrants, which will be the case in inverse relations (figure 9.6 D and E). In addition, the absolute value of the sum (and of the covariance) will be higher for stronger relations (figure 9.6 A and D), than for mild relations (figure 9.6 B and E), because in the former case the predominance of positive or negative quadrants will be overwhelming while in the later will be modest. In summary, the sign of the covariance will reflect the direction of the relation (positive for direct relations and negative for inverse relations), and its absolute value will increase as a relation is stronger. Figure 9.6: Predominance of quadrants and covariance The covariance can be computed with function cov() from the mosaic package (there is also a cov() function in base R, but it does not allow a formula as argument). For instance, the covariance of thorax and waist is: library(mosaic) cov(Thorax ~ Waist, data=d) %&gt;% round(2) [1] 83.04 and that of biceps and wrist, is: cov(Biceps ~ Wrist, data=d) %&gt;% round(2) [1] 1.81 The covariance has a limitation as a measure of linear relation, because it is affected by the units of \\(X\\) and \\(Y\\). For instance, the covariance of thorax and waist is 83.04 cm2, because both variables are expressed in centimeters. If we express them in meters instead, then the covariance is 0.01 m2. This makes the interpretation of the covariance difficult, because its absolute value not only depends on the strength of the relation, but on the units of \\(X\\) and \\(Y\\) as well. The so called Pearson’s correlation coefficient overcomes this problem. 9.3.2 Pearson’s correlation The Pearson’s or product-moment correlation coefficient, is a standardized version of the covariance. The covariance is divided by the standard deviations of \\(X\\) and \\(Y\\) (\\(S_X\\) and \\(S_Y\\)), so that the resulting quantity is dimensionless, and therefore does not depend on their measurement units: \\[r_{(X,Y)} \\quad = \\quad \\frac{Cov(X, Y)}{S_X \\ S_Y}\\]. This measure can only take values in the -1 to +1 range. Because \\(S_X\\) and \\(S_Y\\) are always positive, the sign of \\(r_{(X,Y)}\\) will be that of \\(Cov(X, Y)\\), and reflects the direction of the relation (direct when positive, and inverse when negative); and its absolute value reflects the strength of the linear relation: the higher the absolute value of \\(r_{(X,Y)}\\), the stronger the relation. Perfect linear relations are characterized by \\(r = 1\\) or \\(r = -1\\). The Pearson’s correlation coefficient can be computed with function cor() from the mosaic package (there is also a cor() function in base R, but it does not allow formulas). For instance, the Pearson’s correlation of thorax and waist is: library(mosaic) cor(Thorax ~ Waist, data=d) %&gt;% round(2) [1] 0.92 Quite a high value, reflecting a strong, positive relation. The correlation of biceps and wrist circumferences should be much lower, according to the scatterplot of figure 9.4 (right), and in fact it is: cor(Biceps ~ Wrist, data=d) %&gt;% round(2) [1] 0.64 Now lets look at the relation of age with weight and height. Figure 9.7 shows the scatterplots and the Pearson’s correlation coefficients: Figure 9.7: Pearson’s correlations of age, with height (left) and with height (right) In figure 9.7 (left) age and weight show an independence pattern in the form of a horizontal band (and an outlier with weight &gt; 150 kg), and the Pearson’s r is very close to zero. In the case of age and height (right), a weak inverse relation is suggested, with a negative (and small) value of Pearson’s r. 9.3.3 Spearman’s correlation The Pearson’s correlation coefficient is not just a measure of relation, but a measure of linear relation, and therefore should not be used when non-linearity is apparent in a scatterplot. In addition, it can be affected by influential outliers. In case of outliers, or non-linear but monotonic relations, the Spearman’s correlation coefficient is more appropriate. This is just the Pearson’s correlation coefficient computed with the ranks of \\(X\\) and \\(Y\\). The Spearman’s correlation coefficient can be computed with the same cor() function, specifying the method = \"spearman\"argument. For illustration purposes, let’s compute it for height and age: cor(Height ~ Age, data=d, method = &quot;spearman&quot;) %&gt;% round(2) [1] -0.23 In this case, the value of the Spearman’s and the Pearson’s correlations coefficients are very similar. But let’s see what happens in case of influential outliers, such as those appearing in the scatterplot of figure 9.8. The two outliers with an ankle circumference higher than 30 cm impair the Pearson’s correlation coefficient, but do not affect the Spearman’s correlation coefficient (since it is based on ranks). Figure 9.8: Knee and ankle circumferences 9.3.4 Pearson’s vs Spearman’s correlation Figure 9.9 shows the Pearson’s and Spearman’s correlations coefficients in four scenarios of perfect relations: In a perfect linear relation (figure 9.9 A), both coefficients are 1 (if the relation is direct) or -1 (if inverse). In a non-linear, but still monotonic relation (figure 9.9 B), the Pearsons’s r is no longer 1 (or -1), so that it does not reflect the fact that the relation is perfect; however, the Spearman’s correlation remains 1 (or -1). In a perfect but non-monotonic relation (figure 9.9 C), none of the correlation coefficients reflects a perfect relation. In fact, they do not reflect a relation at all, since they are both equal to zero. Outliers can greatly distort Pearson’s correlations. In figure 9.9 D, the relation is perfectly linear (but for the outlier) and the Pearson’s correlation is zero, but the opposite might happen as well (i.e, an outlier could produce a non-zero value of the Pearson’s correlation coefficient in case of independence). Figure 9.9: Pearson’s vs Spearman’s correlation coefficients From the previous, you can take away the following algorithm to decide which correlation measure to compute in any particular case. Always inspect a scatterplot before computing any correlation coefficient. Then: If the relation is linear and there are no influential outliers, compute the Pearson’s correlation. If the relation is non-linear but still monotonic, or there are influential outliers, compute the Spearman’s correlation. If the relation is non-monotonic, compute neither of them: it makes little sense to compute a measure of linear relation when the relation is not even monotonic. 9.3.5 Tests on correlation coefficients In some cases, we may be interested in testing whether or not two variables are linearly independent. For instance, consider the case of height and age in figure 9.7 (right). The low \\(r\\) value suggests a mild correlation, but could this be a chance finding due to random sampling variation? A test on the population correlation coefficient, denoted by the Greek letter rho (\\(\\rho\\)) can be conducted with the following hypotheses: \\(H_0: \\qquad \\rho = 0\\) \\(H_1: \\qquad \\rho \\ne 0\\) The null hypothesis represents linear independence in the population, while the alternative hypothesis implies some degree of linear relation. If the two variables follow a bivariate normal distribution (see comment at the end of this section), it can be shown that, when \\(H_0\\) is true, the statistic \\[ t = \\frac{r \\ \\sqrt{n-2}}{\\sqrt{1-r^2}}\\] follows a t-distribution with \\(n-2\\) degrees of freedom, and this is used to compute the p-value for the test. The scatterplot of height and age of figure 9.7 (right) makes it difficult to judge whether or not these two variables are related, and this test may be useful in case of doubt. The test can be produced with function cor.test(): cor.test(Height ~ Age, data=d) Pearson&#39;s product-moment correlation data: Height and Age t = -4.052, df = 246, p-value = 6.813e-05 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.3633794 -0.1296058 sample estimates: cor -0.2501346 The output shows the value of the t statistic, which is quite different from 0 (t = -4.052), and the associated p-value, which is very small (p &lt; 0.001). This implies there is evidence against the null hypothesis of linear independence. The value of the Pearson’s correlation is provided at the end (cor = -0.25) and its 95% CI immediately above (-0.36 to -0.13). From this results we can conclude that there is a mild inverse relation between height and age. Strictly, this test is only valid when the two variables follow a bivariate normal distribution. However, this condition is not easy to verify, and the test is robust against violations of this assumption if the sample size is not too small. In practice, non-linearity and outliers are the most frequent threat, and these are clearly seen in a scatterplot. In case of outliers or non-linear but still monotonic relations, a similar test can be conducted on the Spearman’s correlation coefficient, using function cor.test() with argument method = \"spearman\". For instance, for the knee and ankle relation in figure 9.8, this will produce the linear test on the population Spearman’s correlation: cor.test(Knee ~ Ankle, data=d, method=&quot;spearman&quot;) Spearman&#39;s rank correlation rho data: Knee and Ankle S = 697554, p-value &lt; 2.2e-16 alternative hypothesis: true rho is not equal to 0 sample estimates: rho 0.7256017 The small p-value provides evidence of that knee and ankle circumferences are not linearly independent, and an estimate of the Spearman’s correlation coefficient and corresponding 95% CI is provided. 9.4 Linear regression When two variables are linearly related, we may want to know what is the line that best represents the relation, which is called regression line. This may be useful for predictions, or just to characterize and describe the relation in a simple way. Because the regression line is a stright line, we will start by reviewing some basic concepts concerning straight lines. 9.4.1 Straight lines The general equation of a straight line is of the form \\[Y = \\alpha + \\beta X\\] where \\(Y\\) is the dependent or predicted variable, \\(X\\) is the independent or predictor variable, and \\(\\alpha\\) and \\(\\beta\\) are the the intercept and the slope respectively. The intercept and the slope are collectively known as the parameters of the straight line. There is an infinite number of straight lines, but the parameters of a straight line define a particular line. For instance, \\(Y = 1 + 2X\\) and \\(Y = - 3X\\) are different lines, just because they have different parameter values: \\(\\alpha = 1\\) and \\(\\beta = 2\\) in the former, and \\(\\alpha = 0\\) and \\(\\beta = -3\\) in the later. The parameters of a line reflect two important characteristics: The intercept (\\(\\alpha\\)) is the \\(Y\\) value you get when \\(X=0\\), since \\(Y = \\alpha + \\beta \\ 0 = \\alpha\\); graphically, it is where the line intersects the vertical axis (and hence its name). The slope (\\(\\beta = 2\\)) is the rate of change in \\(Y\\) per unit change in \\(X\\); if positive, \\(Y\\) grows with \\(X\\); and if negative, \\(Y\\) decreases as \\(X\\) increases. If you want to review more basic concepts of straight lines, look here (and beware they denote the slope m, and the intercept b). 9.4.2 Regression line A common belief is that men grow their bellies as they age. Figure 9.10 shows a scatterplot of the waist circumference and age. A mild, linear relation is suggested: the waist values seem to increase slightly with age. gf_point(Waist ~ Age, data = d, col = &quot;DarkGray&quot;, alpha = 0.8) %&gt;% gf_refine(theme_bw()) Figure 9.10: Age and waist circumferece Suppose we want to know what is the best line to represent this relation. To find it, we should clarify what we mean by best. A common criterion is to minimize the overall prediction errors, that is, the differences between observed \\(Y\\) values and those predicted by the line. The line that minimizes the overall prediction error is called minimum-squares regression line (or simply, regression line), and it can be shown that this is the line having the following intercept and slope, estimated from sample data: \\[\\text{Slope:} \\qquad b \\quad = \\quad \\frac{Cov(X,Y)}{S^2_X} \\qquad \\qquad \\qquad \\qquad \\qquad \\text{Intercept:} \\qquad a \\quad = \\quad \\bar{Y} - b \\bar{X}\\] where \\(Cov(X,Y)\\) is the sample covariance of \\(X\\) and \\(Y\\), \\(S^2_X\\) is the sample variance of \\(X\\), and \\(\\bar{Y}\\) and \\(\\bar{X}\\) are the sample means of \\(X\\) and \\(Y\\). The regression line for the scatterplot above can be obtained with function lm() (standing for linear model): lm(Waist ~ Age, data = d) Call: lm(formula = Waist ~ Age, data = d) Coefficients: (Intercept) Age 83.6010 0.2003 The output shows the sample estimates of the regression line parameters under Coefficients: the intercept is 83.6, and the slope is 0.2 (both rounded to the first decimal). This means that the equation of the regression line is \\(Y = 83.6 + 0.2 \\ X\\). To overlay the estimated regression line on the scatterplot, we use function gf_smooth() with argument stat = \"lm\", as shown below: gf_point(Waist ~ Age, data = d, col = &quot;DarkGray&quot;, alpha = 0.8) %&gt;% gf_smooth(stat = &quot;lm&quot;) %&gt;% gf_refine(theme_bw()) Figure 9.11: Regression of waist on age It is important to remember that the regression line we just computed from sample data is an estimate of the population line. You can think of the population line as the line we would get if we had data for the whole population. As always, it is common to use Greek letters to denote population parameters and latin letters to denote their corresponding sample estimates: \\[\\text{Population line:} \\qquad \\alpha + \\beta \\ X\\] \\[\\text{Estimated line:} \\qquad a + b \\ X\\] 9.4.3 Predicted values and residuals Regression equations can be used for predictions of \\(Y\\) given an \\(X\\) value. For instance, for 65 years men, the line predicts a waist circumference value of 83.6 + 0.2 x 65 = 96.6 cm. The predictions provided by a regression equation, sometimes called fitted values, are interpreted as an estimate of the mean of \\(Y\\) given \\(X\\). This is often expressed as: \\[\\widehat{Y} = a + b \\ X\\] where \\(\\widehat{Y}\\) denotes the value predicted from \\(X\\). Hence, 96.6 cm is interpreted as the estimated mean of the waist circumference values for men being 65 year old. However, the values of waist circumference actually observed scatter around this predicted value. For any individual, the difference between his actual value of waist circumference and the value predicted by the regression line is called residual, and is usually denoted by \\(e\\): \\[e = Y - \\widehat{Y}\\] Therefore the observed value can be expressed as the sum of the value predicted by the regression line and the residual value: \\[Y \\quad = \\quad \\widehat{Y} + e \\quad = \\quad a + b \\ X + e\\] Figure 9.12 illustrates the concepts of observed, predicted and residual values. A selected case having age = 65 has been highlighted (black dot). His observed value of waist (pointed by the black arrow) is 118 cm. However, the value predicted by the regression line (pointed by blue arrow) is lower than 100. The vertical distance from the observed point to the regression line (red dashed line) is the absolute value of the residual. The sign of the residual will be positive, since \\(Y &gt; \\widehat{Y}\\), and this will be the case for any point above the regression line. Conversely, all points below the line will have a negative residual, since \\(Y &lt; \\widehat{Y}\\). Figure 9.12: Observed (black arrow), predicted (blue arrow), and residual (red dashed line) values Predicted and residual values can be easily obtained for all observations in the sample with functions of the same name. These functions need to be passed the result of lm() as argument. The following script prints predicted and residuals values for the first six cases in d: fit &lt;- lm(Waist ~ Age, data = d) predict(fit) %&gt;% head() # predicted values 1 2 3 4 5 6 88.20813 88.00782 88.00782 88.80907 88.40845 88.40845 resid(fit) %&gt;% head() # residuals 1 2 3 4 5 6 -3.0081340 -5.0078229 -0.1078229 -2.4090671 11.5915550 5.9915550 9.4.4 Inference on regression line parameters Because the regression line obtained from sample data is an estimate of the regression line in the population, it is subject to sampling variation. Consequently, we may be interested in either computing a 95% CI for the parameters, or performing tests on them. The 95% CI for the parameters can be obtained with function confint() applied to the result of lm() (saved as fit in a previous script). Similarly, the point estimates can be obtained with function coef(): coef(fit) %&gt;% round(1) # point estimates (Intercept) Age 83.6 0.2 confint(fit) %&gt;% round(1) # 95% CI 2.5 % 97.5 % (Intercept) 78.8 88.4 Age 0.1 0.3   By combining the uncertainty in the estimation of the the intercept and the slope, a confidence band can be plotted around the regression line, using argument se = TRUE in geom_smooth(). gf_point(Waist ~ Age, data = d, col = &quot;DarkGray&quot;, alpha = 0.8) %&gt;% gf_smooth(stat=&quot;lm&quot;, se = TRUE) %&gt;% gf_refine(theme_bw()) Figure 9.13: Regression line and 95% Confidence bands (gray area) The gray shaded area is the confidence band, which is interpreted as the CI for each predicted value \\(\\widehat{Y}\\). If predicted values are interpreted as estimates of the mean values of \\(Y\\) given an \\(X\\) value, the band shows the 95%CI for this mean, and it does it for all possible \\(X\\) values. Tests on the regression parameters can be done with function summary() on the fit object returned by lm(): summary(fit) Call: lm(formula = Waist ~ Age, data = d) Residuals: Min 1Q Median 3Q Max -22.616 -6.916 -0.813 6.040 55.285 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 83.60098 2.46018 33.982 &lt; 2e-16 *** Age 0.20031 0.05284 3.791 0.000189 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 10.5 on 246 degrees of freedom Multiple R-squared: 0.05519, Adjusted R-squared: 0.05135 F-statistic: 14.37 on 1 and 246 DF, p-value: 0.000189 The output shows several results: the formula of the fit object summarized (Call:), a descriptive of the residuals (Residuals), estimates and tests for the parameters (Coefficients), and overall fit measures at the end of the output. The tests on the parameters, sometimes called Wald tests, correspond to the null hypotheses that they are zero, against a two-sided alternative, that is: \\(H_0: \\quad \\alpha = 0 \\qquad and \\qquad H_1: \\quad \\alpha \\ne 0 \\qquad\\) for the intercept, and \\(H_0: \\quad \\beta = 0 \\qquad and \\qquad H_1: \\quad \\beta \\ne 0 \\qquad\\) for the slope. In most applications, the test on the intercept is not particularly interesting. Conversely, the test on the slope is always important, since a slope of zero implies that \\(Y\\) and \\(X\\) are linearly independent. In fact, this test gives the same p-value as the linear independence test based the Pearson’s correlation coefficient of section 9.3.5. In this case, the p-value is very small in both tests, providing evidence that the regression parameters \\(\\alpha\\) and \\(\\beta\\) are are non-null. Concerning the final part of the output: the Residual standard error of 10.5 is the standard deviation of the residuals. This provides a measure of how large are the residuals, on average. The lower this value, the better is the fit. Multiple R-squared, usually denoted R^2, is the square of the Pearson’s correlation coefficient, and is a measure of the goodness of the fit: the higher the R^2value, the better the fit. In general, R2 can take values from 0 (in case of independence) to 1 (in case of perfect linear relation). In this case, R^2 = 0.06 (rounded to the second decimal) denotes a very bad fit. A useful interpretation of R^2 is that it reflects the proportion of the variance (variability) of \\(Y\\) explained by \\(X\\). In this case, only the 6% of the variance of the waist circumferences can be explained by age. The Adjusted R-squared value is a penalized version of R^2 which is relevant for more complex models (models with more than one \\(X\\) variable). The F-statistic and p-value in the last line of the output is a test on the overall model, which, in the case of a single \\(X\\)-variable we are considering, gives the same result as the test for the slope of \\(X\\). In summary, there is evidence that the waist circumference increases with age (p &lt; 0.001), at an average rate of 0.2 cm per year (95% CI: 0.1 to 0.3). However, the relation is very weak since only 6% of the variability of the waist circumference values can be explained by age. 9.4.5 Assumptions and regression diagnostics The inferential methods in the previous section are based on the following assumptions: The relation of \\(X\\) and \\(Y\\) is linear. Observations are independent from each other. The prediction errors follow a normal distribution. For any given \\(X\\) value, the prediction errors have the same variance; this is called homoscedasticity (when the variance of the errors changes as \\(X\\) changes, it is called heteroscedasticity). The last two assumptions are often stated as the residuals are normally distributed, with constant variance, or the residuals are normally distributed and homoscedastic. These assumptions should be checked before relying on any inferential result in a regression analysis. The most basic, and often sufficient way to verify the assumptions, is by inspection of appropriate graphics, sometimes called regression diagnostic graphics. The linearity assumption can be easily checked in the scatterplot of \\(Y\\) by \\(X\\). The independence of observations is generally met if each point in the scatterplot belongs to a different subject. However, if two (or more) points come from the same individual (i.e., if there are individuals contributing more than one point), this assumption is violated. To check the assumption of normal and homoscedastic errors, two graphics are useful: A QQ-plot of the residuals, to check normality, and A scatterplot of the residuals vs predicted values, to check homoscedasticity. These plots can be produced with the base R function plot() on a fit object resulting from lm(). This function can produce several plots, and the which= argument is used to specify which of those we want to inspect: which=1 for the residual vs predicted plot, and which=2 for the normal QQ-plot. Here we produce the two plots for the regression of waist on age we saved previously as fit. The results are shown in figures 9.14 and 9.15. plot(fit, which = 1) # which = 1 for the residual vs predicted plot Figure 9.14: Residual vs predicted plot plot(fit, which = 2) # which = 2 for the QQ-plot Figure 9.15: Normal QQ-plot The QQ-plot of figure 9.15 shows a good approximation to the normal distribution for most of its range, with some deviation in the lower and upper tails of the distribution, and an outlier in the upper tail. Figure 9.14 shows a horizontal band, the residuals scattering around zero, with constant variability (vertical spread of the residuals). An outlier with a high positive residual (about 55 cm) is apparent, for a predicted value of 93 cm. As it is generally the case with all inferential methods assuming a normal distribution, small deviations from normality are not much of a problem. In practice, the most frequent and dangerous threat are influential outliers. In this case, some outliers appear marked in the diagnostic plots of figures 9.15 and 9.14. These are potentially influential observations, and are identified with the row number in the dataframe. A simple way to verify how much of an influence they have, is to refit the regression after elimination of these cases (note the use of the dplyr function slice() to exclude cases based on the row number): # date excluding outliers (deo) deo &lt;- d %&gt;% slice (-c(39, 41, 212)) # fit again and summarize fit lm(Waist ~ Age, data = deo) %&gt;% summary() Call: lm(formula = Waist ~ Age, data = deo) Residuals: Min 1Q Median 3Q Max -22.1234 -6.4234 -0.6027 6.3379 25.7042 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 83.407 2.234 37.333 &lt; 2e-16 *** Age 0.194 0.048 4.041 7.14e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 9.534 on 243 degrees of freedom Multiple R-squared: 0.06297, Adjusted R-squared: 0.05912 F-statistic: 16.33 on 1 and 243 DF, p-value: 7.139e-05 # plot both regression lines in the scatterplot After exclusion of the outliers, the residual standard error is slightly lower, and the R^2 value slightly higher, indicating a better fit. The parameter estimates are slightly different, but the difference is too small to have any practical relevance. This is apparent if we plot the two fitted lines on the scatterplot (note the second gf_smooth() call, with data = deo. gf_point(Waist ~ Age, data = d, col = &quot;DarkGray&quot;, alpha = 0.8) %&gt;% gf_smooth(stat=&quot;lm&quot;) %&gt;% gf_smooth(data = deo, stat=&quot;lm&quot;, color = &quot;red&quot;) %&gt;% gf_refine(theme_bw()) The two regression lines are almost identical, and therefore will produce very similar predicted values. Therefore, the three potentially influential observations did not actually influence the fit in a relevant way, and there is no reason to exclude them. Resources Correlograms are plots to visualize the results of correlation analyses. Corr is a package to explore correlations. If your variables are ordinal, then you may want to know about polychoric correlation. Learn about prediction intervals in regression analysis. You can learn more on regression diagnostic plots in this post. What if the residuals of a regression analysis are heterocedastic? Here is the answer. What if you have influentianl outliers in a regression analysis? Here is the answer. Exercises It is reasonable to expect that the physical constitution of newborns will be similar to that of their mothers, so that high-weight mothers will tend to have heavier children than a low-weight mothers. If this was true, would you expect a direct or an inverse relation of birthweights and mother’s weights? Use the birthwt dataset in package MASS to answer the following questions. Compute a new variable expressing the weight of mothers in kilograms (lwt_kg). Produce a scatterplot to investigate if birthweights are related to the mother’s weights. Are there any outliers? Compute the Pearson’s and the Spearman’s correlation coefficients. Which one would you choose and why? With the one you have chosen, test the null hypothesis that the correlation is zero in the population. Since a direct relation is expected, what would be the appropriate alternative hypothesis for a one-sided test? Conduct a one-sided test (see the help of the function to learn how to specify the appropriate alternative hypothesis). What is the p-value? Does it provide evidence of a direct relation of birthweight and the mother’s weight?   The anscombe dataset included in base R has variables X1 to X4, and Y1 to Y4 (run head(anscombe) to see he first six rows). With this dataset: Compute the Pearson’s correlations of each Y variable with corresponding X variable (e.g., Y1 with X1, Y2 with X2, and so on). What are the correlation values? For each corresponding X and Y pair, produce a scatterplot with regression line overlaid. In which of the four regressions you think that regression assumptions hold? See the Anscombe’s quartet in Wikipedia for a very short explanation of this dataset. With the anhropometric measures data: For each patient, compute the body mass index (as BMI), and the mean of his/her thorax and waist circumferences (as MTW). Subset the dataframe variables, selecting only these: BMI, Weight, Waist, Thorax, and MTW. Produce scatterplots for all pairs of the selected variables to see if there is any non-linear relation. You may want to try function pairs(d) (assuming d is the dataframe with the selected variables only). Which variable is most strongly correlated with the BMI, according to the Pearson’s correlation coefficient? You may want to try cor(d). Plot the regression line of the BMI on the TWM, and write the equation of this line. What is the rate of change (and its 95% CI) of the BMI per unit increase in the TWM? What is the percentage of the variance of BMI values that can be explained by the TWM? Produce diagnostic plots to assess the assumptions of the regression analysis. Do you think that assumptions are reasonable? Are there outliers? Refit the regression exclusing the far outlier and look at diagnostics of the new model. What is now the regression equation? And what the r-squared value? Do you think that predictions will be very different depending on the regression line used (the one fitted with all data, or the one obtained after exclusion of outliers)? Use the Australian athletes dataset (ais) in package MASS (see ?MASS:ais for details) to explore the relation between hematocrit and blood hemoglobin concentration, and answer the following questions: Is the relation linear? Are there any outliers? Is the relation weak or strong? What is the Pearson’s correlation? If we fit a regression line, what will be the proportion of the variance of the hematocrit values explained by the hemoglobin concentration? Plot the regression of hematocrit (\\(Y\\)) on blood hemoglobin (\\(X\\)), and write the equation of the minimum-squares regression line. What is the rate of change of the hematocrit for a unit change in the hemoglobin concentration? and, what the 95% CI for the rate of chage? Produce diagnostic plots to assess assumptions. Are there influential observations identified in these plots? Refit the regression after exclusion of the far outlier. Is the result very different from what you got with all observations? Plot the two lines overlaid on the scatterplot to compare them. If \\(r_{(X,Y)}\\) is the Pearson’s correlation of \\(X\\) and \\(Y\\), Is \\(r_{(X,Y)} = r_{(Y,X)}\\) ? Is the regression line of \\(Y\\) over \\(X\\), the same as the regression line of \\(X\\) over \\(Y\\)? Given the formulas for the Pearson’s correlation coefficient and the slope of the minimum-square regression line, prove that \\(b = r_{(X;Y)} \\frac{S_X}{S_Y}\\), and verify this result with in the regression of hematocrit on hemoglobin you conducted in the previous exercise. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
