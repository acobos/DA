[["index.html", "Data Analysis for Clinical Researchers Preface", " Data Analysis for Clinical Researchers Albert Cobos Preface This is an introductory book on data analysis for clinical researchers. Data analysis methods are presented through real data examples relevant to clinical research. We focus on concepts, interpretation, and statistical thinking, avoiding or minimizing the mathematical basis of statistical methods, in line with the Guidelines for Assessment and Instruction in Statistics Education (GAISE), and with modern data science. Data analysis requires the use of some software to read and analyze data, and to produce some outputs. There are lots of software tools one may use for this purpose, such as SAS, SPSS, Stata, and many other. This book is based on the R language (R, for short), a programming language for statistical computing and graphics. These are the main reasons to choose R: It is a free and open-source software under the GNU General Public License. May be used with the three major platforms: Windows, Linux and Mac. Has very good graphic capabilities. Is well documented, including an easily accessible help system. It is extensible, and has a growing and very active user community. "],["structure-of-this-book.html", "Structure of this book", " Structure of this book In chapter 1 we present the tools used throughout the book: R and RStudio. Chapter 2 is an introduction to data structures frequently used in R to store data. This is a wearisome subject, but it is important to become familiar with R. The remaining chapters follow the chronological order of tasks to be carried out in any real case of data analysis: acquire the data (chapter 3), prepare the data for analysis (chapter 4), explore the data (chapter 5), conduct inferential analyses (chapters 6-9), and report the results (chapter 10). Throughout the book you will find R tutorials in gray areas containing R code, just like this one: # This will print the date of your system in the console Sys.Date() By hovering the cursor over the top-right corner of gray area, you will be able to copy the whole of its contents, and paste it in the RStudio editor. It is extremely important that you run the R code shown in tutorials (in the first tutorial we provide detailed instructions on how to do this). R programming is a skill, and you won’t develop skills by just reading how to do something. Rather, you need to get your hands dirty by doing. In this case, this means to write R code, or at least copying and running it to see what happens. "],["introduction-to-r-and-rstudio.html", "1 Introduction to R and RStudio 1.1 Installing R and RStudio 1.2 First R session 1.3 R objects 1.4 R functions 1.5 Installing R packages 1.6 Loading packages Resources Exercises", " 1 Introduction to R and RStudio In this chapter we introduce R and RStudio. R is a programming language particularly suited for data analysis and data visualization (via graphics), and its free. Rstudio is an integrated development environment (IDE) that greatly facilitates to work with R, and there is also a free desktop version. 1.1 Installing R and RStudio First of all, you need to have R and RStudio installed in you computer. I you don’t have them, you should proceed as follows: Install the last version of R from the CRAN website; see how in this video. Install the free Desktop version of RStudio from here. Now you have both R and RStudio and, for starters, I recommend you to look at the short video Take a tour of RStudio’s IDE, here. As you have seen in the video, the RStudio screen is divided in different areas or panes, some of which have several tabs. This is a very short description of the main purpose of some of these panes and tabs: The top-left pane is and editor where you can write R code. It is likely that you won’t see this pane when you launch RStudio for the first time after installation. The bottom-left pane is the R Console, and here is where text results will be printed. You can also write R code in the console, after the prompt symbol (&gt;), and submit it by pressing Enter. The top-right pane has several tabs: The Environment tab shows the R objects created in an R session; it will be empty if you have not created any. You will wonder what is an object. Well, the best we can say by now is that anything that exists in an R session is an object. For instance, a collection of data, a graphic, or the results obtained by a statistical analysis procedure, are objects. The History tab shows a log of all the commands you have submitted. The Tutorial tab contains interactive tutorials. The bottom-right pane has several tabs: The Files tab shows the contents of the folder R is pointing to, and is similar to the file explorer in Windows. The Plots tab is where graphics are displayed. The Packages tab is to see and manage R packages. In a fresh installation of R, you will see many packages, but many more can be easily installed from this tab. The Help tab is where you can look for information of specific R commands or functions, and contains several resources to learn R and RStudio, R manuals, and more. The Viewer tab is used to show results that are formatted in specific ways, such as html. This is a lot of information, and is presented just for reference. But don’t worry, you will get familiar with all these as you use RStudio. 1.2 First R session It’s time to open RStudio and have your first R session, proceeding as follows: Open RStudio. Open a new scripting tab using the menu option File &gt; New file &gt; R script: a new tab will appear in the top-left pane, with name Untitled1. Copy the R script in the gray area below and paste it in this new tab (you can easily copy the script using the Copy to clipboard icon that will appear on hovering the cursor over the top-right corner of the gray area). Place the cursor in the very first line you pasted, and press Ctrl+Enter (or click the Run icon in the top-right corner of the Untitled1 tab). The code will be submitted to the console and the result will be printed below. In addition, the cursor will move to the next line, so that you can directly press Ctrl+Enter again to run the line. Note that texts preceded by a hash (#) are comments, and are ignored by the R interpreter. In this first tutorial we show how to use R as a calculator, and we introduce some common mathematical functions, such as sqrt(), exp(), log(), and round(). # Arithmetic operators 10 + 2 # addition 10 - 2 # subtraction 10 * 2 # multiplication 10 / 2 # division 10 ^ 2 # power # Combine them, using brackets for precedence as in standard maths notation 2 * 10 -1 # usual precedence rules 2 * (10 -1) # use brackets for precedence # Mathematical functions sqrt(25) # square root exp(1) # exponential log10(100) # decimal log log(10) # natural log # Combine functions with arithmetic operators or with other functions sqrt(25) * log10(100) / 10^2 # Nested functions: use functions within functions sqrt(exp(2)) # square root of the exponential of 2 exp(sqrt(2)) # exponential of the square root of 2 # Round to the desired number of decimals round(3.141593, 2) round(3.141593, 0) round(3.141593) Once you have finished the tutorial, you can save the script in the Untitled1 pane to a file, using the save button, or the File &gt; Save menu option. This will open a dialog from which you can navigate to a desired folder, provide a file name (e.g. my_first_R_session) and click the Save button. A new file will appear in the folder you chose, named my_first_R_session.R. The .R extension means that this is an R script file. If you save an R script to file, you will be able to reopen it in RStudio using the menu option File &gt; Open file.... We recommend you save the scripts of all the tutorials in this book, so that by the end of it you will have a nice collection of examples to refer to in the future. 1.3 R objects In the previous tutorial we produced some results that were printed in the console, but we did not save any. In this tutorial you will learn how to store your results in objects. In particular, you will learn: how to create an object by assigning some result to an object name with the assignment operator &lt;- (a handy way to compose this operator in RStudio is by pressing Alt+-). that R is case-sensitive: lowercase and uppercase letters are not the same, e.g., \\(A \\ne a\\). how to remove objects from the workspace (Environment tab), with function rm(). how to save the objects in the workspace to a file, with function save.image(), and how to load these file at a later time, with function load(). how to know what is the current working directory in an R session (the folder of your computer R is pointing to), with function getwd(). Before you start, ensure that the workspace (Environment tab of the top-right pane of RStudio) is empty. If it’s not, you can delete all objects listed by using the brush icon. # Create object by assigning a name to a value or expression, # using the assignment operator: &lt;- weight &lt;- 90 height &lt;- 1.80 BMI &lt;- weight / height^2 # These objects now appear in the Environment tab # When you create an object, its contents is NOT printed in the console # to print (the contents of) an existing object, just run its name weight BMI # Object names cannot include blank spaces body weight &lt;- 90 # this is wrong (and gives an error) body_weight &lt;- 90 # this is ok # The assignment operator is an arrow composed with a left angle bracket (&lt;) and # a hyphen or dash (-), WITHOUT blank spaces between them weight &lt; - 50 # this is wrong... but does not give error: why? # Case sensitivity bmi # gets an error: there is no object with this name BMI # ok Round(BMI, 1) # gets an error: there is no function with this name round(BMI, 1) # ok # Care when using both upper and lowercase names a &lt;- 1 A &lt;- 2 A == a # is A equal to a ? a A # Remove objects with function rm() rm(body_weight, a, A) # Now the environment has only three objects: BMI, height and weight # Save your workspace (i.e., all objects in there) to a file save.image(&quot;my_objects.RData&quot;) # A file of name &quot;my_objects.RData&quot; was saved to your working directory # The working directory is the folder R is pointing to. # But what is this folder? getwd() # Now, close RStudio and reopen it for a fresh new session, or # simulate a new session by just removing all objects in the workspace. # This will load the file you saved load(&quot;my_objects.RData&quot;) # Now you should see the objects back in the Environment tab Note that, with RStudio, you can change the working directory using the menu option Session &gt; Set working directory. Alternatively, you can use function setwd() to specify the folder in your computer you want to become the working directory, as in this example: setwd(&quot;C:/Users/acobo/Documents&quot;) A warning for Windows users: note the route of a folder shoud be specified using forward slashes (/). 1.4 R functions An R function is what we use to make something happen. Functions perform a specialized task and provide a result. In previous sections we used some functions, like log(), round(), and Sys.Date(), but there are many other built-in functions in R. In the following tutorial we discuss some general ideas about R functions, how to use them and how to get help on them. To use a function we write its name followed by brackets. Often (but not always), we write some things within the brackets which are called function arguments. A function may take one or more arguments, or no arguments at all. When a function takes more than one argument, we separate them with commas. # a function with no arguments Sys.Date() # a function call with one argument round(10/3) # a function call with two arguments (comma separated) round(10/3, 2) # a function call with three arguments (comma separated) hist(rnorm(100), main = &quot;Histogram (n = 100)&quot;, xlab = &quot;Z score&quot;) The arguments a function can take do have names. When calling a function, arguments can be identified either by name (if explicit), or by position (otherwise). For instance, the round function can take two arguments: x: the number we want to round. digits: the number of decimals we want to round x to. # arguments identified by name: order is irrelevant round(digits = 2, x = 10/3) # arguments identified by position: order is relevant! round(10/3, 2) # a mixture of both; order is relevant for arguments not explicitly named! round(10/3, digits = 2) # how to know the arguments a function can take, and their names? # looking at the function help with ?function_name, e.g.: ?round The help of a function is always structured in several sections, like Description, Usage, Arguments, Details, and Examples. In the Arguments section you can see what are the argument names, an explanation of what they are, and their order. You can access the help of a function by running its name (without the brackets!) after a question mark (e.g., ?round), or writing the function name in the search box of the RStudio Help tab of the bottom-right pane. 1.5 Installing R packages R is a software structured in packages. A package is a bunch of functions packed together. When you install R, a collection of packages is downloaded to your computer. If you look at the Packages tab of RStudio (bottom-right pane) you will see the packages that are already installed. For instance, you will see packages base and stats, since they come with the basic installation of R. However, there are many other packages available in the CRAN repository, and you need to know how to install them. For instance, there is a package called mosaic you will probably not find among your installed packages. Since we will use this package in future chapters, it’s a good idea to install it. To install a package, click the Install button in the Packages tab of RStudio and you will be presented the dialog shown in figure 1.1. Figure 1.1: Dialog to install a package You only need to write the name mosaic in the Packages box (or pick it from the list that will appear as you start writing) and click on the Install button. Be sure the Install dependencies box is checked (dependencies are other packages needed by the one you are installing). You will notice that, when you click the Install button, the following is written in the R console: install.packages(&quot;mosaic&quot;) This means that the install.packages() function is what is actually used to install packages, and the Install button in RStudio only sends this to the console. After running this function, you will see a confirmation message in the console (package ‘mosaic’ successfully unpacked and MD5 sums checked). Installing a package means downloading it to your computer, and therefore is a one time operation: you need to do it just once per R installation. 1.6 Loading packages In previous sections we have used functions from packages in the base R installation such as round(), log() and others. These functions are readily available in any R session, but this is not the case of functions contained in packages you install. For instance, the following script calls favstats(), a function from the mocaisc package, but when we run it we get an error message stating that function favstats() was not found: favstats(age ~ sex , data = HELPrct) Error in favstats(age ~ sex, data = HELPrct): could not find function &quot;favstats&quot; To use a function from packages you have installed, you need to do something special, and there are two options: prefix the function with the package name plus ::, as in mosaic::favstats(), or load the package first using function library(). # Option 1 mosaic::favstats(age ~ sex , data = HELPrct) sex min Q1 median Q3 max mean sd n missing 1 female 21 31 35 40.5 58 36.25234 7.584858 107 0 2 male 19 30 35 40.0 60 35.46821 7.750110 346 0 # Option 2 library(mosaic) favstats(age ~ sex , data = HELPrct) sex min Q1 median Q3 max mean sd n missing 1 female 21 31 35 40.5 58 36.25234 7.584858 107 0 2 male 19 30 35 40.0 60 35.46821 7.750110 346 0 As you see in the previous scripts, both options work, but while library(mosaic) in option 2 loads the entire package into memory, prefixing the function with the package name (option 1) does not. Another important difference is that prefixing the function with the package name makes explicit the function you are using, and this may be important because some packages have functions sharing their name with some base R functions. For instance: The base package has a function mean() (see ?base::mean). The mosaic package has a function mean() (see ?mosaic::mean). Then, which one is going to be used when we call mean()? Well, if you load the mosaic package with library(mosaic), the following warning is printed in the console: The following objects are masked from ‘package:base’: max, mean, min, prod, range, sample, sum) This means that, from now on in your session, every time you call one of the functions mentioned in the above message (such as mean()), the one from mosaic will be used. However, when you look at a long script and see a call to mean(), it may be hard to know if this refers to mean() from the base package, or to mean() from the mosaic package. Prefixing a function with the package name (e.g., mosaic::mean()) removes any possible ambiguity, and for this reason, is considered a good programming practice. Last, in case you prefer to avoid prefixing functions with the package name, remember to load the package first with library(), and to do it in every new R session. While installing a package is a one time operation, loading an installed package is an operation you need to do in every R session (every time you open RStudio). Resources CRAN is the official website of R. Here you can not only download R, but also find useful information, such as the CRAN task views providing guidance on which packages are relevant for tasks related to a certain topic. The RStudio website has a Resources section where you can find webinars and videos, cheatsheets and books, among other things. A two-pages RStudio IDE Cheat Sheet. R-bloggers is a blog on R publishing lots of articles on the use of R, including tutorials for learning R. A post is R-bloggers on how to use the help page for a function in R Another post is R-bloggers with a complete guide to installing R packages form CRAN and other R package repositories. Quick-R is an extremely well organized website on R, particularly useful for beginners. There are sections on data management, statistics, graphics and more. Exercises Look at the help of round() and read sections Description and Usage. What do you get when using floor(10/3) and ceiling(10/3)? In the Usage section you will see round(x, digits = 0). What do you think digits = 0 means? (hint: read the explanation of round in the Description section). Start at fresh RStudio session (or empty the workspace) and write a script in a new editor tab (File &gt; New File &gt; R script) to: Create an object with name my_weight taking the value of your body weight, in kilograms. Create an object with name my_height taking the value of your body height, in meters (remember to use the dot as decimal separator, e.g. 1.78) Compute the body mass index (by dividing the weight over the square of the height) and assign it to an object with name BMI. Round the value of BMI to the first decimal. Look for a package called dplyr in the Packages tab of RStudio. If you are working with a new installation of R, you will not find it. Then, install this package using the Install button in this tab. When the installation finishes, verify that this package now appears in the Packages tab. Open RStudio and investigate what is your working directory with function getwd(). Then change it to another folder of your preference with the menu option Session &gt; Set working directory &gt; Choose Directory.... When done, use again getwd() to see what is now the working directory. "],["r-data-structures.html", "2 R data structures 2.1 Vectors 2.2 Lists 2.3 Dataframes 2.4 Factors 2.5 Dates 2.6 Other data structures Resources Exercises", " 2 R data structures The term data structures refers to objects devoted to store data. There are many different data structures in R. In this chapter we introduce the most basic, and commonly used ones. 2.1 Vectors Vectors are the simplest possible objects in R (sometimes called atomic objects). They are ordered collections of elements. There are six types of vectors in R, but here we will be concerned with the three types listed in table 2.1. Table 2.1: Vector types Type Elements are: Example numeric (real) numbers 1, 0, 3.14, 1984 character (quoted) character strings “Yes”, “No”, “Maybe” logical (unquoted) logical values TRUE, TRUE, FALSE, TRUE A vector can contain any number of elements, but all of them have to be of the same type: we cannot mix different data types in a single vector (e.g., character and numeric elements). In the following tutorial you will learn how to create vectors with function c(), verify what is the type of a vector with function class(), and what is the number of its elements with function length(). A vector can have missing elements, and these are represented with the symbol NA. Last, you will see how to get particular elements in a vector with the subsetting operator [], and how to set them using the assignment operator &lt;-. # Creating vectors with function c() ages &lt;- c(51, 65, 90, 93, 72) # a numeric vector ages antec &lt;- c(&quot;stroke&quot;,&quot;AMI&quot;,&quot;Breast ca.&quot;) # a character vector antec current_smoker &lt;- c(FALSE, FALSE, TRUE) # a logical vector current_smoker mixed &lt;- c(67, &quot;stroke&quot;, FALSE) # try to mix types: no error issued ? mixed # but coerced to character (note the quotes) # Getting the type of a vector class(ages) class(antec) class(current_smoker) class(mixed) # Getting the number of elements in a vector length(ages) length(antec) length(current_smoker) # Use the symbol NA for missing elements more_ages &lt;- c(69, NA, 37) more_ages all_ages &lt;- c(ages, more_ages) # can concatenate vectors with c() all_ages # Getting/setting vector elements # Refer to an element by position using square brackets: all_ages all_ages[7] # gets the 7th element all_ages[7] &lt;- 65 # sets the 7th element to 65 all_ages # verify all_ages[3:6] # gets elements from positions 3 to 6 Very often we need of perform arithmetic operations with numeric vectors. For instance, we may want to compute the body mass index (BMI) from a vector of body weights and a vector of body heights. Then, we may want to compute summary statistics of the BMI values, like the mean and standard deviation. weight &lt;- c(51, 65, 90, 93, 85) # weights of five patients height &lt;- c(1.65, NA, 1.85, 1.80, 1.60) # heights of the same five patients # Arithmetic operations bmi &lt;- round(weight / height^2, 1) # compute BMI bmi # result is NA if height or weight is NA # Statistical functions mean(weight) # the mean of weights median(weight) # the median sd(weight) # the standard deviation summary(weight) # some of the previous at once # Care with missings! If a vector contains missing data (NA), # you need to set argument na.rm = TRUE in most statistical functions: mean(bmi) # since some values are NA, result is NA mean(bmi, na.rm=TRUE) # removes NA for calculation sd(bmi) # same for sd() sd(bmi, na.rm=TRUE) summary(bmi) # but not needed for summary() Also, a very common task is to compare the elements of a numeric vector to some threshold value using relational operators such as ==, !=, &gt;, or &gt;=. We can also compare two vectors x and y element-wise, provided they have the same length (e.g., x == y). In any case, the result of a comparison is always a logical vector. Note that the equality operator is not a single (=) but a double equal sign (==): try to remember this, it’s a very common mistake! Another very common mistake is to compare something to NA using the equality operator (==): this does not work, and function is.na() should be used for this purpose. Comparisons are illustrated in the following tutorial, which is a continuation of the previous one (do not empty the workspace!). # Relational operators for comparisons bmi == 18.7 # is bmi equal to 18.7? note the DOUBLE equal sign! bmi != 18.7 # is x different from 18.7? bmi &lt; 25 # is bmi less than 25? bmi &lt;= 25 # is bmi less than, or equal to 25? bmi &gt; 30 # is bmi greater than 30? bmi &gt;= 30 # is v greater than, or equal to 30? # Can also compare two vectors having the same number of elements x &lt;- c(1, 0, 0, 0, 1, 0, 0, 0, 1) y &lt;- c(0, 0, 1, 1, 1, 1, 1, 0, 0) x == y x != y x &gt; y x &lt;= y # Very important! bmi == NA # this does not work with NA is.na(bmi) # use this instead Character vectors are useful to store textual information like diseases, symptoms or drugs. However, working with character data (sometimes called strings) is often a challenge for several reasons. First, a word may be written in lowercase, uppercase, or a mixture of both. Because R is case sensitive, “stroke” is not the same as “STROKE”. Second, white spaces are also characters (though they do not catch the eye!) and therefore “stroke” and “stroke \\(\\;\\)” are also different. For these reasons, it is sometimes useful to convert character vectors to uppercase (or lowercase), and to know how to remove leading or trailing blanks. Here is a demo: # Working with character vectors antec &lt;- c(&quot;stroke&quot;, &quot; stroke &quot;, &quot;Breast cancer&quot;, &quot;STROKE&quot;, &quot; Stroke &quot;) # Remember case sensitivity: # &quot;stroke&quot;, &quot; stroke &quot;, &quot;STROKE&quot;, and &quot; Stroke &quot; are different things! antec == &quot;stroke&quot; toupper(antec) # convert to uppercase tolower(antec) # convert to lowercase trimws(antec) # remove leading and trailing blanks tolower(antec) == &quot;stroke&quot; # useful for comparisons trimws(tolower(antec)) == &quot;stroke&quot; # better A very useful function to combine strings is the paste() function. In fact, you can combine not only strings, but also numeric values (that will be coerced to character). Sometimes we need to know what is the number of characters of the elements of a character vector, and this can be done with function nchar(). patient_no &lt;- 1:5 age &lt;- c(62, 88, 35, 75, 81) sex &lt;- c(&quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;) paste(patient_no, sex) # paste corresponding elements paste(patient_no, sex, sep = &quot;: &quot;) # can define a separator (default is a blank space) # Can paste more than two vectors, and mix with constants strings (&quot;Patient&quot;, &quot;is a&quot;) # note that constants are recycled (i.e., reused for all elements) narrative &lt;- paste(&quot;Patient&quot;, patient_no, &quot;is a&quot;, age, &quot;years old&quot;, sex, &quot;with antecedent of&quot;, trimws(tolower(antec))) narrative # Number of elements and number of characters length(narrative) # the number of elements in vector nchar(narrative) # the number of characters of each element Other common tasks with the strings are to extract just a part of them, or to substitute a part of them. These can be done with functions substr() (for substring) and sub() (for substitute), respectively. In both functions we indicate the character vector from which to extract or substitute a part in the first argument (named x). In substr() we need to specify the part we want to extract from x by indicating the first and last positions in arguments start and stop, respectively. In sub() we need to specify the pattern to be replaced, the replacement string, and the addition argument fixed = YES to avoid alternative interpretations of the pattern argument 1. # Extract a part of the string substr(narrative, start = 16, stop = 23) # characters 16 to 23 substr(narrative, 16, 17) # characters 16 to 17 substr(narrative, 16, nchar(narrative)) # characters 16 to last # Replace a part of the string sub(pattern = &quot;is&quot;, replacement = &quot;was&quot;, x =narrative, fixed = TRUE) Whatever the type of a vector, its elements can be named. We can provide names for the elements of a vector when creating it with function c(), or later with names() &lt;- . x &lt;- c(Peter = 31, Paul = 28, Mary = 25) x names(x) # gives the names as a character vector names(x)[3] # therefore, you can refer to any element names(x)[3] &lt;- &quot;Adeline&quot; # or set any element x y &lt;- 1:3 # a numeric vector names(y) # whose elements have no names names(y) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) # but we can set them later y Last, it is possible to make type conversions. For instance, you may need to convert a numeric vector containing zeros and ones to a logical vector, or viceversa. Sometimes, you want to extract part of a string containing digits with substr(). However, because a part of a string is by definition a string as well (even if it contains only digits!), we need to convert these to numeric. Other conversions are less common but possible, using a family of functions, the as. family. In some instances, type conversions are done automatically. For example if we try to sum the elements of a logical vector: because addition is an operation defined on numbers, logical values will be coerced to numeric behind the scene, as shown at the end of the following tutorial on type conversions. number_of_seizures &lt;- 0:4 number_of_seizures # numeric to logical (rule: 0 -&gt; FALSE, anything else -&gt; TRUE) seizures &lt;- as.logical(number_of_seizures) seizures # character to numeric x &lt;- substr(narrative, 16, 17) # extracts the age x class(x) # but the result is a character vector age &lt;- as.numeric(x) # now it&#39;s a numeric vector age class(age) # logical to numeric (rule: FALSE -&gt; 0, TRUE -&gt; 1) diabetes &lt;- c(TRUE, TRUE, FALSE, FALSE) as.numeric(diabetes) # character to numeric or logical (less common) z &lt;- c(&quot;0&quot;, &quot;One&quot;,&quot;2&quot;,&quot;120&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;, &quot;Are you tired?&quot;) z as.numeric(z) # only digits converted to numbers as.logical(z) # only words &quot;TRUE&quot; and &quot;FALSE&quot; converted to logical # Automatic conversion from logical to numeric (rule: FALSE -&gt; 0, TRUE -&gt; 1) sum(diabetes) Ops! just one more thing. In R, scalars (single numbers) are considered vectors of length one. Same for a single character string, or a single logical value. So, nothing exists in R simpler than a vector! x &lt;- 3.14 class(x) is.vector(x) length(x) x &lt;- &quot;A&quot; class(x) is.vector(x) length(x) x &lt;- FALSE class(x) is.vector(x) length(x) 2.2 Lists In the previous section we have seen that vectors cannot mix data types, but lists can. In fact, lists can mix not only data types, but object types. Lists are arbitrary collections of objects (including other lists!), and therefore provide a highly flexible structure to store data of any kind. Lists can be created with function list(), by enumerating its elements separated by commas. The length of a list is the number of elements it contains, and can be obtained with length(). # Creating a list x &lt;- list(&quot;male&quot;, # 1st element 79, # 2nd element c(&quot;arterial hypertension&quot;, &quot;diabetes&quot;, &quot;stroke&quot;), # 3rd element matrix(c(260,110, 190, 100, 185, 100, 160, 95), nrow=2, # 4th element dimnames = list(BP = c(&quot;SBP&quot;, &quot;DBP&quot;), day = 1:4))) x class(x) # object type? length(x) # number of elements it contains When working with lists, two common operations are subsetting and extraction: Subsetting a list means taking a part or subset of the list elements, and the result is always a list (even when we subset just one of its elements!); this is done with the subsetting operator: []. Extracting from a list, means getting an element of the list, and the result will not be a list, but an object of the class this element belongs to; this is done with the extraction operator: [[]]. When applied to vectors, these two operators produce the same result (x[3] == x[[3]]), but this is not the case with lists. For instance, in the example above, we created a list with data from a patient, the second of which elements is the age, a numeric vector of length one. Suppose we want to get this value and plug it in some mathematical formula. In this case we need to extract this element (x[[2]]), the result being a numeric vector we can use in arithmetic expressions. However, if we subset the second element of the list (x[2]), the result will be a list of length one (but still a list!), and we cannot plug a list in a mathematical expression. The following tutorial demonstrates this important difference. # Subset a list with the SUBSETTING operator: [] x[1:2] class(x[1:2]) # the result will be ALWAYS another list x[2] # even when subsetting just one element! class(x[2]) # see? # Extract an element from a list with the EXTRACTION operator: [[]] x[[2]] class(x[[2]]) # the result is the element class (numeric vector in this case) # We can use numeric vectors in mathematical expressions, but NOT lists! x[[2]] + 10 # is OK x[2] + 10 # is WRONG People is often confused about the difference between x[2] and x[[2]]. If this is your case, think of a list as a set of numbered boxes, each one containing something. Suppose that box number 2 contains a sandwich. Subsetting the list means taking some of the boxes, maybe just one, maybe just box number 2, but you still have a box in your hands and you can`t eat a box! If you want to eat your sandwich, you open the box to extract your sandwich, and then you eat it. If you are hungry, you better understand the difference between subsetting a list and extracting from a list. Just like vectors, lists can be named, and the names of elements can be provided when creating the list, or later with names(). In the previous tutorial we saw how to extract a single element from a list indicating its position with the extraction operator [[]]. When a list has names, we can extract elements by position, but also by name, using either the extraction operator or the construct listname$elementname. The last is very practical, and is shown below: # Provide names when creating the list x &lt;- list(sex = &quot;male&quot;, age = 79, antecedents = c(&quot;arterial hypertension&quot;, &quot;diabetes&quot;, &quot;stroke&quot;), matrix(c(260,110, 190, 100, 185, 100, 160, 95), nrow=2, dimnames = list(BP = c(&quot;SBP&quot;, &quot;DBP&quot;), day = 1:4))) x names(x) # get the names names(x)[4] # 4th element has no name names(x)[4] &lt;- &quot;BP&quot; # let`s set it names(x) # all named now # Extract an element x[[1]] # by position x[[&quot;BP&quot;]] # by name x$age # by name, using $ x$sex x$antecedents x$BP Lists are very important data structures for several reasons, but one of them is because the results produced by many functions in R, including functions for statistical analysis, are stored in a list. For instance, in the following tutorial we use a binomial test2 to evaluate if fathering a boy or a girl are equal likely, from a series of 100 deliveries. Then, we inspect the result to realize it is a list, we investigate the name of its elements, and we extract some of them. # A binomial test for the probability of boy being 0.5 # suppose we observe n = 100 births, among which x = 45 are boys res &lt;- binom.test(x = 45, n = 100) res class(res) # class &quot;htest&quot; is a specialized type of list, but still a list is.list(res) # see? names(res) # get the names of its elements res$p.value # get the p.value res$estimate # get the proportion of boys res$conf.int # get the confidence interval for this proportion 2.3 Dataframes Dataframes are named lists of vectors, all having the same length. Because they are lists, they can mix vectors of different types, making dataframes ideal to store tabular data (arranged in rows and columns). Dataframe vectors are the columns of a data frame, and are usually called variables. Dataframes can be created with data.frame(), by enumerating its vectors separated by commas. These can be already existing vectors, or vectors we create on the fly. The length()of a dataframe is the number of variables it contains. Because dataframes are named lists of vectors, we can refer to these vectors using the construct dataframename$variablename. This can be used in conjunction with the assignment operator to add a new variable to a dataframe, or to remove an existing variable from a dataframe (by passing it NULL), as shown in below. sex &lt;- c(&quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;) w &lt;- c(60, 69.5, 83.9, 76.3, 77.3, 88.8) # Creating a dataframe from vectors of the same length) d &lt;- data.frame(case = 1:6, # vector created on the fly age = c(25, 32, 19, 41, NA, 56), # same sex, # already existing vector weight = w) # same, but give it a name d class(d) length(d) # number of variables (columns) ncol(d) # same nrow(d) # the number of rows names(d) # get variable names as a character vector # Get dataframe variables d[[2]] # by position d$age # by name class(d$age) # class of this object? class(d$sex) # Note that sex and d$sex are two different objects sex == d$sex # equal by now sex[3] &lt;- NA # change sex sex # verify change d$sex # d$sex remains unaltered # Add variable to a dataframe d$height &lt;- c(165, 175, 158, 180, 175, 160) length(d) d # Remove variable from a dataframe d$case &lt;- NULL length(d) d The data.frame() function is not the only way to create a dataframe. In the next chapter you will learn how to read data stored in an external file (e.g., an Excel file), resulting in a dataframe. This is in fact a very common way to create a dataframe. 2.4 Factors A factor is the data structure used in R to represent categorical variables. Categorical variables are classifications having a number of classes, categories or levels. Examples of categorical variables are sex (male or female), smoker (never smoked, ex-smoker or current smoker) or previous surgery (yes or no). Data on a categorical variable is usually stored as either a character vector of level descriptors (e.g., “male”, “male”, “female”, …), or as a numeric vector using numeric codes (e.g., 1 for males, 2 for females). Factors are created with function factor(). The first argument to this function is the (numeric or character) vector containing the data. If numeric, we need to clarify the meaning of numeric codes, with two additional arguments: levels: to specify the numeric codes (as a numeric vector) labels: to specify the corresponding texts (as a character vector) The following script shows how to create a factor from a numeric vector. We first create a numeric vector x using the rep() function (to repeat values 1 and 2, five times), and then define a factor from it. x &lt;- rep(1:2, 5) # numeric vector using rep() x # Creating a factor from a numeric vector sex &lt;- factor(x, levels = c(1,2), labels = c(&quot;male&quot;, &quot;female&quot;)) sex class(sex) A couple of things are worth noting in the previous example. First, both levels and labels have to be passed as vectors. Second, it is the order of levels and labels what establishes their correspondence: first level with first label, second level with second label, and so on. Last, when a factor (like sex) is printed to the console, its values are not surrounded by quotes (as is the case with character vectors), and the levels of the factor are stated after printing the values. Let’s now see how to define a factor from a character vector. In this case, if the strings of this vector are self-explanatory (e.g., “male”, o “female”), we only need to specify the character vector from which the factor is to be created. However, the levels will be defined in alphabetical order, and this is not always what we want. If we want a specific ordering of the levels, we can just define them in the levels argument. Last, if the strings are not self-explanatory, or we want to change them, we can also use the labels argument. The following script illustrates these three options. x &lt;- rep(c(&quot;low&quot;, &quot;normal&quot;, &quot;high&quot;), 2) x # option 1: levels and labels not explicitly defined hb &lt;- factor(x) hb class(hb) levels(hb) # option 2: define levels to force desired ordering hb &lt;- factor(x, levels = c(&quot;low&quot;, &quot;normal&quot;, &quot;high&quot;)) hb class(hb) levels(hb) # option 3: define levels and labels x &lt;- rep (c(&quot;l&quot;, &quot;n&quot;, &quot;h&quot;), 2) hb &lt;- factor(x, levels = c(&quot;l&quot;, &quot;n&quot;, &quot;h&quot;), labels = c(&quot;low&quot;, &quot;normal&quot;, &quot;high&quot;)) hb class(hb) levels(hb) You may be wondering why we should care about the ordering of the levels of a factor. The reason is this ordering has implications for graphics, tables and statistical analyses. For instance, when we tabulate results of a factor, its levels appear in the order they are defined. Also, the first level of a factor, which is sometimes called the reference level, is the one other levels will be compared to in some analyses such as ANOVA or regression models. For this reasons, it is convenient to ensure a reasonable ordering of factor levels. 2.5 Dates Date is a class of objects used to represent dates in R. There are two related object classes that are used to represent date-time values (values that combine date and time, such as “21/03/2020 07:45:28”): POSIXct, and POSIXlt. When we have just date values (e.g., “21/03/2020), the appropriate class to use is Date. Dates can be created from a character vector with as.Date(). The format argument is used to specify the formatting of the character strings. If these are formatted as yyyy-mm-dd, we don’t need to specify the format value, but otherwise we need to specify it. Dates can be created from POSIXct and POSIXlt objects as well. # From a character vector of yyyy-mm-dd values x &lt;- c(&quot;1958-03-05&quot;, &quot;1991-10-28&quot;) x class(x) dob &lt;- as.Date(x) dob class(dob) # From a character vector of dd/mm/yyyy values x &lt;- c(&quot;05/03/1958&quot;, &quot;28/10/1991&quot;) x class(x) dob &lt;- as.Date(x, format = &quot;%d/%m/%Y&quot;) dob class(dob) # see help on format symbols (Details section) ?strptime # From POSIXct or POSIXt values x &lt;- as.POSIXct(c(&quot;1958-03-05 07:30:04&quot;, &quot;1991-10-28 10:05:39&quot;)) x class(x) dob &lt;- as.Date(x) dob class(dob) The reason why we should store dates in a Date object is to allow computations. For instance, we can compute the current age from birth dates. Or, we can determine the date in ninety days from today. When we operate on dates, the result is not a standard numeric value, but a difftime object. However, we can convert it to numeric with as.numeric(). This is recommended before further operations with time intervals. dob &lt;- as.Date(c(&quot;1958-03-05&quot;, &quot;1991-10-28&quot;)) # birth dates dob class(dob) today &lt;- Sys.Date() # current date today class(today) today + 90 # 90 days from today # Current age age &lt;- today - dob1 # difference as number of days age class(age) # a difftime object # Use as.numeric to get standard numeric vector age &lt;- as.numeric(today - dob1) age class(age) # To compute age in years we divide the NUMERIC version by 365.25 age_in_years &lt;- as.numeric(today - dob1) / 365.25 age_in_years # Use floor() to get whole years age_in_years &lt;- floor(as.numeric(today - dob1) / 365.25) age_in_years 2.6 Other data structures The data structures presented in previous sections are those you will be using again and again, so it’s really important you get familiar with them. In this section we briefly introduce a couple of additional data structures, because you may find them when looking at the help of some functions. These are matrices and arrays. A matrix is just a vector with a dim attribute defining the dimensions of the matrix, that is, the number of rows and columns. A matrix can be created with function matrix(), or by defining the dim attribute of a vector with dim(). Like vectors, matrices cannot mix data types (e.g., character and numeric). x &lt;- 1:10 # a vector x class(x) # Create a matrix of 2 rows and 5 cols m &lt;- matrix(x, nrow = 2) m dim(m) class(m) dim(x) # vectors do not have a dim attribute dim(x) &lt;- c(2, 5) # but we can define it x # and now its a matrix class(x) dim(x) Matrices are a special case of a more general data structure called array. In fact, a matrix is an array with two dimensions (rows and columns) but, in general, arrays can have multiple dimensions. Arrays are created with function array(), or by defining the dim attribute of a vector. x &lt;- 1:30 # Creating an three-dimensional array a &lt;- array(x, dim = c(3, 5, 2)) a class(a) dim(a) dim(x) # vectors do not have a dim attribute dim(x) &lt;- c(3, 5, 2) # but we can define it x # and now its an array class(x) dim(x) For more information on matrices and arrays see this section of the first edition of the book Advanced R by Hadley Wickham. Resources A very useful base R cheat sheet: a pdf with only two pages! Objects types and some useful R functions for beginners is a post in R-bloggers where you can find many of the concepts we covered in this chapter, and more. Fore a more thorough discussion of what we covered in this chapter, look at Data structures in the first edition of the book Advanced r by Hadley Wickham. Exercises Before undertaking each exercise, ensure your workspace is clean (i.e., there are no objects in the Environment pane). Create a vector w of body weights in kilograms, and a vector h of body heights in centimeters, with the following values corresponding to 10 patients: w: 47.3, 87.0, 80.9, 85.1, 95.4, 90.0, 70.3, 46.9, 86.7, 74.0 h: 160, 165, 180, 170, 190, 180, 172, 156, 170, 169 Then compute the body mass index (kg/m^2), store it as bmi, and write appropriate expressions to: Verify that bmi is a numeric vector. Get the number individuals with low weight (BMI below 18.5). Get the number individuals with overweight (BMI of 25 or more). Get the number individuals with obesity (BMI of 30 or more). Get the mean and standard deviation of bmi values. Hint: remember that the result of a comparison is a logical vector, and that the sum of a logical vector is the number of TRUEs it contains. LETTERS and lettersare built-in vectors containing all the letters in the alphabet (print them in the console to see them). What is length(letters)? Using these vectors, get the following: The letter in position 20. A character vector with the first five LETTERS only. A character vector whose first element is “A is an uppercase LETTER, and a is a lowercase letter” (and similar statements for the remaining letters). Hint: use the paste() function. A character vector whose first element is “A is in position 1 of the alphabet” (and similar statements for the remaining letters). Hint: use 1:length(LETTERS) in the paste() function. What is the result of 1:length(LETTERS)? The following are five CIP codes, whose structure is as follows: the first four characters are initials of surname and first name, the first digit is a code for sex (0 for male, 1 for female), followed by birth date in format yyddmm, and three trailing zeros: YAGU0741511000, DEFO0702702000, GITI1642102000, AOPU1620401000, BABE0740206000 Define vector cip containing these CIP codes and then: Get a vector containing the two digits corresponding to the year of birth. Is it a numeric vector? Get a vector of four-digit years of birth by adding 1900. Using only the year of birth (ignore month and day of birth), get an estimate of their ages by 2050. Get a vector with first element “YAGU was born in 1974” (and analogous for the remaining persons). See the help of the rep() function by running ?rep, and look at the first two examples. Use this function to create three vectors containing the following elements: 0, 1, 0, 1, 0, 1 0, 0, 0, 1, 1, 1 “male”, “female”, “male”, “female”, “male”, “female” rep(0:1, 3) rep(0:1, each = 3) rep(c(&quot;male&quot;, &quot;female&quot;), 3) See the help of the seq() function by running ?seq, and look at the examples. Use this function to create vectors containing: All multiples of five up to 100. All odd numbers between zero and ten. The following four probabilities: 0.2, 0.4, 0.6, 0.8. Run the following code to conduct a t-test on the lwb data in package COUNT (you need to install this package if you do not have it!): library(COUNT) data(lbw) res &lt;- t.test(bwt ~ smoke, data = lbw) res What is the class of object res? Is it a list? What are the names of the elements in res? Get the p-value of the test, with 3 decimals. Get the means of bwt for smokers and non-smokers, rounded to one decimal. Get their difference with function diff(), and round to one decimal. Get the confidence interval for this difference, rounded to one decimal. In one step, create a dataframe with these data: first_name age sex Peter 33 male Paul 29 male Mary 25 female Then extend this dataframe with the following additional variables: case_id containing numbers 1 to 3. age_in_10y reflecting the age they will have in 10 years from now. older_than_30 as TRUE or FALSE. is_male as TRUE of FALSE. Remove variable is_male from the dataframe What is now the length of the dataframe? and what is the number of rows? Using the rep() function, create numeric vector x with the sequence 1,2,3 repeated 3 times (so 9 values), an then define a factor from x with labels “good”, “neutral” and “bad”. Create a character vector x with two repetitions of the vector c(“never-smoker”, “ex-smoker”, “current-smoker”), and then define a factor from x. Is the ordering of levels good enough? Redefine the factor with a better ordering. Create two Date objects: one with today’s date, and the other with your date of birth. Then, use these objects to: Compute your age in days Compute your age in (whole) years Print your date of birth, including the weekday the full name of the month and the year with four digits When fixed = FALSE (the default value), this is interpreted as a regular expression; regular expressions are a powerful way to manipulate strings, but are out of the scope of this book.↩︎ This and other statistical tests will be introduced in future chapters. Our purpose here is to show how the results of statistical tests are organized, not their meaning.↩︎ "],["data-acquisition.html", "3 Data Acquisition 3.1 Reading MS Excel data 3.2 Reading text data 3.3 Reading SPSS, SAS or Stata data 3.4 Reading databases 3.5 Reading other formats 3.6 Getting data from R packages 3.7 Problems when importing data from external files Resources Exercises", " 3 Data Acquisition Data acquisition is the process of importing data into your R session, so that it can be viewed and analyzed. There are several ways in which you can get data, but the usual case is to read an external data file. Common data file formats are Excel, plain text, SPSS, SAS or Stata data files. It is very easy to read files in these file formats by clicking the Import Dataset button of the Evironment pane in RStudio; this will unfold a list of file formats, and you just need to select the appropriate one. Upon selection, a new window will show-up from which we can pick the data file and provide information on how to read it. This information is partially different depending on the format, and in the following sections we show how to proceed in each case. 3.1 Reading MS Excel data In the Import Dataset button of the Evironment pane of RStudio, select From Excel..., and you will be presented with a dialog like the one in figure 3.1. Figure 3.1: Dialog to read Excel files The File/URL box at the top of this dialog is where we should write the Excel file address. If the data file is located in you computer, you can pick it using the (top-right) Browse button. If the Excel file is somewhere in the internet, you can write (or paste) the URL in the box. As soon as you pick the file (or write its URL), you will see the result of a default reading in the Data Preview box. You should always look at the result of the reading in the Data Preview to ensure that you get what you want. In some cases, the default reading will be enough, but in some others you will realize that there is some problem. Then, you can use the Import options as appropriate: Name: to provide a name for the resulting dataframe; this is automatically filled but you can change it to any valid name. Sheet: to indicate the sheet you want to read (if there is more than one sheet in the Excel file). Range: to indicate the range of cells to read, in standard Excel syntax (e.g., B3:F51). Max rows: to indicate the maximum number of rows you want to read (may be handy for test-reading of very large files). Skip: to start reading data after skipping the indicated number of rows. NA: to define the character(s) used in the Excel file to represent missings. First row as names: check if the first row that will be read contains column names. Open data viewer check to open the resulting dataframe in a new tab at the top-left RStudio pane. Last, the Code preview box shows the R code that will be executed when you click the Import button. 3.2 Reading text data There are two entries in the list under the Import Dataset for reading text data: From Text (base) and From Text (readr). We recommend the later, which opens the dialog of figure 3.2. Figure 3.2: Dialog to read text files Some of the Import options are similar to those commented in the previous section for Excel files (Name, Skip and First Row as Names, NA, Open Data Viewer), but the following are specific of text data files: Trim spaces: to trim leading and trailing white spaces for each data point Delimiter: to define what is used to separate data points Quotes: to indicate the type of quotes used for texts (if any) Locale: to indicate the configuration of locale settings, such as the decimal mark, the thousands separator, or the time zone. Escape: does the file use backslash to escape special characters? or does the file escape quotes by doubling them? Comment: to indicate the symbol used in the file for comments (that should be ignored) Before reading a text data file, it is very important to inspect how the data are arranged. This can be done by opening the file with any text editor, like the Notepad in Windows, or with RStudio (using the menu File &gt; Open file...). For instance, figure 3.3 shows a text data file containing variable names in the first row, where data are delimited by tabs, texts are unquoted, and the dot is used as the decimal separator. Figure 3.3: Text data file (delim: tab) As a second example, the text data file shown in figure 3.4 also contains variable names in the first row, but data are delimited by semicolons, texts are quoted (with double quotes), and uses the dot as the decimal separator. Figure 3.4: Text data file (delim: semicolon) 3.3 Reading SPSS, SAS or Stata data The dialog opened by options From SPSS, From SAS and From Stata are very similar. In figure 3.5 we show the former, where there is only one active Input Option refering to the input data file: Format: to specify the type of SPSS file (default: .SAV) Figure 3.5: Text data file (delim: semicolon) 3.4 Reading databases There are many different database management systems (DBMS) that are widely used, like MySQL, PostgreSQL, Microsoft Access and SQL Server, Oracle, or Sqlite. To read a database (DB) with R we need to establish a connection to the the DB, either using specific drivers provided by the BD vendor, or using open DB connectivity (ODBC), a way to access any SQL-based DBMS which is implemented in the R package RODBC In the following tutorial we show how to connect to a Microsoft Access DB, using the RODBC package. It is important to note that this will only work when your are using 32-bits R to read 32-bits DB drivers. After installing and loading the RODBC package, a connection is established to the Access DB with function odbcConnectAccess(). Then, the sqlTables() function is used to get a list of all tables in the DB. To get a specific table from the DB we use function sqlFetch(). It is important to close the connection when the job is done. library(RODBC) con &lt;- odbcConnectAccess(&quot;./data/demo.mdb&quot;) # establish connection to the DB sqlTables(con) # print table-like objects in the DB d &lt;- sqlFetch(con, &quot;demo&quot;) # gets the table demo d class(d) # the result is a dataframe close(con) # don`t forget to close the connection! rm(con) # before removing it!! 3.5 Reading other formats Besides the facilities described in previous sections to read common data file formats from RStudio, many other file formats can be read with R. These are some of them: PDF files can be read with package pdftools (see this example). HTML, XML: there are several packages to work with HTML and XML files, but package xml2 can be use to read both (see here). JSON files can be read with package rjson (see this tutorial). Web pages: the rvest package, is useful for webscraping (getting data from web pages) (see this tutorial). There are several packages that can be used to read/write and visualize geospatial data (maps) (see this CRAN Task View). There are several packages to read/write and visualize medical images in formats DICOM, NALYZE or NIfTI-1 (see this CRAN Task View for more info). Genetic data can be imported from a wide range of formats, including those of popular population genetics software (GENETIX, STRUCTURE, Fstat, Genepop) with package adegenet (see here for more info). This list is by no means comprehensive. If you are interested in reading a particular file format not covered here, it is very likely you will find useful info by Googleing “R (format_name)”. 3.6 Getting data from R packages Many R packages contain example data sets. In the base installation of R you will find thedatasets package (look for it in the Packages tab of RStudio, and click on the name of the package) that contains many example datasets. However, only a few are related to clinical research (e.g., the Indometh dataset). A (non-comprehensive) list of clinical datasets contained in R packages can be found here. To work with any of these datasets you need to install the corresponding package, and then look at the help of the dataset. For instance, the MASS package includes the birthwt dataset. The following script prints its first rows with head() (six rows are printed by default, but you could indicate a different number in a second argument): head(MASS::birthwt) # this will print the first 6 rows low age lwt race smoke ptl ht ui ftv bwt 85 0 19 182 2 0 0 0 1 0 2523 86 0 33 155 3 0 0 0 0 3 2551 87 0 20 105 1 1 0 0 0 1 2557 88 0 21 108 1 1 0 0 1 2 2594 89 0 18 107 1 1 0 0 1 0 2600 91 0 21 124 3 0 0 0 0 0 2622 For a brief description of the data, look at the help after loading the package by running ?birthwt. Since the beginning of the COVID-19 pandemics, several resources have been developed with R, including R packages (see a list here). Among them, the COVID19 package provides a unified dataset by collecting worldwide fine-grained case data that can be easily extended with World Bank Open Data, Google Mobility Reports, and Apple Mobility Reports (see this short tutorial). In addition, some R packages provide more or less easy access to many different databases of interest for clinical researchers, such as the following: rentrez is a package for retrieving data from the National Center for Biotechnology Information (NCBI), including PubMed and GenBank. rclinicaltrials provides an interface to ClinicalTrials.gov, a registry and results database of clinical studies conducted around the world. nhanesA allows retrieval of data from the National Health and Nutrition Examination Survey (NHANES) conducted by the National Center for Health Statistics (NCHS). WHO allows to download public health data from the World Health Organization’s Global Health Observatory. wbstats provides access to the World bank data, a comprehensive source of global socio-economic data. Some additional R packages to access open public health data are commented in this post. 3.7 Problems when importing data from external files It is very common to have problems when reading real-life data, especially when data have been entered manually performing no validation. This is often the case when data are entered in Excel spreadsheets (though Excel has some data validation tools, these are ignored by many users). Some of the most frequent problems are: Inadequate variable names Incorrect reading of numeric data Incorrect reading of dates In this section we show how to deal with these problems using the dplyrpackage, which is very convenient to work with dataframes. 3.7.1 Package dplyr Before proceeding, you should install and load the dplyrpackage: library(dplyr) The dplyr package provides several functions to perform common tasks, and function names are verbs describing the task they perform. Some of these functions are: select(): selects variables from a dataframe (and optionally renames them) filter(): filters rows of a dataframe according to some logical condition mutate(): creates new variables in a dataframe rename(): renames variables in a dataframe arrange(): changes the ordering of the rows of a dataframe The first argument to all these (and other) dplyr functions is a dataframe, and all of them provide a dataframe as a result. Each function takes other function-specific arguments. Table 3.1 shows some examples of use of these functions with a description of the result they produce. In these examples, we assume that d is a dataframe containing variables sex, age, race, weight (in kilograms) and height (in centimeters). Table 3.1: Example of dplyr function calls Function call Result: a dataframe with… select(d, sex, age, race) all rows in d, and variables sex, age, race. select(d, gender=sex, age, race) all rows in d, and variables gender, age, race. filter(d, sex == \"male\") males in d, and all variables. rename(d, gender = sex) all rows in d, and variables gender, age, race, weight, height. mutate(d, height_meters = height/100) all rows and variables in d, plus new variable height_meters arrange(d, age) all rows and variables in d, with rows sorted by To note in the examples of table 3.1: variable names are neither prefixed with the dataframe name, nor quoted. a double equal sign is used to specify equality conditions, as in filter(d, sex == \"male\"). a single equal sign (instead of the assignment operator &lt;-), is used to define new variables with mutate(), as in mutate(d, height_meters = height/100). When you load the dplyr package, you can use the so called pipe operator (%&gt;%). As you will see soon, this allows to chain operations in a very practical way. The basic usage of the pipe is shown in table 3.2: Table 3.2: Usage of the pipe with dplyr functions This is equivalent… to this select(d, sex, age, race) d %&gt;% select(sex, age, race) filter(d, sex == \"male\") d %&gt;% filter(sex==\"male\") mutate(d, height_meters = height/100) d %&gt;% mutate(height_meters = height/100) arrange(d, age) d %&gt;% arrange(age) This works because d is the first argument of select(), filter(), mutate() and arrange(). You can use this operator with any function (not just with functions from dplyr), provided that the object before %&gt;% is the first argument of the function after %&gt;%. Table 3.3 shows a basic usage of the pipe with base R functions (in the first example we assume d is some existing dataframe): Table 3.3: Usage of the pipe with base R functions This is equivalent… to this names(d) d %&gt;% names() round(2.38, 1) 2.38 %&gt;% round(1) log(100) 100 %&gt;% log() What is really powerful about the pipe operator is that you can chain as many operations as you wish, provided you stick to the rule: what goes before %&gt;% is the first argument of the function after %&gt;%. For instance, the following two expressions are equivalent: round(log(100), 4) # nested functions [1] 4.6052 100 %&gt;% log() %&gt;% round(4) # chained functions [1] 4.6052 Notice that it is much more difficult to write the first of these expression: while you think “compute the log of 100 and round the result to four decimals”, nesting functions requires you to work from the inside out. However, when chaining operations with the pipe you follow the natural order of operations: “take 100, then compute the log, then round to four decimals”. As you see, each pipe can be read as then, or more precisely, then do whatever the next function indicates, taking the previous result as its first argument. Chaining can be applied to dataframe operations as well, as in the following example: take d, then filter males, then compute the height in meters, and then select the two height variables. # creating an example dataframe d &lt;- data.frame(sex = rep(c(&quot;male&quot;, &quot;female&quot;), 3), age = c(19, 23, 45, 38, 57, 61), height = c(180, 165, 174, 175, 168, 177)) # chaning operations with the pipe (%&gt;%) d %&gt;% filter(sex == &quot;male&quot;) %&gt;% mutate(height_meters = height/100) %&gt;% select(height, height_meters) height height_meters 1 180 1.80 2 174 1.74 3 168 1.68 We have only scraped the surface of the dplyr package, and there is much more power in it for dataframe operations. But what we covered should be enough to start using it as we do in the following section. 3.7.2 Reading the SARA data To illustrate problems that appear quite often when reading a real dataset, we will use a simplified version of the data collected in the SARA trial. This was a randomized clinical trial that compared catheter ablation vs. antiarrhythmic drug treatment in patients with persistent atrial fibrillation. The data has been modified in several ways for didactic purposes, and therefore the results shown here may differ from those described in the paper. You can download this file from here. We first read the dataset, and inspect variable names: library(readxl) raw_data &lt;- read_excel(&quot;data/SARA_simplified.xlsx&quot;) names(raw_data) [1] &quot;Patient no&quot; &quot;Birth Date (yyyymm-dd)&quot; [3] &quot;Sex&quot; &quot;Date of randomization&quot; [5] &quot;Random allocation of Treatment&quot; &quot;Body Weight at baseline (kg)&quot; [7] &quot;Body Height at baseline (cm)&quot; &quot;NYHA classification at baseline&quot; [9] &quot;SBP at baseline (mmHg)&quot; &quot;DBP at baseline (mmHg)&quot; [11] &quot;Heart Rate at baseline (bpm)&quot; The variable names contain blank spaces and some special characters like brackets, and are too long. We better clean them using function clean_names() from the janitor package, and revise the result: library(janitor) d &lt;- clean_names(raw_data) # clean variable names names(d) # review [1] &quot;patient_no&quot; &quot;birth_date_yyyymm_dd&quot; [3] &quot;sex&quot; &quot;date_of_randomization&quot; [5] &quot;random_allocation_of_treatment&quot; &quot;body_weight_at_baseline_kg&quot; [7] &quot;body_height_at_baseline_cm&quot; &quot;nyha_classification_at_baseline&quot; [9] &quot;sbp_at_baseline_mm_hg&quot; &quot;dbp_at_baseline_mm_hg&quot; [11] &quot;heart_rate_at_baseline_bpm&quot; Note that spaces have been replaced with underscores, and special character like brackets have been eliminated. This function made a good job, and in many instances the result will be good enough to proceed. In this case however, variable names are too long, so we rename them using the rename() function of dplyr: d &lt;- rename(d, # rename vars patient = patient_no, birth_dt = birth_date_yyyymm_dd, rand_dt = date_of_randomization, group = random_allocation_of_treatment, weight = body_weight_at_baseline_kg, height = body_height_at_baseline_cm, nyha = nyha_classification_at_baseline, sbp = sbp_at_baseline_mm_hg, dbp = dbp_at_baseline_mm_hg, hr = heart_rate_at_baseline_bpm) names(d) # review [1] &quot;patient&quot; &quot;birth_dt&quot; &quot;sex&quot; &quot;rand_dt&quot; &quot;group&quot; &quot;weight&quot; [7] &quot;height&quot; &quot;nyha&quot; &quot;sbp&quot; &quot;dbp&quot; &quot;hr&quot; Much better. Now, let’s have a first look at the data by printing the first rows of the dataframe: head(d) # prints first six rows # A tibble: 6 × 11 patient birth_dt sex rand_dt group weight height nyha sbp &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 00:00:00 1 11/05/2009 Cathet… 104 188 1 110 2 1-002 1970-02-27 00:00:00 1 12/05/2009 Antiar… 92 182 1 NA 3 1-003 1964-02-21 00:00:00 2 11/06/2009 Cathet… 95 177 1 125 4 1-004 1962-01-04 00:00:00 2 20/07/2009 Cathet… 80 162 1 139 5 1-005 1974-06-02 00:00:00 1 17/08/2009 Antiar… 93 175 NA 122 6 1-006 1964-06-22 00:00:00 1 21/09/2009 Cathet… 115 185 1 120 # … with 2 more variables: dbp &lt;dbl&gt;, hr &lt;dbl&gt; In the previous output, the header of each data frame column includes the variable name and the type of data is indicated with abbreviations: &lt;chr&gt; for character, &lt;dttm&gt;for datetime, and &lt;dbl&gt; for double (which is a type of numeric). The following is to note: birth_dt is a datetime object, but time values are 00:00:00, and we want it to be a Date object anyway. rand_dt is character, but we want it to be Date as well. weight is character, but we expected a numeric. But let’s verify the object classes of these three variables: class(d$birth_dt) [1] &quot;POSIXct&quot; &quot;POSIXt&quot; class(d$rand_dt) [1] &quot;character&quot; class(d$weight) [1] &quot;character&quot; Yes, these objects classes are as indicated by the abbreviations in the output above. The problem with weightdeserves some investigation. When a variable containing digits is not recognized as numeric, it is likely due to some value(s) containing forbidden characters for numeric data. To verify this, we use mutate() to create a numeric version of weight with as.numeric(), and then filter the rows where the numeric version is missing but the original weight is not. Last, we select() the patient number and the two versions of weight, and we store the resulting dataframe as commas for future reference: commas &lt;- d %&gt;% mutate (weight_numeric = as.numeric(weight)) %&gt;% filter(is.na(weight_numeric) &amp; !is.na(weight)) %&gt;% select(patient, weight, weight_numeric) commas # A tibble: 9 × 3 patient weight weight_numeric &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1-034 67,2 NA 2 1-052 84,5 NA 3 1-053 99,5 NA 4 2-022 92,5 NA 5 6-002 85,5 NA 6 6-013 83,5 NA 7 7-003 85,5 NA 8 7-008 91,5 NA 9 7-014 92,5 NA There are nine patients with decimals in weight that could not be interpreted as numbers because the comma (instead of a dot) was used as decimal separator. Now we need to replace this commas with dots, and this can be done with function sub(). Once the decimal separator problem has been fixed, we can safely convert to numeric. Last, we filter patients in commas to verify the result. d %&gt;% mutate(weight_fixed = sub(pattern = &quot;,&quot;, replacement = &quot;.&quot;, x = weight, fixed=TRUE), weight_numeric = as.numeric(weight_fixed)) %&gt;% select(patient, weight, weight_fixed, weight_numeric) %&gt;% # filter patients filter(patient %in% commas$patient) # A tibble: 9 × 4 patient weight weight_fixed weight_numeric &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1-034 67,2 67.2 67.2 2 1-052 84,5 84.5 84.5 3 1-053 99,5 99.5 99.5 4 2-022 92,5 92.5 92.5 5 6-002 85,5 85.5 85.5 6 6-013 83,5 83.5 83.5 7 7-003 85,5 85.5 85.5 8 7-008 91,5 91.5 91.5 9 7-014 92,5 92.5 92.5 Note in the script above we have defined two new variables in the mutate() function call (weight_fixed and weight_numeric), separated by comma. Moreover, note that the second variable is defined from the first. This means that, in a single call of mutate(): we can define as many new variables as needed (comma separated), and new variables are immediately available once defined We are left with the problem of dates. This can be fixed with function as.Date(): d %&gt;% mutate(birth_dt = as.Date(birth_dt), rand_dt = as.Date(rand_dt, &quot;%d/%m/%Y&quot;)) # A tibble: 152 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 1 2009-05-11 Catheter… 104 188 1 110 70 2 1-002 1970-02-27 1 2009-05-12 Antiarrh… 92 182 1 NA NA 3 1-003 1964-02-21 2 2009-06-11 Catheter… 95 177 1 125 70 4 1-004 1962-01-04 2 2009-07-20 Catheter… 80 162 1 139 63 5 1-005 1974-06-02 1 2009-08-17 Antiarrh… 93 175 NA 122 74 6 1-006 1964-06-22 1 2009-09-21 Catheter… 115 185 1 120 80 7 1-007 1950-06-30 1 2009-10-01 Catheter… 89 177 1 126 102 8 1-008 1955-03-29 1 2009-10-06 Catheter… 77 178 1 NA NA 9 1-009 1941-07-25 2 2009-11-10 Antiarrh… 62 162 1 140 85 10 1-010 1955-04-22 1 2009-11-17 Antiarrh… 86 168 1 122 83 # … with 142 more rows, and 1 more variable: hr &lt;dbl&gt; Finally, we can perform all the operations we have done by chaining them with pipes, so that we do everything in a single code chunk. This is done in the following script. Note that in this case the problem with commas in weight has been fixed in a single step, and the result overwrites the weight variable, to avoid additional variables in the dataframe. library(readxl) library(janitor) library(dplyr) d &lt;- read_excel(&quot;data/SARA_simplified.xlsx&quot;) %&gt;% # reads data clean_names() %&gt;% # for well formed names rename(patient = patient_no, # for shorter names birth_dt = birth_date_yyyymm_dd, rand_dt = date_of_randomization, group = random_allocation_of_treatment, weight = body_weight_at_baseline_kg, height = body_height_at_baseline_cm, nyha = nyha_classification_at_baseline, sbp = sbp_at_baseline_mm_hg, dbp = dbp_at_baseline_mm_hg, hr = heart_rate_at_baseline_bpm) %&gt;% mutate(weight = as.numeric(sub(pattern = &quot;,&quot;, # commas to dots, replacement = &quot;.&quot;, # and convert to x = weight, # ...numeric fixed=TRUE)), birth_dt = as.Date(birth_dt), # convert to Date rand_dt = as.Date(rand_dt, &quot;%d/%m/%Y&quot;)) d # A tibble: 152 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 1 2009-05-11 Catheter… 104 188 1 110 70 2 1-002 1970-02-27 1 2009-05-12 Antiarrh… 92 182 1 NA NA 3 1-003 1964-02-21 2 2009-06-11 Catheter… 95 177 1 125 70 4 1-004 1962-01-04 2 2009-07-20 Catheter… 80 162 1 139 63 5 1-005 1974-06-02 1 2009-08-17 Antiarrh… 93 175 NA 122 74 6 1-006 1964-06-22 1 2009-09-21 Catheter… 115 185 1 120 80 7 1-007 1950-06-30 1 2009-10-01 Catheter… 89 177 1 126 102 8 1-008 1955-03-29 1 2009-10-06 Catheter… 77 178 1 NA NA 9 1-009 1941-07-25 2 2009-11-10 Antiarrh… 62 162 1 140 85 10 1-010 1955-04-22 1 2009-11-17 Antiarrh… 86 168 1 122 83 # … with 142 more rows, and 1 more variable: hr &lt;dbl&gt; And that’s it, at least by now! Resources A comprehensive tutorial on R data import (including some formats we have not mentioned) can be found here. Data Import Cheat Sheet For more information on the use of package janitor, see this tutorial. For more information on the use of package dplyr, see this tutorial. To easily read many different data formats with a single function, see this introduction to the rio package. Exercises Download this Excel file and read it using the Import Dataset button in the Environment pane of RStudio. How many variables are in the resulting dataframe, and what are their names? Get well formed names with function clean_names() of package janitor. Download this Excel file and read it using the Import Dataset button in the Environment pane of RStudio. How many variables are in the resulting dataframe, and what are their names? Give appropriate names to the dataframe variables. Read this text data file. Why do you think the Weight variable is read as character vector? Look at the character used for missings. Can you imagine a way to read this correctly, so that Weight is a numeric vector? (Hint:explore changing the NA Import Optionto NA, and look at the Code Preview; maybe if you copy this code to the editor you can adapt it to indicate that the NA symbol is…). Read this text file. Are the names of the countries properly written? Try to use the Locale Import Option to set the Encoding to “ISO-8859-1”. Download this SPSS file and read it using the Import Dataset button in the Environment pane of RStudio. What is the class of the Sex variable in the resulting demo dataframe?. Now run this code d &lt;- as_factor(demo). What is the class of Sex in d? Read the Nobel Price Laureates dataset with RStudio and list the variable names of the resulting laureate dataframe. Then, use janitor::clean_names() to get a dataframe with better names, and save it as d. With d, do the following: Look at the class of variables born and died. If not Date, convert this variables to standard dates. Nobel Laureates are awarded the Nobel prize in a ceremony held the 10 December every year, on the anniversary of Alfred Nobel’s death. Use paste() to build a character vector containing the dates the laureates were awarded the prize. Convert the previous result to a Date object and use it to compute the age each laureate was awarded the Nobel Prize. Filter laureates for the “medicine” category and save the resulting dataframe as medicine. Use count(medicine, born_country) to get the number of medicine laureates per country of birth; look at the help of count() to see how to get the result sorted by frequency. Use the same function to get the number of medicine laureates per name, and then filter institutions with more than one laureate. Filter medicine to see the laureates that, when awarded the prize, where working in a country different from their born_country. How many of them were in a different country? "],["data-preparation.html", "4 Data preparation 4.1 Steps in data preparation 4.2 Reading raw data 4.3 Reviewing data 4.4 Modifying data 4.5 Computing new variables 4.6 Selecting valid cases 4.7 Saving the R script Resources Exercises", " 4 Data preparation Data preparation is the process by which we generate tidy data from raw data. Raw data is the original version of the data as stored during collection, whatever its source, format, structure and state. It is very common that raw data need some tweaks to make them ready for statistical analysis. These tweaks may involve simplifying the data structure, providing appropriate labels for coded variables, detecting and amending errors, recovering missing data, or computing new variables from those available. Tidy data is what we get after these tweaks: a version of the data ready for analysis. For a set of data to be considered tidy it should have the following structure: Each variable is a column in a dataframe Each observation is a row in a dataframe Each type of observational unit is a dataframe For instance, suppose we conducted a study on 100 patients and collected data on demographic and anthopometric variables age, sex, and body height and weight. The observational unit is then the patient, and these data can be arranged in a dataframe, having columns for patient, age, sex, height and weight, so that all data for one patient fits in a single row (see table 4.1). Table 4.1: Demographic and anthropometic data patient age sex height weight 1 67 male 180 91 2 42 male 168 75 3 39 female 176 69 4 … … … … 5 … … … … Now consider data on vital signs such as pulse, systolic and diastolic blood pressure, taken in three visits for the same set of patients. In this case, the observational unit is not the patient, but the visit of a patient, and data should be stored in a dataframe having patient, visit, temperature, pulse, systolic and diastolic blood pressure as columns, so that all data of one visit (of a given patient) fits in one row (see table 4.2). Table 4.2: Vital signs patient visit pulse sbp dbp 1 1 77 120 85 1 2 85 145 90 1 3 69 110 65 2 1 70 120 85 2 2 80 135 85 2 3 … … … 3 1 … … … Last, consider data on adverse events collected during a clinical trial. For each event, some characteristics are recorded, such as the duration(days), severity, actions taken, and outcome. In this case, the observational unit is the adverse event, and the data should be stored in a dataframe with columns patient, event, start date and stop date, severity, actions taken, and outcome. Again in this case, all data for an event fits in a single row (see table 4.3. Table 4.3: Adverse events patient visit event duration actions outcome 1 1 Headache 1 none recovered 2 1 Nausea 2 none recovered 2 2 Vomiting 1 none recovered 2 3 Abdominal cramps 1 none recovered 3 1 Hip fracture 1 surgery recovered 3 2 … … … … 4 1 … … … … You will note that the dataframe of figure 4.1 will have as many rows as patients in the study, that is, one-hundred. However, the dataframe of figure 4.2 will have three rows per patient, therefore totaling three-hundred rows (assuming all patients were visited three times). Last, the number of rows in the dataframe of figure 4.3 cannot be known in advance, because a patient can experience no AE at all, one AE or many AE. It should be clear that the structure of these three dataframes is different, and that is why we need three dataframes to accommodate all the data. Trying to put all of it in a single dataframe would result in undesirable things, such as having different columns containing the same type of data (such as AE1, AE2, AE3, …), and structural missings (AE2, AE3, … will be missing for patients experiencing just one AE, but this missings do not reflect lack of data, and are only due to an inappropriate data structure). For the sake of simplicity, in this chapter we will be concerned with the case where all the available data can be stored in a single dataframe having the simplest possible structure, just like the one in figure 4.1. If this is the case, each row will correspond to a patient, and each column to a variable. This should be enough for many simple observational studies collecting a limited amount of data on a single time point. But even in this simple case, it is important to have a unique case identifier; this is always convenient, and it is critical when the data has to be split in different dataframes, to allow record linkage. In any case, it is good practice to create dataframes with the following additional characteristics: Variable names should be short but meaningful, and should contain neither spaces, nor non-standard characters (like slashes, or accents in Spanish words). Variable names may follow different styles, like CAPITALIZED, camelCase, or snake_case, but whatever the style, consistency is a plus. In these book we use the snake_case style. Categorical variables should have meaningful levels (e.g., “male” or “female” rather than 1 or 2, or “m”, or “f”), and should be factors rather than character vectors. Quantitative variables should have the units of measurement documented somewhere, though not necessarily in the variable name (in fact this is not recommended, to keep variable names as short as possible, and free of special characters such as brackets, slashes or Greek letters); the study protocol is a reasonable place to document variable units. A minimal information on the study design, or on how the data were collected, should be available somewhere (e.g., in the study protocol). It is very important that the data preparation process is traceable and reproducible. To ensure traceability and reproducibility, this process should be done programmatically rather than manually. Manual editions of the data are not reproducible (unless very well documented, which takes a lot of time and is prone to errors), and should be avoided. In this chapter we will see what are the most common tasks in data preparation and how to accomplish them following this principle by writing R scripts. 4.1 Steps in data preparation It is convenient to approach data preparation in a systematic way. The following is a reasonable order of the most common data preparations tasks: Reading raw data Reviewing data (looking for problems: missings and errors) Modifying data (to fix problems) Computing new variables Selecting valid cases Saving the R script performing steps 1 to 5 above All the data preparation tasks in steps 2 to 5 above, no matter how complex, can be accomplished using base R. However, some of them are easier using the dplyr package introduced in the previous chapter, and this is the approach we will follow, with few exceptions. 4.2 Reading raw data Reading external data files was addressed in the previous chapter, where we read the SARA data with the following script: library(readxl) library(janitor) library(dplyr) d &lt;- read_excel(&quot;data/SARA_simplified.xlsx&quot;) %&gt;% clean_names() %&gt;% rename(patient = patient_no, birth_dt = birth_date_yyyymm_dd, rand_dt = date_of_randomization, group = random_allocation_of_treatment, weight = body_weight_at_baseline_kg, height = body_height_at_baseline_cm, nyha = nyha_classification_at_baseline, sbp = sbp_at_baseline_mm_hg, dbp = dbp_at_baseline_mm_hg, hr = heart_rate_at_baseline_bpm) %&gt;% mutate(weight = as.numeric(sub(pattern = &quot;,&quot;, replacement = &quot;.&quot;, x = weight, fixed=TRUE)), birth_dt = as.Date(birth_dt), rand_dt = as.Date(rand_dt, &quot;%d/%m/%Y&quot;)) head(d) # A tibble: 6 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 1 2009-05-11 Catheter … 104 188 1 110 70 2 1-002 1970-02-27 1 2009-05-12 Antiarrhy… 92 182 1 NA NA 3 1-003 1964-02-21 2 2009-06-11 Catheter … 95 177 1 125 70 4 1-004 1962-01-04 2 2009-07-20 Catheter … 80 162 1 139 63 5 1-005 1974-06-02 1 2009-08-17 Antiarrhy… 93 175 NA 122 74 6 1-006 1964-06-22 1 2009-09-21 Catheter … 115 185 1 120 80 # … with 1 more variable: hr &lt;dbl&gt; We will use this d dataframe to illustrate the data preparation tasks covered in this chapter. 4.3 Reviewing data Data review is a must with any real data set. You cannot assume your data is free of errors. Rather, you should assume that data has errors unless you prove otherwise. The very first test of a dataset should be to verify that the unique case identifier is really unique, i.e., that all patients have a different value in this variable. The unique() function applied to a vector returns all distinct values with no repetitions (if any). Then, the length() of the resulting vector will be the number of distinct values, and this can be compared to the number of patients (rows) in the dataset: length(unique(d$patient)) # number of distinct values in d$patient [1] 152 length(unique(d$patient)) == nrow(d) # is it equal to the number of rows in d? [1] TRUE Now we are sure that there are no two patients with the same case identifier (otherwise the previous comparison would have produced a FALSE). 4.3.1 Missings Because missing data is a very common problem in real life datasets, we should always start by looking at the number of missings in the data. There are several functions for this purpose, but the simplest one is the base R function is.na(). In previous chapters we used this function on a vector, and summed the result to get the number of missings: sum(is.na(d$weight)) # missings in weight [1] 19 Interestingly, we can use is.na() with dataframes as well. In this case, the result will be a matrix of logical values, the columns (and rows) of which correspond to the columns (and rows) of the dataframe. The colSums() function applied to this matrix will result in the number of missings for each column in the original dataframe. By nesting both functions, a single line of code is enough to get the number of missings in each variable of a dataframe, as shown below (you can do it in two steps if you want to see the matrix produced by is.na(d)) colSums(is.na(d)) # missings in columns of d patient birth_dt sex rand_dt group weight height nyha 0 0 0 0 0 19 22 2 sbp dbp hr 14 14 0 Another option is provided by the plot_missing() function in package DataExplorer, showing the number (and percentage) of missings for each variable in a graphic: DataExplorer::plot_missing(d) 4.3.2 Data errors Errors in the data are very common, and we should attempt to detect at least gross errors before proceeding with the analysis. Error detection may be easy or very difficult depending on the type of variable and the type of error. Gross errors in numeric variables and dates can be easily detected by looking at their extreme values (minimum and maximum). These (among other statistics) are provided by summary() for each numeric variable in a dataframe: summary(d) patient birth_dt sex rand_dt Length:152 Min. :1939-06-24 Min. : 1.000 Min. :2009-05-07 Class :character 1st Qu.:1949-11-09 1st Qu.: 1.000 1st Qu.:2010-01-17 Mode :character Median :1954-03-30 Median : 1.000 Median :2010-08-25 Mean :1955-11-06 Mean : 1.362 Mean :2010-08-27 3rd Qu.:1962-01-01 3rd Qu.: 1.000 3rd Qu.:2011-03-22 Max. :1983-06-22 Max. :22.000 Max. :2011-11-24 group weight height nyha Length:152 Min. : 50.00 Min. : 83.0 Min. :1.00 Class :character 1st Qu.: 74.00 1st Qu.:165.0 1st Qu.:1.00 Mode :character Median : 84.50 Median :174.0 Median :1.00 Mean : 85.75 Mean :172.0 Mean :1.26 3rd Qu.: 95.00 3rd Qu.:179.8 3rd Qu.:1.00 Max. :167.00 Max. :193.0 Max. :3.00 NA&#39;s :19 NA&#39;s :22 NA&#39;s :2 sbp dbp hr Min. :100.0 Min. : 60.00 Min. : 4.00 1st Qu.:119.2 1st Qu.: 70.00 1st Qu.: 57.00 Median :127.0 Median : 80.00 Median : 66.00 Mean :127.4 Mean : 80.42 Mean : 71.14 3rd Qu.:137.0 3rd Qu.: 87.00 3rd Qu.: 82.00 Max. :197.0 Max. :110.00 Max. :150.00 NA&#39;s :14 NA&#39;s :14 In the previous output we see the maximum value of sex is 22, which is an obvious error. There is also a suspicious maximum of 167 (kg) in weight, a very suspicious minimum of 83 (cm) in height, and an impossible minimum heart rate (hr) of 4 (bpm). However, the previous output is not useful for the group variable, and this is because it’s a character vector. The result would be useful if group was a factor (we would then see each possible value and its frequency), but we have not defined factors yet. Note that summary() also provides information on the number of missings (NA's). We can inspect the values of a categorical variable stored in a character vector using unique(). This will print all distinct values appearing in the vector, without repetitions: unique(d$group) [1] &quot;Catheter ablation&quot; &quot;Antiarrhythmic drug treatment&quot; unique(d$sex) # useful for coded categorical variables as well [1] 1 2 22 unique(d$nyha) [1] 1 NA 2 3 Sometimes it is worth looking at a graphic combining two variables whose values are related. For instance, we may look at the join distribution of weightand height as done in figure 4.1 (these and other graphics will be presented in detail in the next chapter). Figure 4.1: Weight and height By looking at this figure we confirm the weight value 167 kg is an error, since it corresponds to the patient with the minimum height value of 83 cm. This error is likely due to a permutation of heigh and weight values for this patient, which is a common data entry error. Interestingly, there is another patient showing an unusual combination of weight (125 kg) and height (about 160 cm). Though these values have nothing strange when we consider them separately, their combination is unlikely. For this reason, this error was not detected when we looked at these variables separately by inspecting their extreme values, but are easily detected in the plot above. Once we have detected problematic values in our data, we need to investigate what are the patients affected by these errors. # Patients with errors d %&gt;% filter(sex == 22) %&gt;% select(patient, sex) # A tibble: 1 × 2 patient sex &lt;chr&gt; &lt;dbl&gt; 1 3-001 22 d %&gt;% filter(hr &lt; 40) %&gt;% select(patient, hr) # A tibble: 1 × 2 patient hr &lt;chr&gt; &lt;dbl&gt; 1 6-030 4 d %&gt;% filter(weight &gt; 124) %&gt;% select(patient, weight, height) # A tibble: 2 × 3 patient weight height &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4-001 167 83 2 5-007 125 158 # Patients with missings d %&gt;% filter (is.na(weight) | is.na(height) | is.na(nyha)| is.na(sbp) | is.na(dbp)) %&gt;% select(patient, weight: dbp ) # A tibble: 30 × 6 patient weight height nyha sbp dbp &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-002 92 182 1 NA NA 2 1-005 93 175 NA 122 74 3 1-008 77 178 1 NA NA 4 2-002 93 180 1 NA NA 5 2-003 93 179 1 NA NA 6 2-005 95 NA 2 120 70 7 2-016 65 158 1 NA NA 8 2-017 120 192 1 NA NA 9 3-004 70 175 NA 130 70 10 5-005 NA NA 1 135 75 # … with 20 more rows Now we should appeal to source documents (such as the clinical records) and try to recover the missing or erroneous data for these patients. 4.4 Modifying data Suppose we have recovered the following data from clinical records, and the remaining missing data could not be recovered: patient variable correct_value 1-005 nyha 1 3-004 nyha 1 3-001 sex 1 4-001 weight 83 4-001 height 167 6-030 hr NA To set these data points to their correct values it is very practical to use vector subsetting. When we subset a vector by a condition which is either TRUE or FALSE for each patient, the returned values are those for whom the condition is TRUE. Thus, if we subset a variable using a condition that identifies a single patient, we refer to the value of this variable for this patient. Note that we need to prefix the variable names with the dataframe names, since this is base R (not dplyr): d$sex[d$patient == &quot;3-001&quot;] # sex for patient 3-001 [1] 22 Then, we can use this to set a new value for this data point: d$sex[d$patient == &quot;3-001&quot;] &lt;- 1 # sex for patient 3-001 d$sex[d$patient == &quot;3-001&quot;] # verify [1] 1 In this manner, we can set all the remaining correct values: d$nyha[d$patient == &quot;1-005&quot;] &lt;- 1 d$nyha[d$patient == &quot;3-004&quot;] &lt;- 1 d$weight[d$patient == &quot;4-001&quot;] &lt;- 83 d$height[d$patient == &quot;4-001&quot;] &lt;- 167 d$hr[d$patient == &quot;6-030&quot;] &lt;- NA Finally, we can verify if everything went as expected by printing data for the patients we set new (correct) values. To filter these patients we use %in% operator, so that d rows will be filtered if the patient is one of those specified in the character vector after %in%. d %&gt;% filter(patient %in% c(&quot;1-005&quot;, &quot;3-001&quot;, &quot;3-004&quot;, &quot;4-001&quot;, &quot;6-030&quot;)) %&gt;% select(patient, sex, weight, height, nyha, hr) # A tibble: 5 × 6 patient sex weight height nyha hr &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-005 1 93 175 1 70 2 3-001 1 101 187 1 75 3 3-004 1 70 175 1 60 4 4-001 1 83 167 1 51 5 6-030 1 NA NA 1 NA It seems that everything is fine now! 4.5 Computing new variables Once all data problems are fixed, the next step is to define factors for all categorical variables, and to compute new variables derived from those in the data. Computing new variables may involve different type of operations, such as using computational formulas, making conditional assignments, categorizing a quantitative variable, recoding categorical variables or manipulating character strings. All these cases are very common, and are illustrated in the following sections. 4.5.1 Defining factors Dataframe d includes the following categorical variables: sex is a categorical variable coded as 1 (for males), or 2 (for females). nyha is a categorical variable coded with numbers 1 to 3 to represent NYHA classes I, II and III respectively. group is a a categorical variable stored as a character vector, with self-explanatory values but a bit too long to be practical (for example, when producing graphics, long strings are usually a problem); we can take the chance of defining a factor to shorten the descriptors as “CA” for Catheter ablation and “ADT” for Antiarrhythmic drug treatment. The following script defines factors for these three variables and overrides3 d with the result. d &lt;- d %&gt;% mutate(sex = factor(sex, levels = 1:2, labels = c(&quot;male&quot;, &quot;female&quot;)), nyha = factor(nyha, levels = 1:3, labels = c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;)), group = factor(group, levels = c(&quot;Catheter ablation&quot;, &quot;Antiarrhythmic drug treatment&quot;), labels = c(&quot;CA&quot;, &quot;ADT&quot;))) head(d) # A tibble: 6 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;fct&gt; &lt;date&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 male 2009-05-11 CA 104 188 I 110 70 2 1-002 1970-02-27 male 2009-05-12 ADT 92 182 I NA NA 3 1-003 1964-02-21 female 2009-06-11 CA 95 177 I 125 70 4 1-004 1962-01-04 female 2009-07-20 CA 80 162 I 139 63 5 1-005 1974-06-02 male 2009-08-17 ADT 93 175 I 122 74 6 1-006 1964-06-22 male 2009-09-21 CA 115 185 I 120 80 # … with 1 more variable: hr &lt;dbl&gt; 4.5.2 Formulas Many derived variables are obtained using a computational formula involving arithmetic operations, mathematical functions, or both. For instance, the age of patients at randomization can be computed from the birth and randomization dates; the body mas index is computed from the body height and weight. These computations are easily implemented via mutate(), as in the following script, where we finish by selecting relevant variables to verify the result of the computations: d %&gt;% mutate(age = floor(as.numeric(rand_dt - birth_dt)/365.25), bmi = round(weight / (height/100)^2,1)) %&gt;% select(patient, rand_dt, birth_dt, age, weight, height, bmi) # A tibble: 152 × 7 patient rand_dt birth_dt age weight height bmi &lt;chr&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 2009-05-11 1974-11-15 34 104 188 29.4 2 1-002 2009-05-12 1970-02-27 39 92 182 27.8 3 1-003 2009-06-11 1964-02-21 45 95 177 30.3 4 1-004 2009-07-20 1962-01-04 47 80 162 30.5 5 1-005 2009-08-17 1974-06-02 35 93 175 30.4 6 1-006 2009-09-21 1964-06-22 45 115 185 33.6 7 1-007 2009-10-01 1950-06-30 59 89 177 28.4 8 1-008 2009-10-06 1955-03-29 54 77 178 24.3 9 1-009 2009-11-10 1941-07-25 68 62 162 23.6 10 1-010 2009-11-17 1955-04-22 54 86 168 30.5 # … with 142 more rows We computed the age of patients as the difference of the randomization and birth dates (which results in a number of days) divided by 365.25 to take into account leap years, and then used floor() to round down so as to get completed years. The body weigh (kg/m^2) is defined as weight in kilograms over the square of height in meters. Because height is expressed in centimeters, we divided by 100 to convert it to meters before squaring. 4.5.3 Conditional assignments Computational formulas are applied exactly the same way for all rows in a dataframe. However, in some instances we want to assign different values to a new variable depending on a condition. For instance, we may want to create an indicator of obesity, which is defined as a body mass index of 30 kg/m^2 or more. Thus, a new variable obesity should take the value “no” for patients having bmi &lt; 30, or “yes” otherwise. This is called a conditional assignment, and can be done with function iflese(), which takes three arguments specified in this order: a condition that can be evaluated as either TRUE or FALSE, the value we want to assign when the condition is TRUE, and the value to be assigned when the condition is FALSE. This is done in the following script after computing bmi: d %&gt;% mutate(bmi = round(weight / (height/100)^2,1), obesity = ifelse(bmi &lt; 30, &quot;no&quot;, &quot;yes&quot;)) %&gt;% select(patient, bmi, obesity) # A tibble: 152 × 3 patient bmi obesity &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1-001 29.4 no 2 1-002 27.8 no 3 1-003 30.3 yes 4 1-004 30.5 yes 5 1-005 30.4 yes 6 1-006 33.6 yes 7 1-007 28.4 no 8 1-008 24.3 no 9 1-009 23.6 no 10 1-010 30.5 yes # … with 142 more rows For more complex conditional assignments, several iflese()functions may be nested as in the following example: d %&gt;% mutate(bmi = round(weight / (height/100)^2,1), nutritional_status = ifelse(bmi &lt; 18.5, &quot;underweight&quot;, ifelse(bmi &lt; 25, &quot;normal weight&quot;, ifelse(bmi &lt;30, &quot;overweight&quot;, &quot;obesity&quot;)))) %&gt;% select(patient, bmi, nutritional_status) # A tibble: 152 × 3 patient bmi nutritional_status &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1-001 29.4 overweight 2 1-002 27.8 overweight 3 1-003 30.3 obesity 4 1-004 30.5 obesity 5 1-005 30.4 obesity 6 1-006 33.6 obesity 7 1-007 28.4 overweight 8 1-008 24.3 normal weight 9 1-009 23.6 normal weight 10 1-010 30.5 obesity # … with 142 more rows Nesting ifelse() functions is very flexible, and there is no limit to the number of nested levels, but it becomes difficult to follow if more than two or three nesting levels are necessary. If the condition depend on a single quantitative variable (as in this case, bmi) it is easier to use a different function allowing to define cutpoints, which is presented in the next section. 4.5.4 Categorization of quantitative variables A common type of derived variable is what results from the categorization of a numeric variable according to one or more cutpoints. For instance, suppose we want to create age groups by decades, i.e., group patients in bins defined by cutpoints 20, 30, 40, … and so on. This can be easily done using function cut(), that takes two arguments: the numeric variable we want to categorize, and the breaks or cutpoints we want to use, passed as a numeric vector such as c(20, 30, 40, ...). When the cutpoints are equidistant, it is practical to create this numeric vector with seq(), as done in the following script: d %&gt;% mutate(age = floor(as.numeric(rand_dt - birth_dt)/365.25), age_group = cut(age, breaks = seq(20, 70, 10))) %&gt;% select(patient, age, age_group) -&gt; foo foo # A tibble: 152 × 3 patient age age_group &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; 1 1-001 34 (30,40] 2 1-002 39 (30,40] 3 1-003 45 (40,50] 4 1-004 47 (40,50] 5 1-005 35 (30,40] 6 1-006 45 (40,50] 7 1-007 59 (50,60] 8 1-008 54 (50,60] 9 1-009 68 (60,70] 10 1-010 54 (50,60] # … with 142 more rows The result is a factor with the following levels: levels(foo$age_group) [1] &quot;(20,30]&quot; &quot;(30,40]&quot; &quot;(40,50]&quot; &quot;(50,60]&quot; &quot;(60,70]&quot; By default, intervals are defined from cutpoints as left-open and right-closed, using standard symbols ( and ] respectively. This means that the lower bound is not included in the interval, and the upper bound is included. For instance, a patient 40 years old is included in interval (30, 40] , and excluded from interval (40, 50]). Sometimes we may need left-closed and right-open intervals instead. For instance, the WHO nutritional status defines intervals in this way (note there is an error in the definition of the interval for Obesity class III, which is defined as Above 40; a patient with a BMI of exactly 40 does not fit in any of the classes! Therefore, the last class should be defined as 40 or more). We can produce this classification with cut(), using the argument right = FALSE, as done below. Note the use of count() to get the number of cases in each of the who_nsintervals. d %&gt;% mutate(bmi = round(weight / (height/100)^2,1), who_ns = cut(bmi, breaks = c(0, 18.5, 25, 30, 35, 40, Inf), right = FALSE)) %&gt;% count(who_ns) # A tibble: 6 × 2 who_ns n &lt;fct&gt; &lt;int&gt; 1 [18.5,25) 25 2 [25,30) 61 3 [30,35) 38 4 [35,40) 5 5 [40,Inf) 1 6 &lt;NA&gt; 22 We could also define custom labels for the resulting intervals as in the following: d %&gt;% mutate(bmi = round(weight / (height/100)^2,1), who_ns = cut(bmi, breaks = c(0, 18.5, 25, 30, 35, 40, Inf), labels = c(&quot;Underweight&quot;, &quot;Normal weight&quot;, &quot;Overweight&quot;, &quot;Obesity class I&quot;, &quot;Obesity class II&quot;, &quot;Obesity class III&quot;), right = FALSE)) %&gt;% count(who_ns) # A tibble: 6 × 2 who_ns n &lt;fct&gt; &lt;int&gt; 1 Normal weight 25 2 Overweight 61 3 Obesity class I 38 4 Obesity class II 5 5 Obesity class III 1 6 &lt;NA&gt; 22 4.5.5 Grouping factor levels Sometimes we want to re-classify observations by pooling some of the levels of a factor. For instance, suppose we want a simpler classification of the nutritional status with a single obesity class obtained by merging the three obesity classes I, II and III. This can be done in with function recode() from the dplyr package as shown below. Note that we only need to specify the levels to recode, plus .default = levels(who_ns) to keep all other levels unchanged. d %&gt;% mutate(bmi = round(weight / (height/100)^2,1), who_ns = cut(bmi, breaks = c(0, 18.5, 25, 30, 35, 40, Inf), labels = c(&quot;Underweight&quot;, &quot;Normal weight&quot;, &quot;Overweight&quot;, &quot;Obesity class I&quot;, &quot;Obesity class II&quot;, &quot;Obesity class III&quot;), right = FALSE), simpler_ns = recode(who_ns, &quot;Obesity class I&quot; = &quot;Obesity&quot;, &quot;Obesity class II&quot; = &quot;Obesity&quot;, &quot;Obesity class III&quot; = &quot;Obesity&quot;, .default = levels(who_ns))) %&gt;% count(simpler_ns) # A tibble: 4 × 2 simpler_ns n &lt;fct&gt; &lt;int&gt; 1 Normal weight 25 2 Overweight 61 3 Obesity 44 4 &lt;NA&gt; 22 4.5.6 Character strings We sometimes need to use strings stored in a character variable to derive a new variable. For instance, variable patient is a character vector containing a code for the study center (hospital), a hyphen, and a patient number within the center. Suppose we want to have a variable with the center code. This can be done with substr(). Similarly, we could extract the number of patient in each center. d %&gt;% mutate(site = substr(patient, 1, 1), site_patient = as.numeric(substr(patient, 3, 5))) %&gt;% select(patient, site, site_patient) # A tibble: 152 × 3 patient site site_patient &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1-001 1 1 2 1-002 1 2 3 1-003 1 3 4 1-004 1 4 5 1-005 1 5 6 1-006 1 6 7 1-007 1 7 8 1-008 1 8 9 1-009 1 9 10 1-010 1 10 # … with 142 more rows Function substr() always returns a character vector, as is the case of site above. However, we can easily get a numeric vector by wrapping substr() within as.numeric(), as we did for site_patient. Another common need is exactly the opposite of what we did: to combine two variables to form a new one. As an example, we recompose the patient variable from the two pieces obtained above, using paste(): d %&gt;% mutate(site = substr(patient, 1, 1), site_patient = substr(patient, 3, 5), recompose_patient = paste(site, site_patient, sep = &quot;-&quot;)) %&gt;% select(patient, site, site_patient, recompose_patient) # A tibble: 152 × 4 patient site site_patient recompose_patient &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1-001 1 001 1-001 2 1-002 1 002 1-002 3 1-003 1 003 1-003 4 1-004 1 004 1-004 5 1-005 1 005 1-005 6 1-006 1 006 1-006 7 1-007 1 007 1-007 8 1-008 1 008 1-008 9 1-009 1 009 1-009 10 1-010 1 010 1-010 # … with 142 more rows Working with strings is always difficult, and we often face problems we did not cover in the previous examples. However, you should be aware that there is much more power in R for working with strings, including detection and substitution of complex patterns, fuzzy string matching, and translations from, or to other languages. In the resources section we provide some links which are relevant for working with strings. 4.6 Selecting valid cases In virtually all real studies, some of the patients included in the study database are not valid for analysis. Common causes for this are inappropriate enrollment of patients that do not meet all predefined selection criteria, and lack of critical data. Then, we need to get rid of these patients, and keep only those who are valid. Validity criteria are always functions of the variables in our data, so that we should be able to write an expression to retain valid patients only. For instance suppose we want to declare invalid only those patients having a missing in hr. The result is a dataframe with 151 rows, after removing the only case with missing hr, as shown below: d %&gt;% filter(!is.na(hr)) %&gt;% nrow() [1] 151 Note the use of the not operator ! in front of is.na() in the filter() function. This operator reverses the meaning of what follows. Therefore, if is.na(hr) means hr is missing, !is.na(hr) means hr is NOT missing. Sometimes you will need to drop all cases having a missing somewhere, so as to keep only cases with complete data. This is easily achieved with function na.omit(). d %&gt;% na.omit() -&gt; complete nrow(complete) [1] 124 colSums(is.na(complete)) patient birth_dt sex rand_dt group weight height nyha 0 0 0 0 0 0 0 0 sbp dbp hr 0 0 0 In a more realistic case, you may need to investigate different aspects of the data, including compliance with all selection criteria (collected in several variables) and availability of important variables. This may take deriving a new variable that summarizes several validity criteria (such as valid: yes or no), and then use this variable for selection of valid patients. In the SARA study, only patients that did not comply with the study selection criteria were declared invalid and excluded form all analyses. These patients were: 1-013, 1-038, 1-054, 2-012, 5-002, and 6-021. In the following script we define a vector invalid, and then use the %in% operator to identify rows in d where patient is one of those in invalid: invalid &lt;- c(&quot;1-013&quot;, &quot;1-038&quot;, &quot;1-054&quot;, &quot;2-012&quot;, &quot;5-002&quot;, &quot;6-021&quot;) d %&gt;% filter(patient %in% invalid) # A tibble: 6 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;fct&gt; &lt;date&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-013 1955-01-01 male 2010-01-22 ADT 100 176 II 143 100 2 1-038 1953-05-03 male 2011-03-14 CA 106 180 II 142 107 3 1-054 1943-05-16 female 2011-11-07 CA 67 162 I 130 84 4 2-012 1951-12-05 male 2010-06-25 CA 110 180 I 120 70 5 5-002 1946-03-12 male 2009-09-16 CA 85 179 I 133 87 6 6-021 1969-12-22 male 2011-01-31 ADT NA NA I 140 90 # … with 1 more variable: hr &lt;dbl&gt; To subset dretaining all the remaining patients, we just use the not operator in front of the same expression to get just the opposite result, i.e., patients not in the invalid vector. d %&gt;% filter(!(patient %in% invalid)) # A tibble: 146 × 11 patient birth_dt sex rand_dt group weight height nyha sbp dbp &lt;chr&gt; &lt;date&gt; &lt;fct&gt; &lt;date&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1974-11-15 male 2009-05-11 CA 104 188 I 110 70 2 1-002 1970-02-27 male 2009-05-12 ADT 92 182 I NA NA 3 1-003 1964-02-21 female 2009-06-11 CA 95 177 I 125 70 4 1-004 1962-01-04 female 2009-07-20 CA 80 162 I 139 63 5 1-005 1974-06-02 male 2009-08-17 ADT 93 175 I 122 74 6 1-006 1964-06-22 male 2009-09-21 CA 115 185 I 120 80 7 1-007 1950-06-30 male 2009-10-01 CA 89 177 I 126 102 8 1-008 1955-03-29 male 2009-10-06 CA 77 178 I NA NA 9 1-009 1941-07-25 female 2009-11-10 ADT 62 162 I 140 85 10 1-010 1955-04-22 male 2009-11-17 ADT 86 168 I 122 83 # … with 136 more rows, and 1 more variable: hr &lt;dbl&gt; 4.7 Saving the R script Saving the R script that performs all the needed data preparation tasks is critical if we want this process to be reproducible. This is why we should always save the data preparation script, rather than its result, the tidy dataframe. There is no need to save the tidy data if we can reproduce it at no cost4, and as we will see, there is a very easy way to run the data preparation script once it has been saved to a file. The following script accumulates what we have done in previous sections, from the reading of raw data to the selection of valid cases: # reading raw data, cleaning and renaming vars d &lt;- read_excel(&quot;data/SARA_simplified.xlsx&quot;) %&gt;% clean_names() %&gt;% rename(patient = patient_no, birth_dt = birth_date_yyyymm_dd, rand_dt = date_of_randomization, group = random_allocation_of_treatment, weight = body_weight_at_baseline_kg, height = body_height_at_baseline_cm, nyha = nyha_classification_at_baseline, sbp = sbp_at_baseline_mm_hg, dbp = dbp_at_baseline_mm_hg, hr = heart_rate_at_baseline_bpm) %&gt;% # fixing problems in weight (commas instead of points!), and dates mutate(weight = as.numeric(sub(pattern = &quot;,&quot;, replacement = &quot;.&quot;, x = weight, fixed=TRUE)), birth_dt = as.Date(birth_dt), rand_dt = as.Date(rand_dt, &quot;%d/%m/%Y&quot;)) # data changes (after verification in hospital records) d$sex[d$patient == &quot;3-001&quot;] &lt;- 1 d$nyha[d$patient == &quot;1-005&quot;] &lt;- 1 d$nyha[d$patient == &quot;3-004&quot;] &lt;- 1 d$weight[d$patient == &quot;4-001&quot;] &lt;- 83 d$height[d$patient == &quot;4-001&quot;] &lt;- 167 d$hr[d$patient == &quot;6-030&quot;] &lt;- NA # invalid patients invalid &lt;- c(&quot;1-013&quot;, &quot;1-038&quot;, &quot;1-054&quot;, &quot;2-012&quot;, &quot;5-002&quot;, &quot;6-021&quot;) # derived vars sara &lt;- d %&gt;% # define factors mutate(sex = factor(sex, levels = 1:2, labels = c(&quot;male&quot;, &quot;female&quot;)), nyha = factor(nyha, levels = 1:3, labels = c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;)), group = factor(group, levels = c(&quot;Catheter ablation&quot;, &quot;Antiarrhythmic drug treatment&quot;), labels = c(&quot;CA&quot;, &quot;ADT&quot;))) %&gt;% # define new variables mutate(age = floor(as.numeric(rand_dt - birth_dt)/365.25), age_group = cut(age, breaks = seq(20, 70, 10)), bmi = round(weight / (height/100)^2,1), obesity = factor(ifelse(bmi &lt; 30, &quot;no&quot;, &quot;yes&quot;)), who_ns = cut(bmi, breaks = c(0, 18.5, 25, 30, 35, 40, Inf), right = FALSE), simpler_ns = recode(who_ns, &quot;Obesity class I&quot; = &quot;Obesity&quot;, &quot;Obesity class II&quot; = &quot;Obesity&quot;, &quot;Obesity class III&quot; = &quot;Obesity&quot;, .default = levels(who_ns)), site = substr(patient, 1, 1)) %&gt;% filter(!(patient %in% invalid)) %&gt;% # sort by patient arrange(patient) %&gt;% # define order of variables in dataframe select(patient, site, birth_dt:group, age, age_group, height, weight, bmi, simpler_ns, everything(), -who_ns) rm(invalid, d) After filtering valid cases, we used arrange() to ensure that rows will be sorted by patient, and select() to sort the variables as desired. Finally, we removed intermediate objects we no longer need with rm(). In the select() statement, note some useful possibilities: colons to indicate groups of adjacent variables (as in birth_dt:group). everything() to indicate all the remaining, non-mentioned variables. negative sign preceding a variable to drop it (-who_ns). We now print the first rows of sara to verify the result: sara # A tibble: 146 × 17 patient site birth_dt sex rand_dt group age age_group height weight &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;fct&gt; &lt;date&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1-001 1 1974-11-15 male 2009-05-11 CA 34 (30,40] 188 104 2 1-002 1 1970-02-27 male 2009-05-12 ADT 39 (30,40] 182 92 3 1-003 1 1964-02-21 fema… 2009-06-11 CA 45 (40,50] 177 95 4 1-004 1 1962-01-04 fema… 2009-07-20 CA 47 (40,50] 162 80 5 1-005 1 1974-06-02 male 2009-08-17 ADT 35 (30,40] 175 93 6 1-006 1 1964-06-22 male 2009-09-21 CA 45 (40,50] 185 115 7 1-007 1 1950-06-30 male 2009-10-01 CA 59 (50,60] 177 89 8 1-008 1 1955-03-29 male 2009-10-06 CA 54 (50,60] 178 77 9 1-009 1 1941-07-25 fema… 2009-11-10 ADT 68 (60,70] 162 62 10 1-010 1 1955-04-22 male 2009-11-17 ADT 54 (50,60] 168 86 # … with 136 more rows, and 7 more variables: bmi &lt;dbl&gt;, simpler_ns &lt;fct&gt;, # nyha &lt;fct&gt;, sbp &lt;dbl&gt;, dbp &lt;dbl&gt;, hr &lt;dbl&gt;, obesity &lt;fct&gt; names(sara) # to see variable order [1] &quot;patient&quot; &quot;site&quot; &quot;birth_dt&quot; &quot;sex&quot; &quot;rand_dt&quot; [6] &quot;group&quot; &quot;age&quot; &quot;age_group&quot; &quot;height&quot; &quot;weight&quot; [11] &quot;bmi&quot; &quot;simpler_ns&quot; &quot;nyha&quot; &quot;sbp&quot; &quot;dbp&quot; [16] &quot;hr&quot; &quot;obesity&quot; The R script above should be saved to file, with an appropriate name such as “data_preparation.R”, in the same folder where the raw_data is located (e.g., the study folder). Once this is done, you can start a fresh new R session and run this script using function source() with the complete file name as argument (don’t forget the quotes and the .R file extension!). As long as your working directory is the study folder, this will run the script, and the sara dataframe will appear in your workspace (i.e., Environment pane): source(&quot;data_preparation.R&quot;) You are now ready for statistical analysis! Resources The forecats package provides useful functions to work with factors (and there is a Factors with forcats Cheat Sheet. The lubridate package provides useful functions to work with dates (and there is a Dates and Times Cheat Sheet. The stringr package provides useful functions to work with strings (and there is a Work with Strings Cheat Sheet. For exact matching of strings, see the help of the grep() function of base R. If you need fuzzy matching (approximate matching) of strings, see the help of the agrep() function of base for starters; if you want more, see packages fuzzywuzzyR; if you want to get mad, see package stringdist. The googleLanguageR package uses Google Cloud Translation API for language detection and translation of strings to a different language (e.g., Spanish to English). It comes at a price, though quite affordable if you don’t exceed a billion of characters per month (and free for the first 500000 characters). Exercises The following script generates a dataframe with a hospitalcode, and a within-hospital patient number. set.seed(1); d &lt;- data.frame(hospital = c(rep(1:5, each=20)), patient = rep(1:20, 5) - rpois(n = 100, 0.05), sex = sample(c(&quot;male&quot;, &quot;female&quot;), replace = TRUE, size = 100), age = floor(rnorm(100, mean=45, sd=15))) head(d) hospital patient sex age 1 1 1 female 35 2 1 2 female 45 3 1 3 female 31 4 1 4 male 47 5 1 5 male 35 6 1 6 male 71 Create a unique case identifier by combining hospital and patient into a single variable case_id. Is case_id really unique? If it’s not, investigate what is/are repeated case_id value(s), and what are their positions (rows) in the dataframe Fix the problem by appending “a” or “b” to the case_id, so that it is unique. Amend the following data errors: The true age of the 16 year old patient is 26. Patient 1 in hospital 1 is not a female, but a male. The top age for this study was 75 years, so that any patient older than 75 is invalid. Eliminate these patients from the tidy dataframe. How many patients are left? Read this data of an observational study on pre-hypertension (previous stage to arterial hypertension) conducted in relatives of patients with arterial hypertension (the units of quantitative variables are whole years for age, cm for height and abdominal circumference, kg for weight, and mmHg for SBP and DBP). Ensure that all variable have appropriate names in snake_case, and that the case identifier is unique. Investigate the number of missings in this dataset. What are the variables containing missings? How many missings are there in each variable? Look at the minimum and maximum values of numeric variables. Are there gross errors, like impossible or very suspicious values? Define factors for all categorical variables (see the coding here). Compute the following derived variables: age_group: defined as working age (15-64 years) or elderly (65 years or older). bmi: the body mass index (BMI) (kg/^2), rounded to 1 decimal. nutritional status: defined as underweight (BMI &lt; 18.5), normal weight (18.5–24.9), overweight (25.0–29.9), or obesity (30 or above). Keep only the patients having complete data (no missings). Reorder variables so that derived variables come immediately after those they depend on. Ensure rows are ordered by subject number. Save your R script to a file with the name “pre_hta_data_preparation.R”, start a fresh R session, and verify your working directory is the folder where you saved the file (check this by running getwd()). Without opening the script in the RStudio editor, run it with source(\"pre_hta_data_preparation.R\"). Answer these questions: How many patients are left? Are there any missings? What is the mean of bmi? How many patients are there in each nutritional status category? Overriding the dataframe you start with (d in this case) is not a good idea unless you have tested the code and are pretty sure it works well. For testing, use a different name, so that you do not loose d if something goes wrong.↩︎ In studies with lots of data and/or very complex data preparation, the R script may be very long and take too much time to be executed. In these cases, it is reasonable to save not only the data preparation script, but also the resulting tidy dataframe.↩︎ "],["exploratory-data-analysis.html", "5 Exploratory Data Analysis 5.1 Number and type of variables 5.2 Univariate description 5.3 Bivariate description 5.4 Adding infromation from additional variables 5.5 Graphic refinement Resources Exercises", " 5 Exploratory Data Analysis Data exploration and description are first steps in data analysis. Graphics are the best tool for exploring data, while summary statistics are best suited for description under space restrictions, as is the case of published papers. Of course, papers include graphics as well, but these are often restricted to the most relevant results. Exploratory graphics differ from publication ready graphics. Graphics for publication must have a title, appropriate labels in axes and legends, and must be stored in a high quality image, all of which takes time, and it is usually done only for a selection of a few graphics. Conversely, data exploration requires lots of graphics, and we do not care about nice labeling or super-high image quality. So, exploratory graphics are fast and dirty. There are several graphics functions in base R., such as plot(), hist(), boxplot(), and barplot(). There are also several packages specialized in graphics, such as ggplot2, plotly, lattice and ggformula. With few exceptions, in this chapter we use the latter because of its simple, consistent, user-friendly syntax, that makes it ideal for fast (though not so dirty!) graphics. Similarly, there are several functions in base R to compute summary statistics, but we will use functions from the mosaic package. You should install the ggformula and mosaic packages now (using the Install button in the Packages pane of RStudio), and then load them with library(). library(ggformula) library(mosaic) All the graphic functions in the ggformula package start with gf_ (e.g., gf_histogram()), so you will recognize them very easily. To illustrate the exploratory methods in this chapter, we use data from the Predimed trial that compared two Mediterranean Diets (MD) to a Control diet. The compareGroups package includes dataframe predimed containing a simplified version of the trial data. The following script loads this package, uses function data(predimed) to make this dataset available, redefines the levels of the group variable (to have shorter labels for graphics), and shows the first 6 lines of (a subset of columns of) the predimed dataset. For a brief description of this dataset run ?predimed. # Remember to install the compareGroups package first, # using the `Install` button in the Packages pane of RStudio library(compareGroups) data(predimed) d &lt;- predimed levels(d$group) &lt;- c(&quot;Control&quot;, &quot;MD+nuts&quot;, &quot;MD+VOO&quot;) head(d) group sex age smoke bmi waist wth htn diab hyperchol famhist 1 Control Male 58 Former 33.53 122 0.7530864 No No Yes No 2 Control Male 77 Current 31.05 119 0.7300614 Yes Yes No No 4 MD+VOO Female 72 Former 30.86 106 0.6543210 No Yes No Yes 5 MD+nuts Male 71 Former 27.68 118 0.6941177 Yes No Yes No 6 MD+VOO Female 79 Never 35.94 129 0.8062500 Yes No Yes No 8 Control Male 63 Former 41.66 143 0.8033708 Yes Yes Yes No hormo p14 toevent event 1 No 10 5.374401 Yes 2 No 10 6.097194 No 4 No 8 5.946612 No 5 No 8 2.907598 Yes 6 No 9 4.761123 No 8 &lt;NA&gt; 9 3.148528 Yes 5.1 Number and type of variables The descriptive methods presented in this chapter are classified according to the number and type of variables they deal with. Concerning the number of variables, statistical methods are usually characterized as univariate, bivariate, or multivariate, when they deal with one, two, or more than two variables, respectively. While univariate methods are used to summarize or display the distribution5 of a single variable, bivariate and multivariate methods deal with the relationship between variables. There are two main types of variables: Categorical variables: nominal, or ordinal Quantitative variables: continuous, or discrete Categorical variables are classifications, like gender (male or female) or the New York Heart Association (NYHA) Classification of heart failure (I: No symptoms, II: Mild symptoms, III: Marked limitation in activity due to symptoms, or IV: Severe limitations). When the classes or categories of a categorical variable have a natural ordering, as is the case of the NYHA classification, the variable is said to be ordinal. Otherwise, it is said to be nominal, and this is the case of gender or ethnicity. Categorical variables with only two categories (like sex) are sometimes called dichotomous, binary or indicator variables, while those with more than two categories are called polytomous variables. Quantitative variables express a quantity and are the result of a counting or measurement procedure. Examples of quantitative variables are the number of seizures experienced by patient in a period of time, or his/her body weight. If the variable can only take a finite number of values in a given interval, it is said to be discrete. This is the case of the number of seizures: given any interval of values (like 2 and 8), there is only a finite number of possible values between them (3, 4, 5, 6, or 7). Otherwise, the variable is said to be continuous (there are infinite possible values of weight between 60 and 61 Kg: e.g., 60.5, 60.05, etc.). 5.2 Univariate description Table 5.1 shows the most common summary statistics and graphics used for univariate description. All methods listed in this table are presented in this section. Table 5.1: Univariate description Variable Statistics Graphics Categorical frequencies (counts, proportions, percentages) barchart Quantitative mean, median, standard deviation, variance, interquartile range, extremes, quantiles histogram, density, boxplot 5.2.1 Categorical variable The tools for the univariate description of a categorical variable are frequency tables and barcharts. They both inform on the frequency distribution of the variable, that is, how frequent are the different values the variable can take. We can compute either absolute frequencies (counts), or relative frequencies (proportions or percentages). 5.2.1.1 Summary statistics The tally() function from the mosaic package can be used to produce frequency tables. Below we have used this function to get counts and percentages for the event variable of the d dataframe. tally( ~ event, data = d) # counts event No Yes 6072 252 tally( ~ event, data = d, format = &quot;percent&quot;) # percentages event No Yes 96.01518 3.98482 From these results we see that 252 subjects experienced a cardiovascular event during the study, which is roughly a 4 % of the total number of subjects. Note the tilde (~) preceding the variable name in the tally() function calls. This symbol is needed, and omitting it will produce an error. 5.2.1.2 Graphics A barchart is nothing but the graphical expression of a frequency table: for each value of the variable, we draw a bar extending from zero to the value indicated in a frequency axis. Bar charts are easily produced with functions gf_bar() and gf_percents() from the ggformula package. In the following script we save the plots produced by these functions as plot1 and plot2, and then use package patchwork to print them at once, side by side. library(ggformula) plot1 &lt;- gf_bar( ~ event, data = d) # counts plot2 &lt;- gf_percents(~ event, data = d) # percentages library(patchwork) plot1 + plot2 # print both in a row Figure 5.1: Barcharts As you can see in figure 5.1, barcharts of absolute or relative frequencies display exactly the same image, but for the scale shown in the vertical axis (counts or percentages). This implies we can use either to get an idea of how common are the different values of the variable. 5.2.2 Quantitative variable 5.2.2.1 Sumary statistics The summary statistics used to describe the distribution of a quantitative variable can be classified in three types: centrality measures, spread or variability measures, and position measures: Centrality measures provide a typical or “central” value, informing on where the distribution is centered; the most common ones are the mean and the median (see table 5.2). Spread measures inform on the variability of values, that is, how scattered the values are around the center of the distribution; the most common ones are the variance, the standard deviation (SD), and the interquartile range (IQR)(see table 5.3). Position measures inform on the relative position of specific values in the distribution. There are two types of position measures: extremes (minimum, maximum) and quantiles(quartiles, quintiles, deciles, percentiles)(see table 5.4). In tables 5.2 to 5.4 we provide the definitions of the statistics listed above. In these tables we denote \\(x_i\\) the \\(i-th\\) value of variable X in a collection of \\(n\\) observations (so that \\(i = 1, 2, ... n\\)), and \\(\\sum\\) denotes summation over \\(i\\): Table 5.2: Centrality measures Name Definition: symbol = formula R function mean Sum of all values divided by the number of values: \\(\\bar{x} = \\frac{\\sum x_i}{n}\\) mean() median Value that occupies the central place in the ordered collection of the \\(n\\) values, thus dividing it in two parts having the same number of values. If \\(n\\) is even, there is no single central place, but two; in this case, these two central values are averaged. median() Table 5.3: Variability measures Name Definition: symbol = formula R function Variance Sum of the squared deviations from the mean, divided the number of values: \\(\\sigma^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n}\\). For estimation purposes (i.e., when the collection of values is a sample rather than a population) we use \\(s^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n-1}\\) var(x) Standard deviation (SD) Square root of the variance: \\(\\sigma = \\sqrt{\\sigma^2}\\), or \\(s = \\sqrt{s^2}\\) sd(x) Interquartile range (IQR) Difference between the third and the first quartiles (see position measures): \\(IQR = Q_3 -Q_1\\). IQR(x) Table 5.4: Position measures Name Definition: symbol = formula R function Extremes: minimum and maximum Lowest and highest value in the collection, respectively min(x) max(x) Quantiles Values that occupy particular positions in the ordered collection of values, dividing the collection in a number of parts having the same number of values. They are named according to the number of parts they generate: Quartiles Divide the distribution in four parts: \\(Q_1, Q_2, Q_3\\) quantile(x) Quintiles Divide the distribution in five parts: \\(Q_1, Q_2, Q_3\\) quantile(x, probs = c(.2, .4, .6, .8)) Deciles Divide the distribution in ten parts: \\(D_1, D_2,…, D_9\\) quantile(x, probs = seq(.1, .9, .1)) Percentiles Divide the distribution in 100 parts: \\(P_1, P_2,…, P_{99}\\) quantile(x, probs = seq(.1, .99, .1)) A couple of comments on quantiles: Note that we need K-1 cuts to divide the distribution in K parts; therefore, there are three quartiles, four quintiles, nine deciles, and ninety-nine percentiles. It should be clear that: the median is the same as the second quartile and the percentile 50 (median = \\(Q_2 = P_{50}\\)) the first quartile is the same as percentile 25 (\\(Q_1 = P_{25}\\)), and the third quartile is the same as percentile 75 \\(Q_3 = P_{75}\\). The R functions listed in tables 5.2 to 5.4 are useful when we want to compute just one of these statistics. However, there are other functions providing several statistics at once. This is the case of function favstats() from package mosaic, which is used in the script below to get summary statistics for the age variable (as you see, we use the pipe to round() to the first decimal the results obtained with favstats()). library(mosaic) favstats( ~ age, data=d) %&gt;% round(1) min Q1 median Q3 max mean sd n missing 49 62 67 72 87 67 6.2 6324 0 In the resuls above we see that the age of subjects ranged from 49 to 87 years, with mean and median values of 67 years. The fact that they match is an indication that the distribution is pretty symmetric (in asymmetric distributions the median and the mean are different, the extent of the difference depending on the degree of asymmetry). The standard deviation of 6.2 can be loosely interpreted as the average deviation from the mean of age values. Quantiles can be obtained with the quantile() function. By default, the quartiles (and extremes) are provided, but other quantiles can be obtained by providing appropriate probabilities, as a numeric vector, in argument probs. In some of the example below we use seq() to generate this vector of probabilities. quantile( ~ age, data=d) # quartiles (default) 0% 25% 50% 75% 100% 49 62 67 72 87 quantile( ~ age, data=d, probs = seq(.2, .8, .2)) # quintiles 20% 40% 60% 80% 61 65 69 73 quantile( ~ age, data=d, probs = seq(.1, .9, .1)) # deciles 10% 20% 30% 40% 50% 60% 70% 80% 90% 59 61 63 65 67 69 71 73 76 quantile( ~ age, data=d, probs = c(.05, .95)) # percentiles 5 and 95 5% 95% 58 77 From the last result above, we see that only 5% of the cases are 58 years or younger, and an additional 5% are 77 years or older. Therefore, 90% of the cases have an age between 58 and 77 years. 5.2.2.2 Graphics The most classical graphic to show the distribution of a quantitative variable is the histogram. Histograms are built by defining bins in a variable axis, and drawing a bar for each bin, the area of which is proportional to the frequency of cases in that bin. You can produce histograms with functions gf_dhistogram() or gf_histogram(): the former draws a density axis (which is the relative frequency divided by the bin width), and the later a count axis. They both produce the same skyline, as shown in figure 5.2. plot1 &lt;- gf_dhistogram( ~ age, data=d) plot2 &lt;- gf_histogram( ~ age, data=d) plot1 + plot2 Figure 5.2: Histograms A limitation of histograms is that the resulting shape depends on how the bins are defined, which is an arbitrary decision. In the two histograms of figure 5.3, we get quite different shapes for the age distribution by just changing the number of bins (and therefore the bin width), and the bin boundaries. Clearly, this is not desirable for a graphic aimed at characterizing the shape of a distribution. plot1 &lt;- gf_histogram( ~ age, data=d, bins = 10) plot2 &lt;- gf_histogram( ~ age, data=d, bins = 10, boundary = 50) plot1 + plot2 Figure 5.3: Histograms changing bin width and boundaries A better approach to see the shape of a distribution is the density plot, which is a smoothed version of a histogram. Figure5.4 shows the density plots produced by functions gf_dens() and gf_density(): the former shows the curve and the later fills the area under it. plot1 &lt;- gf_dens( ~ age, data=d, bins = 10, boundary = 50) plot2 &lt;- gf_density( ~ age, data=d, bins = 10, boundary = 50) plot1 + plot2 Figure 5.4: Density plots The density plots in figure 5.4 display a bell-shaped distribution, though not perfectly symmetric. In a density plot, the area below the curve for any given interval of values is proportional to the frequency of observations in it. Thus, age values in the 60-75 interval are far more common than below 60 or above 75, just because the corresponding areas under the curve are markedly different. A simple but very useful graphic is the box-and-whiskers plot or boxplot for short (see figure 5.5). In a boxplot, a box is drawn from the first to the third quartile, and is divided by a line at the median. In addition, two lines (the whiskers) are drawn from the box limits to either the extremes, if there are no outliers, or to the last non-outlier observation. Outliers are values lying farther away than 1.5 times the IQR from the box edges, and are depicted individually using dots, if any (there is none in figure 5.5, but there are several in the distribution of bmi shown in figure 5.6). Boxplots can be produced with function gf_boxplot() of the ggformula package. gf_boxplot( ~ age, data=d) Figure 5.5: Boxplot In figure 5.5 it’s easy to see that the age distribution is pretty symmetric about its median of 67 years (there is just a slight asymmetry identified by an upper whisker a bit lengthier than the lower one), and that no outliers are detected. Because there are no outliers, the whisker limits are the minimum and maximum values (49 and 87, respectively). Also, the box limits are the first and third quartiles (62 and 72, respectively), and thus the box contains the central half of the age values. Last, note that the width of the box does not convey any information and is irrelevant, as is the unlabeled vertical axis. Boxplots are very good at detecting asymmetry and outliers. For instance, in figure 5.6, a clear asymmetry is apparent in the toevent variable, outliers are seen in the upper tail of the distribution of the bmi, and both asymmetry and outliers are present in the case of p14. plot1 &lt;- gf_boxplot( ~ toevent, data=d) plot2 &lt;- gf_boxplot( ~ bmi, data=d) plot3 &lt;- gf_boxplot( ~ p14, data=d) plot1 / plot2 / plot3 # print them stacked Figure 5.6: Boxplots showing assymetry, outliers, or both 5.2.3 Important remarks When summarizing the distribution of a variable using only summary statistics (as is the case in published papers), it is extremely important to select the appropriate ones. This is not a problem for categorical variables, because a frequency table summarizes the distribution with no information loss. Consider for instance this frequency table: sex n Percentage female 4 40 male 6 60 The information in the frequency table above is enough to reproduce the whole distribution of sex values: female, female, female, female, male, male, male, male, male, male This means that, when we summarize the distribution of a categorical variable by providing a frequency table, no information is lost. However, when we summarize the distribution of a quantitative variable by its mean and SD, we loose a lot of information, and as a result we are unable to reproduce the collection of raw values. For this reason, it is especially important to choose the summary statistics wisely. To summarize the distribution of a quantitative variable, we need to provide at least a central tendency measure and a variability measure. The mean and SD are good choices provided the distribution has no marked asymmetry, and no influential outliers. Otherwise, the median and IQR (or the three quartiles) are much better choices. The reason for this is that the mean and SD are affected by asymmetry or influential outliers. Both asymmetry and outliers are easily detected in a boxplot, making it ideal to decide which summary statistics to use. Outliers pull the mean and inflate the SD, but these effects depend on how far they are, how many they are, and how large is the overall number of observations. For instance, it is unlikely that a single outlier in 200 observations is influential, because its effect will be diluted. Conversely, it might have a dramatic effect in a collection of 20 observations. All in all, the main messages for the description of a quantitative variable are: Start always by looking at a graphic, ideally a boxplot Based on what you see in the boxplot, decide how to summarize the distribution: Use mean and SD if neither marked asymmetry nor influential outliers are seen Use the median and IQR otherwise 5.3 Bivariate description Bivariate descriptions involve two variables. The following table 5.5 shows the most common statistics and graphics used in bivariate descriptions according to the type of the two variables. Table 5.5: Bivariate description Variables Statistics Graphics Both categorical frequencies in contingency tables, Pearson’s chi-square, risk difference, relative risk, odds ratio, number needed to treat stacked barchart, mosaic plot Both quantitative covariance, correlation coefficients scatterplot, contour plot One of each type mean, median, standard deviation, variance, interquartile range, extremes, quantiles, mean difference, effect size histogram, density, jitter, violin plot, boxplot In this section, all the graphics and most of the statistics mentioned in table 5.5 will be illustrated, but we will defer the discussion of some statistics - those in italics in the table above - until the chapters devoted to statistical inference. 5.3.1 Two categorical variables The most basic methods to describe the join distribution of two categorical variables are contingency tables and stacked barcharts. A contingency table is a table showing all possible combinations of values of the two variables, and the frequencies of these combinations. In the script below, the tally() function of the mosaic package is used to build contingency tables for the event and group variables. By default, counts are computed. If the format = \"percent\"argument is used, percentages are computed. Note that, in this last case, column percentages are computed, and columns correspond to the variable after the ~). The optional margins = TRUE argument is used to print the column totals to make it clear that the percentages are column percentages, since they add up to 100% in each column. tally(event ~ group, data = d) # counts group event Control MD+nuts MD+VOO No 1945 2030 2097 Yes 97 70 85 tally(event ~ group, data = d, format = &quot;percent&quot;, # percentages margins = TRUE) # print col totals group event Control MD+nuts MD+VOO No 95.249755 96.666667 96.104491 Yes 4.750245 3.333333 3.895509 Total 100.000000 100.000000 100.000000 The results above show that the frequency of events is very low in all three groups, but the percentage is slightly higher in the control group than in any MD group. A stacked barchart is a barchart showing a bar for each category of one of the variables, with each bar split (using different colors) according to the distribution of the other variable. Stacked barcharts can be produced with functions gf_bar() and gf_props(), that show an axis of absolute or relative frequencies, respectively. In the first barchart of figure 5.7 (left), the length of each bar is determined by the absolute frequency (count) of the corresponding diet group. This is not very useful to compare the splits of different bars. This is not very problematic in this case, because the number of patients randomized to each diet group is similar. Even so, it is better to display a vertical axis of proportions, so that all bars have the same length and their splits can be compared fairly. This is the case of the second barchart in figure 5.7 (right), where the higher proportion of events in the control group is apparent. plot1 &lt;- gf_bar( ~ group , fill = ~ event, data = d) # counts plot2 &lt;- gf_props(~ group , fill = ~ event, data = d, position = &quot;fill&quot;) # percentages plot1 + plot2 Figure 5.7: Staked barcharts An interesting alternative to stacked barcharts is the mosaic plot. Such a plot is composed of tiles corresponding to the cells of a contingency table, created by recursive vertical and horizontal splits of a square. The area of each tile is proportional to the corresponding cell frequency, given the dimensions of previous splits. A mosaic plot can be produced with the mosaic() function of package vcd. library(vcd) mosaic(event ~ group, data=d) Figure 5.8: Mosaic plot Mosaic plots are particularly useful when we want to represent higher dimensional tables involving more than two variables. For instance, because diabetes is a known risk factor of cardiovascular events, we may want to take it into account when comparing the frequency of events among the tree treatment groups. Here is a mosaic plot on the three-dimensional contingency table generated by the event, group and diab variables: library(vcd) mosaic(event ~ group + diab, data=d) Figure 5.9: Mosaic plot with three variables Several things are immediately apparent in this plot. First, the number of diabetics is higher in the MD+VOO group, followed by the control group and by the MD+nuts group. Second, for both diabetic and non-diabetic subjects, the frequency of events is higher in the control group than in any of the MD groups. 5.3.2 Quantitative and categorical variable When one of the variables is quantitative and the other is categorical, we can apply the methods for the description of a quantitative variable for each level of the categorical variable. For instance, suppose we want to compare the body mass index (BMI) in the three treatment groups of the predimed trial. We can compute summary statistics for each group, using the favstats() function, indicating the quantitative variable before the tilde, and the categorical variable after it as shown below: favstats(bmi ~ group, data = d) group min Q1 median Q3 max mean sd n missing 1 Control 19.71 27.53 30.000 32.7675 51.94 30.28044 3.963947 2042 0 2 MD+nuts 19.64 26.95 29.455 32.1800 51.83 29.68725 3.766372 2100 0 3 MD+VOO 20.04 27.22 29.735 32.4200 49.12 29.94050 3.706966 2182 0 In terms of graphics, we can plot the density of the BMI for each group, using colors to differentiate groups. This can be done with function gf_density() by assigning different fill colors depending on the group: note the use of the fill argument, and the ~ preceding variable group. In this case, the distribution of the BMI is very similar in all three groups, since the densities are almost perfectly overlapped. gf_density( ~ bmi, data = d, fill = ~ group) Figure 5.10: Density by group We can also use boxplots, violin plots, or jitter plots. These are illustrated in in figure 5.11. A violin plot (figure 5.11, center) is just a mirrored density for each diet group. A jitter plot (figure 5.11, right) displays all individual values as points, randomly scattered in the direction perpendicular to the variable axis to prevent excessive overlapping of points. In the jitter plot below we have used the argument alpha to decrease the opacity of the points, making them transparent. Transparency is important when plotting very many observations, so that regions with higher density of cases are distinguished (if a solid color was used, the result would be too pasted to appreciate density variations). In all three plots, we have used different colors for fancy: they do not convey any information, since groups are already distinguished by the horizontal axis. Later in this chapter we will show a wiser way to use colors. p1 &lt;- gf_boxplot(bmi ~ group, data = d, color = ~ group) p2 &lt;- gf_violin(bmi ~ group, data = d, color = ~ group) p3 &lt;- gf_jitter(bmi ~ group, data = d, color = ~ group, alpha=0.2) p1 + p2 + p3 Figure 5.11: Boxplot, violin and jitter plots, by group All three plots above allow to compare the distribution of the BMI among treatment groups, which are very similar as expected by the random allocation of the three diets. 5.3.3 Two quantitative variables When we want to look at the bivariate distribution of two quantitative variables, the scatterplot is the graphic of choice. In a scatterplot, individual observations are depicted as points positioned according to the value of each variable. The points collectively form a cloud of points whose shape characterizes the relationship between the two variables. Figure 5.12 shows the scatterplot of waist and bmi obtained with function gf_poin(). gf_point(waist ~ bmi, data=d, alpha=0.1) %&gt;% # scatterplot gf_lims(x = c(20, 55), y = c(40, 180)) # sets axis limits Figure 5.12: Scatterplot Despite using a low value in the alpha argument, the cloud of points in figure 5.12 is quite pasted due to the high number of observations (n = 6324), making it difficult to appreciate density variations in the inner part of the cloud. An alternative to the scatterplot when the number of observations is very high is the contour plot, which is a sliced two-dimensional density: points of the plane having the same density are connected with lines. Figure 5.13 shows the contour plot for the bmi and waist variables, obtained with function gf_density2d(). It is easy to see that the density increases progressively towards the center of the cloud from any outside point. gf_density2d(waist ~ bmi, data=d) %&gt;% # contour plot gf_lims(x = c(20, 55), y = c(40, 180)) # set axis limits Figure 5.13: Contour plot The scatterplot and the contour plot above were built using the same scale for easy comparison, setting the axis limits with gf_lims() after a pipe (%&gt;%). In both graphics, bmiand waistappear positively related, since low (or high) values of one variable tend to be associated with low (or high) values in the other. 5.4 Adding infromation from additional variables The graphic functions in the ggformula package have several arguments we can use to create more informative graphics, by adding information on additional variables. In fact, we have already used one of these arguments, the fill argument, in figure 5.7 to split the bars according to the event variable, or in figure 5.10 to distinguish the densities in each treatment group. The fill argument is useful to color areas. Similarly, there is a color argument to color lines and symbols. This was used in figure 5.14 to map the color of points to variable sex, making it clear that, for any given value of waist, the waist to height ratio (wth) tends to be higher in females than in males. Note that this plot was built from a random sample of 300 subjects of the d dataset, to improve the visibility of individual points. set.seed(1) # for reproducibility rand_sample &lt;- sample(d,150) # random sample of 150 cases gf_point(wth ~ waist, col= ~ sex, data=rand_sample, alpha=0.5) Figure 5.14: Scatterplot with color maped to sex Also, the size argument can be used to map the size of symbols to a variable, as done for bmi in the bubble plot of figure 5.15. In the resulting plot, the size of the bubbles tends to increase as the waist and wth increase. gf_point(wth ~ waist, size= ~ bmi, data=rand_sample, alpha=0.1) Figure 5.15: Bubble plot A very useful way to add information on a categorical variable, that can be used with any type of graphic, is to use facets. These are obtained by adding the variable to the formula after a bar symbol (|), as done in the script below: the resulting expression can be read BMI as a function of group, given sex. As you see in figure 5.16, this results in a boxplot of bmi by group, but the two levels of sex are shown in two panels or facets that share the same scale for bmi to facilitate the comparison. It is quite clear that females have slightly higher BMI values than males, but no clear differences are see between treatment groups in either males or females. gf_boxplot(bmi ~ group | sex, data=d, alpha=0.5) Figure 5.16: Facets Facets can be defined according to two variables as well, as shown in the following script (note the use of the + to add smoke after sex). In the resulting figure 5.17, facets are arranged in a matrix, the rows of which define the sex, and the columns the smoking status. gf_boxplot(bmi ~ group | sex + smoke, data=d, alpha=0.5) Figure 5.17: Facets for sex and smoking habit Last, we can combine some of the previous options as in the following boxplots, where we can inspect the distribution of bmi according to four variables (group, sex, smoke and diab). gf_boxplot(bmi ~ group | sex + smoke, color= ~diab, data=d, alpha=0.5) Figure 5.18: Facets for sex and smoking habit, and color for diabetes There are still other arguments that can be used in some functions of the gg_formula package we did not cover here. You should be aware that not all arguments can be used in all functions (for instance, gf_contour() has no size argument). To know which arguments are available in a particular function of gg_formula, look at its fast help , by calling the function with no arguments, as shown below for gf_point(): gf_point() gf_point() uses * a formula with shape y ~ x. * geom: point * key attributes: alpha, color, size, shape, fill, group, stroke For more information, try ?gf_point 5.5 Graphic refinement We started this chapter saying that EDA graphics are fast and dirty. This does not mean you cannot produce “proper” graphics with ggformula functions. For instance, all functions in this package have optional arguments like title, subtitle, xlab, ylab allowing to define titles and axis labels within the function call. In some cases, overlaying two graphics produces a more informative result. You can overlay graphics using the %&gt;%, as in the following examples. In figure 5.19 contours are overlaid on the scatterplot of more than six-thousand points, to make visible the density gradient. When calling two functions chained by %&gt;%, there is no need to repeat identical arguments in the second function. gf_point(waist ~ bmi, data=rand_sample, alpha=0.1) %&gt;% gf_density2d() # no need to repeat formula and data Figure 5.19: Countour overlaid on a scatterplot As a second example, in figure 5.20 jitter points are overlaid on a boxplot. The with argument in the gf_jitter() function controls the amount of horizontal jitter. Note the outlier.alpha = 0 to avoid double plotting of outliers. gf_boxplot( bmi ~ group | sex, data=rand_sample, fill = NULL, color = ~ group, outlier.alpha = 0) %&gt;% gf_jitter(width = 0.1, alpha = 0.5) Figure 5.20: Jittered points overlaid on a boxplot Overlaying can be used to add information on summary statistics computed by function df_stats, which is very similar to the favstats() function presented in section 5.2.2.1. In the following script, we compute the summary statistics for bmi, save them in dataframe bmi_stats, and use this dataframe to overlay mean values to a boxplot. This is very useful to assess the influence of outliers, by comparing the mean and the median. bmi_stats &lt;- df_stats(bmi ~ group, data = d) # compute stats gf_boxplot(bmi ~ group, data = d) %&gt;% gf_point(mean ~ group, color = &quot;red&quot;, data = bmi_stats) %&gt;% # use them here gf_refine(coord_flip()) # flips axes (to get horizontal boxplots) Figure 5.21: Boxplot with means overlaid (red dot) In figure 5.21 the means and the medians of each group are practically identical, reflecting no or little influence of outliers on the mean, despite being quite far away (some values above 50!). Last, functions gf_text() and gf_label() are very handy to overlay annotations on a graphic. For instance, the lack of influence of outliers on the mean of bmi commented above is likely due to dilution because the number of observations, but this number is not reflected in the boxplot. Thus, it is a good idea to print this number for each group. This has been done in figure 5.22, using variable n from bmi_stats in function gf_label(). Note the use of paste() to prefix n with the text N =. gf_boxplot(bmi ~ group, data = d) %&gt;% gf_point(mean ~ group, color = &quot;red&quot;, data = bmi_stats) %&gt;% gf_label(15 ~ group, label = ~ paste(&quot;N =&quot;, n), data = bmi_stats) %&gt;% gf_lims(y = c(11, 54)) %&gt;% # to get space for labels gf_refine(coord_flip()) Figure 5.22: Boxplot with means and annotation of N Resources Plotting with formulas is a tutorial on the ggformula package you will find it in the Tutorial pane of RStudio (once you have installed this package). We strongly advise you follow this tutorial. If you enjoy it, you may want to learn how to refine your plots by following the second tutorial Refining ggformula plots. DataExplorer is a package for EDA you might want to know about. The summarytools package provides a few functions to neatly summarize your data. Venn diagrams and upset plots are useful when we want to visualize the combination for multiple dichotomous variables (like combinations of COVID19 symptoms). The ggplotgui package is graphical user interface (GUI) to build ggplot2 and Plotly graphics. No need to write R code, but you can save the code for reproducibility. The esquisse package is another GUI for ggplot2 graphics. Exercises Look at the help of the gf_bar() function and experiment with the arguments to describe the smoke variable of the d dataframe used in this chapter. We suggest the following: color = \"darkblue\" fill = \"darkblue\" xlab = \"Treatment group\" title = \"Smoking habits in the Predimed trial In the previous exercise, the ordering of the smoking categories in the plots (Never, Current, Former) is not very nice. Reproduce some of these plots by redefining the levels of smoke so that the ordering is Never, Former, Current. This text data file contains data on people involved in a well known historical event. Read the data and answer the following questions: How any people were involved in this event? Explore the univariate distribution of the four variables with bar plots. Did most of these people survive? Was Sex evenly distributed? What about Age, and what about Class? What are the levels of the Class variable? Produce stacked barcharts to explore Survival by Class, by Age, and by Sex. Did children survive more than adults? Did females survive more than males? What were the Classes surviving the most and the least? Produce contingency tables to explore Age by Class and Sex by Class. Are Age and Sex evenly distributed among Classes? Do you guess what is this historical event? Install package gapminder. Then load it with library(gapminder) and look at the help of the gapminder dataset included in this package. Subset the gapminder dataset for year 2007 and answer the following questions: How many countries there are in each continent? What is the range of life expectancy (LE) values overall? and by continent? Look at the distribution of life expectancy in all countries by producing a density plot. Does it look symmetric? Inspection an appropriate graphic to investigate if life expectancy is similar in all continents. What was the life expectancy in Spain? Is there a relationship between life expectancy and GDP per capita? If yes, is it linear? Is it similar in all continents? Now subset the gapminder dataset for Spain, France, Portugal, and Italy, keeping all years (not only 2007!), and use function gf_lines() to plot the evolution of life expectancy in these countries (you will need to specify the group argument). How does Spain compare to France, Portugal, and Italy? Here, the term distribution refers to the collection of observed values of a variable in a set of individuals, which is sometimes called, more precisely, empirical distribution of that variable. In other contexts, notably in probability theory, it is used to refer to theoretical models characterizing the behavior of random variables, such as the normal distribution, the binomial distribution, etc.↩︎ "],["statistical-inference.html", "6 Statistical inference 6.1 Population and sample 6.2 Inference problems 6.3 Probability distributions 6.4 Random samples 6.5 Example data 6.6 Estimation 6.7 Sample size and CI’s 6.8 Significance tests 6.9 Sample size and p values 6.10 Types of tests Resources Exercises", " 6 Statistical inference Statistical inference refers to the attempt to learn something about a population through the analysis of a sample. Let’s start by clarifying the meaning of these two terms. 6.1 Population and sample In statistics, a sample is a finite number (n) of observations, no matter how large, and population refers to a much wider set of individuals from which the sample was drawn. Actually, in statistical theory populations are often assumed to be infinite. In practical applications however, they may be finite, but much, much larger than the sample. For instance, suppose we want to investigate how common is low weight at birth (less than 2.5 kg) among singleton births. To this end, the weight of newborns in the 189 singleton births attended in hospital H is recorded during a year. This is just a sample of newborns, and our interest is not restricted to these 100 newborns in particular. Rather, we would like to learn something about the population of all newborns, or at least all newborns in a geographical area. Figure 6.1 illustrates the conceptual framework for statistical inference, were a quantity of interest in the population -like the proportion of low weight newborns- is denoted by \\(\\pi\\). A sample of n = 100 observations is drawn from the population, and the sample proportion, denoted by p, is computed by dividing the number of low weight newborns over the sample size (p = 23/100 = 0.23). It is crucial to realize that the value of p will not necessarily be equal to that of \\(\\pi\\). Moreover, if we draw another sample, it is very likely that the value of p in this new sample is different from that in the first sample, due to sampling variation. The pattern of variation of a statistic (like p) across samples is called its sampling distribution, and plays a central role in inferential statistics. In some circumstances the sampling distribution of a statistic is well known, and this knowledge is used to devise methods allowing to solve common questions about population quantities (like \\(\\pi\\)). However, for this to be the case, samples need to be drawn at random. Later in this chapter we will explain what random samples are, and how to draw a random sample from a population when this is possible. Figure 6.1: Conceptual framework for statistical inference Population quantities such as \\(\\pi\\) are called population parameters, or simply parameters, while quantities computed from sample data, like p, are called statistics. When a statistic is used to estimate a parameter, it is said to be an estimator of this parameter. Because it is fundamental not to confuse parameters and estimators, we usually denote the former by Greek letters and the later by Latin letters. The following table shows the usual notation for some very common parameters and corresponding estimators: Parameters Estimators Mean \\(\\mu\\) \\(\\bar{x}\\) Standard deviation \\(\\sigma\\) \\(s\\) Variance \\(\\sigma^2\\) \\(s^2\\) Proportion \\(\\pi\\) \\(p\\) Correlation coefficient \\(\\rho\\) \\(r\\) … (greek letters) (latin letters) 6.2 Inference problems There are two main types of inference problems: estimation and significance testing. Estimation is just a nice name for guessing quantities or, more specifically, population parameters. For instance, in the example of the previous section we were interested in estimating \\(\\pi\\), the proportion of low weight newborns in the population. Other examples of research questions involving estimation could be: What is the mean weight of newborns? What is the median survival time of breast cancer patients? What is the length of the COVID-19 incubation period, on average? How long does the protection of a COVID-19 vaccine last, on average? Significance testing is a procedure to assess the plausibility of scientific hypotheses. A scientific hypothesis is formalized into a statistical hypothesis, which is a statement about one or more parameters6. For instance, according to the National Center for Health Statistics, 8.31% of newborns in the US are low birthweight. Thus, we could hypothesize that this is the value of \\(\\pi\\) in the geographic area of hospital H, that is \\(\\pi = 0.0831\\). In this case, the statistical hypothesis proposes a particular value for a parameter, but in other cases it will involve the comparison of two (or more) parameters; for example, we might be interested in testing whether the frequency of low birth weight is the same in smoking and non-smoking mothers. Other examples of questions that could be addressed by testing appropriate statistical hypotheses are: Is a new diagnostic test more specific than the test currently used ? Is a specific genotype associated with coronary heart disease ? Does vitamin D reduce the risk of COVID-19? In this and subsequent chapters we will cover estimation and significance testing methods, all of which are based on the assumption that we have a random sample of observations. 6.3 Probability distributions Probability distributions are mathemtical models useful to describe the behaviour of random variables (i.e., variables taking values depending on chance). In random samples, statistics computed from sample data are random variables. There are many different probability distributions, for both discrete and continuous variables. Some important probability distributions models for discrete variables are the Bernoulli, the binomial, and the Poisson distributions. Important probability distributions models for continuous variables are the normal, chi-squared, Student’s t, and F distributions. Despite probability distributions are at the heart of inferential methods, their study can be omitted in a purely instrumental approach to data analysis, such as that adopted in this book. However, we will present some basic ideas about the normal distribution for two reasons: first, because it is the most important distribution model for continuous variables; and second, because some of the inferential methods that will be introduced in later chapters will require to judge whether the normal model is adequate to represent empirical the empirical data. A couple of words will be said also on two additional distribution models, the Student’s t and the chi-squared distributions, because these are used in some methods of inferential analysis that will be introduced later in this book. 6.3.1 The normal distribution The normal or Gaussian distribution is a bell-shaped curve symmetric about the mean, such as those shown in figures 6.2 and 6.3. A normal distribution is defined by two parameters, the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)). The mean is where the distribution is centered, and the standard deviation determines its spread. Figure 6.2 displays three normal distributions with means \\(\\mu = 0\\) (black), \\(\\mu = 5\\) (red), and \\(\\mu = 20\\) (blue), all having the same standard deviation \\(\\sigma = 1\\). Conversely, figure 6.3 shows three other normal distributions, all having the same mean \\(\\mu = 10\\), but differing in spread (black: \\(\\sigma = 1\\); red: \\(\\sigma = 2\\); and blue: \\(\\sigma = 4\\)). The vertical scale of figures 6.2 and 6.3 shows probability densities, which have no simple interpretation. In particular, they cannot be interpreted as probabilities. Rather, probabilities are given by areas under the curve. Figure 6.2: Normal distributions differing in mean (0, 5, and 20), and standard deviation 1 Figure 6.3: Normal distributions with mean = 10, differing in standard deviation (1, 2 and 4) There is an infinite number of normal distributions that differ by the value of the parameters, which jointly determine a single normal distribution. Among all members of the normal distribution family, the standard normal distribution is of particular interest. This is a normal distribution with parameters \\(\\mu = 0\\) and \\(\\sigma = 1\\). Variables follwing a standard normal distribution are often denoted with letter \\(Z\\). In a standard normal distribution, the \\(Z\\) values -1.96 and +1.96 limit a central probability interval of 95% (leaving 2.5% probability regions below -1.96 and above +1.96). Figure 6.4: Standard normal distribution with central 95% probability interval Any normal distribution can be mapped to the standard normal distribution with a transformation called standardization. If a variable \\(X\\) follows a normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\), then the following transformation will map \\(X\\) values to values of a standard normal distribution, in such a way that the relative positions of values, and the probability intervals they may define, are preserved: \\[Z \\quad = \\quad \\frac{X - \\mu}{\\sigma}\\] For instance, suppose that a particular variable \\(X\\) follows a normal distribution with \\(\\mu = 100\\) and \\(\\sigma = 10\\), which is often written as \\(X \\sim N(100, 10)\\). Solving for \\(X\\) the stardardization formula above gives, \\[ X \\quad = \\quad \\mu + Z \\sigma\\] which in this case is \\[ X \\quad = \\quad 100 + 10 \\ Z\\] If we know that the standard normal values \\(Z_1 = -1.96\\) and \\(Z_2 =+1.96\\) limit a central interval of 95% probability, then the corresponding \\(X\\) values will also limit such an interval (see figure 6.5): \\(X_1 \\quad = \\qquad \\quad 100 + (-1.96) \\ 10 \\quad = \\quad 100 -19.6 \\quad = \\quad 80.4\\) \\(X_2 \\quad = \\qquad \\quad 100 + (+1.96) \\ 10 \\quad = \\quad 100 +19.6 \\quad = \\quad 119.6\\) Figure 6.5: N(100,10) distribution with central 95% probability interval 6.3.2 The chi-squared distribution The chi-squared distribution is a probability distribution model for continuous variables taking positive values only. This distribution depends on a single parameter called degrees of freedom, usually denoted by the greek letter \\(\\nu\\). Figure 6.6 displays several chi-squared distributions, with \\(\\nu = 1\\) (red), \\(\\nu = 3\\) (blue), and \\(\\nu = 5\\) (black) degrees of freedom. Figure 6.6: Chi-squared distributions with 1, 3, and 5 degrees of freedom Some statistics that are functions of table counts follow a chi-squared distribution, which provides the basis for some methods of inferential analysis we will introduce in chapter 7. 6.3.3 The Student’s t distribution The Student’s t distribution, or simply t dsitribution, is a probability distribution model for continuous variables, very similar in shape to the standard normal distribution. However, this distribution depends on one parameter only, called its degrees of freedom. Figure 6.7 displays a t distributions with 5 and 10 degrees of freedom (blue), and a standard normal distribution (black). Though very similar, the t distribution has slightly hevier tails than the standard normal. This difference decreases as the number of degrees of freedom increases, so that for 30 degrees of freedom and above, the t distribution is virtualy identical to the standard normal. Figure 6.7: t distribution with 10 degrees of freedom (blue), and standard normal distribution (black dashed line) Under certain circumstances, some statistics that are functions of sample means follow a t-distribution, which provides the basis for some methods of inferential analysis on means we will introduce in chapter 8. 6.4 Random samples Many different procedures can be used to get a sample of observations from a population. When a sampling procedure is such that the probability of selecting an individual is known for all population individuals, we say this is a random or probabilistic sampling procedure, and the resulting sample a random (or probabilistic) sample. In any other case, the sampling procedure, and the resulting sample, are non-random. Examples of non-random samples are: Patients with surname starting with letters A to L. Patients visited on Monday, Tuesday or Wednesday. Consecutive patients. There are several types of random sampling procedures. In simple random sampling, all population individuals have the same probability of being selected. In order to guarantee this, random number generating functions should be used, such as the sample() function in R. Suppose we want to select a simple random sample (SRS) of patients from the population of 90,000 patients visited in an emergency service during the last year. For simplicity, assume that these patients are uniquely identified by numbers 1 to 90,000. To get a SRS of say 10 patients, we can use the sample() function. The first argument to this function should be either a vector of unique identifiers of all population individuals, or the size of the population (in which case identifiers are assumed to be the natural numbers from 1 up to the population size); the second argument (size) should be the required sample size: sample(1:90000, 10) # vector of identifiers as 1st argument [1] 8465 6563 62126 20205 62480 87817 76380 68581 55968 8615 sample(90000, size = 10) # population size as 1st argument [1] 41381 45196 9346 13824 8696 61798 61140 39557 7131 61256 The results of the two functions calls above are different, and if you run them twice you will get different results once again, so that the random sampling result is not reproducible. If we want our random selection to be reproducible (as we should), then we need to set a seed for the random number generator with set.seed(), before calling sample(): set.seed(123) # setting the seed to 123 (arbitrary number) sample(90000, 10) [1] 51663 57870 2986 29925 68293 62555 45404 65161 46435 9642 Now, the re-execution of this code (both lines!) will produce the same results. See? set.seed(123) # second execution sample(90000, 10) [1] 51663 57870 2986 29925 68293 62555 45404 65161 46435 9642 It should be noted that to get a SRS we do need the list of individuals in the population (a population census) and their unique identifiers (like 1, 2, …90000); otherwise, it is just impossible to get a SRS. There are other, more complex random sampling procedures, like stratified sampling, cluster sampling, and multi-stage sampling. These are frequently used in community health surveys. However, we will not discuss them because the inferential methods presented in this book, and in many other books and courses on statistics, assume the data come from a SRS. The inferential analysis of data obtained through random sampling procedures other than SRS is more complex, and is out of the scope of this book. Many people, including some researchers, have a wrong idea of what is random sampling or a random sample. It is not just an apparently harmless selection procedure like, for instance, consecutive patients. Rather, it requires the probability of selection to be known for all population individuals and, in the case of SRS, this probability should be the same for all of them. If this is not the case, the selection `procedure should not be called random, and the resulting sample should not be described as a random sample. Depending on what is the population of interest, obtaining a SRS may be just impossible (i.e., if there is no census available), and then a convenience sample (any sample we can possibly get) is better than nothing. However, if SRS is possible, there is no excuse to use a non-random sample. In any case, to use inferential methods with a convenience, non-random sample, requires to assume that the sampling process behaves as a random sampling procedure, and that the sample composition resembles that of a random sample. This is actually the case in many research studies, based on convenience samples. 6.5 Example data To illustrate the concepts introduced in the remaining of this chapter we will use the birthwt dataset in package MASS, containing data on 189 births collected at the Baystate Medical Center, Springfield, Massachusetts (US). We will be concerned with the following variables (see ?MASS::birthwt for a description of all the variables in the dataset): low: an indicator of low birth weight (less than 2500 grams). race: mother’s race (1 = white, 2 = black, 3 = other). smoke: mother’s smoking status during pregnancy (0 = no, 1 = yes). ht: mother’s history of arterial hypertension (AHT) (0 = no, 1 = yes). bwt: birth weight in grams. Here are the first six rows of the dataset: head(MASS::birthwt) low age lwt race smoke ptl ht ui ftv bwt 85 0 19 182 2 0 0 0 1 0 2523 86 0 33 155 3 0 0 0 0 3 2551 87 0 20 105 1 1 0 0 0 1 2557 88 0 21 108 1 1 0 0 1 2 2594 89 0 18 107 1 1 0 0 1 0 2600 91 0 21 124 3 0 0 0 0 0 2622 The following script defines factors for categorical variables, renames low to bw (with levels labeled as low or normal) and prints the first six rows of the resulting dataframe d: library(dplyr) d &lt;- MASS::birthwt %&gt;% mutate(low = factor(low, levels = 1:0, labels = c(&quot;low&quot;, &quot;normal&quot;)), race = factor(race, levels = 1:3, labels = c(&quot;white&quot;, &quot;black&quot;, &quot;other&quot;)), smoke = factor(smoke, levels = 1:0, labels = c(&quot;smoker&quot;, &quot;non-smoker&quot;)), ui = factor(ui, levels = 1:0, labels = c(&quot;UI&quot;, &quot;no-UI&quot;)), ht = factor(ht, levels = 1:0, labels = c(&quot;AHT&quot;, &quot;no-AHT&quot;))) %&gt;% rename(bw = low) head(d) bw age lwt race smoke ptl ht ui ftv bwt 85 normal 19 182 black non-smoker 0 no-AHT UI 0 2523 86 normal 33 155 other non-smoker 0 no-AHT no-UI 3 2551 87 normal 20 105 white smoker 0 no-AHT no-UI 1 2557 88 normal 21 108 white smoker 0 no-AHT UI 2 2594 89 normal 18 107 white smoker 0 no-AHT UI 0 2600 91 normal 21 124 other non-smoker 0 no-AHT no-UI 0 2622 6.6 Estimation Proportions and means are common parameters we may want to estimate. Proportions are relevant when dealing with categorical variables (such as bw), while means are relevant when dealing with quantitative variables (such as bwt). For example, we may want to answer these two questions: What is the proportion of low weight newborns (\\(\\pi\\))? What is the mean weight of newborns (\\(\\mu\\))? The simplest way to estimate a parameter is to compute the corresponding estimator: the sample proportion (p) in the case of \\(\\pi\\), or the sample mean (\\(\\bar{x}\\)) in the case of \\(\\mu\\). The result provided by an estimator when computed in a particular sample is called a point estimate of the corresponding parameter. To get a point estimate for \\(\\pi\\), we simply compute the frequency table of bw with function tally() of the mosaic package, using argument format to get the proportions: library(mosaic) tally(~bw, data = d, format = &quot;proportion&quot;) bw low normal 0.3121693 0.6878307 To get a point estimate for \\(\\mu\\), we can use function mean() to compute the sample mean of bwt: mean(~bwt, data = d) [1] 2944.587 So, the point estimates are 31.2 % for \\(\\pi\\), and 2945 grams for \\(\\mu\\) (rounded to unit). The problem with point estimates is that, because of (random) sampling variation, we have no idea of how similar or different from the parameter value they may be. To solve this uncertainty, confidence intervals are computed. A confidence interval (CI) is an interval around the point estimate, such that we have a given confidence level that it will contain the true value of the parameter, somewhere within the interval. Although it is quite common to compute CI’s with 95% confidence, we could use other confidence levels, such as 90%, or 99%. To compute the 95% confidence interval for \\(\\pi\\), it is convenient to chain functions binom.test(), confint() and round(). The computation of the CI is made by function binom.test(), but this function produces other results as well we are not interested in right now, and function confint() extracts just the CI. Then, round() will round the results to the desired number of decimals. This will provide the 95% CI for the category of bw indicated in argument success in the binom.test() function (which is low): binom.test(~ bw, data = d, success = &quot;low&quot;) %&gt;% confint() %&gt;% round(3) probability of success lower upper level 1 0.312 0.247 0.383 0.95 The result provided includes the point estimate (labeled as probability of success), the lower and upper bounds of the CI, and the confidence level (labeled as level). This result means that we are 95% confident that the value of \\(\\pi\\) is somewhere between 24.7% and 38.3%. To compute a CI with a different confidence level, use the conf.level argument in binom.test() to indicate the desired level (as a proportion, not as a percentage!): binom.test(~ bw, data = d, success = &quot;low&quot;, conf.level = .99) %&gt;% confint() %&gt;% round(3) probability of success lower upper level 1 0.312 0.228 0.406 0.99 Thus, with 99% confidence, the value of \\(\\pi\\) is somewhere between 22.8% and 40.6%. Note that, the higher the confidence level, the wider the resulting CI. There are several methods available to compute the CI for a proportion (see ?binom.test and references therein). Most books and courses on statistics present the so called Wald method, which is an approximated method based on the normal distribution. Though perfectly valid in many instances, in some cases it should be avoided, at the risk of getting unreliable or even absurd results (e.g., values lower than zero, or higher than one). For this reason, the default method used by biniom.test() (the so called Clopper-Pearson method) is safer, can be applied always, and never produces values out of the 0 to 1 range. However, if you ever need to compute the Wald CI, you can get it using the ci.method argument: binom.test(~ bw, data = d, success = &quot;low&quot;, ci.method = &quot;Wald&quot;) %&gt;% confint() %&gt;% round(3) probability of success lower upper level 1 0.312 0.246 0.378 0.95 As you see, this CI is slightly different to what we got when we did not specify the ci.method, and the default (Clopper-Pearson) was used. To compute a CI for \\(\\mu\\) we can chain functions t.test() and confint(). Again, the actual CI computation is made by t.test() along with other results, and confint() extracts just the CI. t.test(~ bwt, data = d) %&gt;% confint() mean of x lower upper level 1 2944.587 2839.952 3049.222 0.95 Thus, we are 95% confident that the population mean \\(\\mu\\) is somewhere between 2840 and 3049 grams (rounding to unit). For a different confidence level, the conf.level argument of t.test() can be used: t.test(~ bwt, data = d, conf.level = 0.99) %&gt;% confint() mean of x lower upper level 1 2944.587 2806.558 3082.616 0.99 Again, we see that a higher confidence level results in a wider CI. The method used by t.test() to compute the CI is based on the assumption that the quantitative variable (bwt) is about normally distributed in the population. In practice, it will be sufficient to verify that the distribution is approximately symmetric, and there are no influential outliers. This can be easily verified by inspecting a boxplot: library(ggformula) gf_boxplot(~ bwt, data = d) Figure 6.8: Boxplot of birthweights In this case, the distribution is fairly symmetric. There is a single outlier, pretty close to the edge of the lower whisker, so it can hardly affect the mean, given the sample size (n = 189). Therefore, we can rely on the CI computed by t.test(). If this was not the case, alternative methods are available, such as the bootstrap method that will be presented in subsequent chapters. We have seen how to compute CI’s for a proportion and for a mean, but CI’s can be computed for many other parameters as well, such as: A difference of proportions, \\(\\pi_1 - \\pi_2\\) A difference of means, \\(\\mu_1 - \\mu_2\\) A relative risk An odds ratio … and many others. To compute a CI for these and other parameters, we will need to use different functions. However, the result will be always an interval (defined by a lower and an upper bound), and its interpretation is always the same: we have the specified confidence that the CI contains the value of the population parameter we are estimating. When we compute a CI with a given confidence level, there is some risk that it does not contain the value of the parameter. This risk is the complementary of the confidence level, so that: 95% CI’s will not contain the parameter about 5% of the times 99% CI’s will not contain the parameter about 1% of the times Therefore, the higher the confidence level used to compute a CI, the lower the risk. So, why not to compute CI’s always with a very high confidence level, such as 0.999999? Well, we have seen that increasing the confidence level results in wider, less informative CI’s. The width of the CI reflects uncertainty (since we can’t know where the parameter value is inside the CI). Therefore, the wider the CI the less informative it is. Fortunately, the width of the CI depends also on the sample size, and this makes it possible to get high confidence, as well as narrow, informative CI’s. 6.7 Sample size and CI’s Figure 6.9 shows the 95% CI’s for a proportion \\(\\pi\\) resulting from several hypothetical studies, all of them having the same sample proportion p = 0.312 (or 31.2%), but different sample size (n). It i s clear that the CI shrinks as n increases. This means that, for a given value of p, we can choose a sample size that will produce a 95% CI of desired width. Of course, this is relevant when designing a study (once the study has been conducted, the sample size is fixed). For instance, if we want the CI width to be no larger than 0.10 (or 10%), we need to collect n = 350 subjects. Figure 6.9: Effect of sample size on the CI for a proportion (p = 0.312) When designing a study whose objective is to estimate a parameter with a given confidence level (say 95%), an important question is to determine the sample size needed, so that the resulting CI width is no greater than a desired value. However, because the CI width depends also on the value of the point estimate (p), figure 6.9 cannot be used as guidance (but in the case of expecting p to be 0.312). Function prec_prop() in package presize allows to compute the required sample size from the expected value of p, the desired width of the CI, and the confidence level. Several methods are available, depending on how the CI will be computed. Here we use method = exact. By default, 0.95 (or 95%) confidence level is assumed. The printed result documents the arguments used in the computation, and the required sample size (n). library(presize) prec_prop(p = 0.312, conf.width = 0.10, method = &quot;exact&quot;) sample size for a proportion with exact confidence interval. p padj n conf.width conf.level lwr upr 1 0.312 NA 347.2988 0.1 0.95 0.2636203 0.3636203 NOTE: padj is the adjusted proportion, from which the ci is calculated. Thus, assuming we expect p = 0.312, to get a 95% CI of width = 0.10, the required sample size is 348 (obviously, the sample size has to be an integer since it is the number of subjects, so that the result should be rounded up always). If the CI is to be computed with a different confidence level (say 0.99) the conf.level argument should be set to this value as done below. Because increasing the confidence level results in a wider CI, the sample size required to keep the CI width = 0.10 is now n = 586, much larger than it was for a 95% CI. prec_prop(p = 0.312, conf.width = 0.10, method = &quot;exact&quot;, conf.level = 0.99) sample size for a proportion with exact confidence interval. p padj n conf.width conf.level lwr upr 1 0.312 NA 585.9679 0.1 0.99 0.2635561 0.3635561 NOTE: padj is the adjusted proportion, from which the ci is calculated. We have seen that the sample size has an effect on the width of the CI for a proportion, and how to use this fact to determine the sample size when designing a study. Similar arguments hold for many other parameters (i.e., increasing the sample size reduces their CI width), and in particular for means. Suppose we are designing a new study to estimate the mean weight of newborns, and we want the resulting 95% CI to have a width of 200 grams. We can use function proc_mean() from the presize package to determine the required sample size, but in this case we need to provide an estimate of the standard deviation (SD) of the weight of newborns, and we assume this to be 730 grams. Though it is not necessary for the calculation, this function expects an argument specifying the expected mean (mu). prec_mean(mean = 3000, sd = 730, conf.width = 200) sample size for mean mean sd n conf.width conf.level lwr upr 1 3000 730 207.1364 200 0.95 2900 3100 Thus, assuming that SD = 730 grams, we require a sample of size n = 208 to get a 95 CI of width = 200 grams. If the CI is to be computed with a different confidence level (e.g., 0.99), the conf.level argument is used: prec_mean(mean = 3000, sd = 730, conf.width = 200, conf.level = 0.99) sample size for mean mean sd n conf.width conf.level lwr upr 1 3000 730 357.3903 200 0.99 2900 3100 In this case, we would require a sample size of n = 358. 6.8 Significance tests Significance tests are tools for evaluating scientific hypotheses. To explain the basic concepts involved in a significant test, let’s consider the following example. According to a National Vital Statistics Report, 8.31% of all births in the US in 2019 were low weight births (&lt;2500 grams). Suppose we suspect that this could be higher in the population of deliveries assisted in the Baystate Medical Center, and use the sample of births in the birthwt dataset to verify it. To conduct a significance test, we set a pair of conflicting formal statistical hypotheses called null and alternative, denoted \\(H_0\\) and \\(H_1\\) respectively. In this case, these hypotheses involve a single parameter \\(\\pi\\), that represents the proportion of low birthweight in the population of deliveries assisted in the Baystate Medical Center: \\(H_0: \\qquad \\pi = 0.0831\\) \\(H_1: \\qquad \\pi &gt; 0.0831\\) Note that the working hypothesis stated in the first `paragraph of this section (we suspect that this could be higher in the population of deliveries assisted in the Baystate Medical Center) is represented by the alternative hypothesis \\(H_1\\), the null hypothesis \\(H_0\\) being the opposite (i,e., the proportion of low birthweights in the population of deliveries assisted in the Baystate Medical Center is as in the whole US); so that \\(H_0\\) and \\(H_1\\) are mutually exclusive. In the birthwt dataset, the sample proportion of low birthweigh is 0.312. Now we compute the probability of getting a random sample of 189 births in which this proportion is 0.312 or more extreme in the direction of \\(H_1\\), if \\(H_0\\) was true. This probability is called p value, and in this case is &lt;0.0000001 (that is, less than one in ten million). Now, there are two possible explanations for this result (assuming the sample is a random sample): \\(H_0\\) is true, and something very unlikely happened when drawing the sample, or \\(H_0\\) is false, in which case, the computed p value does not reflect the actual probability of the sample, since it was computed assuming \\(H_0\\) is true). Clearly, the second explanation seems better, since it does not require to believe that something very unlikely happened. Therefore, a very unlikely result under \\(H_0\\), is taken as evidence against \\(H_0\\), and supports its rejection in favor of \\(H_1\\). Thus, in this case we conclude that the proportion of low birthweight in the population of deliveries assisted in the Baystate Medical Center is higher than that in the whole US. A difficulty with this argument is to decide what probability values should be considered very unlikely. It is usual to take values below 0.05 as unlikely enough to reject \\(H_0\\), but this is a choice we make as investigators, and there is nothing wrong with choosing a different, perhaps more strict value, like 0.01. In any case, the probability value below which we will reject a null hypothesis in a test is called the significance level of the test, is denoted by the Greek letter \\(\\alpha\\), and should be established before getting the sample data. In this book, we will use \\(\\alpha\\) = 0.05 unless otherwise stated. But whatever the value of \\(\\alpha\\) chosen a priori by an investigator, a p value &lt; \\(\\alpha\\) is used as argument to reject \\(H_0\\) and conclude \\(H_1\\). To compute the p value reported above, we used function binom.test(). This function is available both in base R and in the mosaic package, but here we use the later (by loading mosaic previously). The arguments we need to pass to binom.test() are the factor bw preceded by a tilde and the dataframe containing this factor, the value proposed by the null hypothesis for \\(\\pi\\) in argument p, the category for which we want to estimate the proportion in argument success, and the form of alternative hypothesis in argument alternative): binom.test(~ bw, data=d, p = 0.0831, # Null hypothesis success = &quot;low&quot;, # Category of bw we are interested in alternative = &quot;greater&quot;) # Alternative hypothesis data: d$bw [with success = low] number of successes = 59, number of trials = 189, p-value &lt; 2.2e-16 alternative hypothesis: true probability of success is greater than 0.0831 95 percent confidence interval: 0.2565953 1.0000000 sample estimates: probability of success 0.3121693 The results offered by binom.test() will always refer to the category indicated in its argument success (or to the first level of the factor if argument success is not used), and are identified in the output with the word “success”. The first line of the results includes the number of low cases (labeled as “number of successes”), the sample size (“number of trials”), and the p value in scientific notation. In a second line, the alternative hypothesis of the test is stated in words. In addition, the 95% CI for \\(\\pi\\) is provided, as well as the sample proportion (“probability of success”). Note that the CI excludes the value proposed by \\(H_0\\), which is consistent with the rejection of \\(H_0\\) implied by the extremely low p value. In summary, when we conduct a significance test we compute a p value, which is a measure of how consistent the data are with \\(H_0\\). A very low p value (p &lt; \\(\\alpha\\)) indicates that the data are very inconsistent with \\(H_0\\), leading to the rejection of \\(H_0\\). Conversely, a high p value reflects consistency of the data and \\(H_0\\), but by no means this can be taken as proof that \\(H_0\\) is true. Therefore, when p \\(\\ge \\alpha\\) we can neither reject \\(H_0\\), nor conclude it is true: this is an inconclusive result. Current guidelines recommend to report p values with two or three decimal places at most, so that the p value we computed previously should be reported as &lt;0.001. However, when a p value is greater than that, its actual value should be reported (e.g. p = 0.021, rather than p &lt; 0.05). 6.9 Sample size and p values The p value of a test depends on how different the point estimate is from the parameter value proposed by \\(H_0\\): the more different they are, the lower the p value. For instance, suppose that we conduct the binomial test of the previous section on a sample of 100 births, 10 of which are low weight. The sample proportion is then p = 0.1 (or 10%). In this case, the p value would be 0.319, implying that the result is not very inconsistent with \\(H_0\\). Indeed, a sample proportion of 0.1 is not very different from the population value proposed by \\(H_0: \\pi = 0.0831\\). Conversely, if the sample proportion was 0.2 (or 20%), which is quite different from 0.0831, the p value would be &lt;0.001. However, p values depend on the sample size as well. Table 6.1 shows the p values obtained in the binomial test for a series of hypothetical studies, all having a sample proportion of 0.10, but differing in sample size. Although all studies have the same sample proportion, p values decrease as the sample size increases. In studies with a sample size of 800 births or less, the p value does not allow to reject \\(H_0\\), but studies based on 850 or more births would lead to the its rejection, providing evidence that the population proportion is greater than 0.0831. Table 6.1: Effect of the sample size on the p value of a binomial test with sample proportion 0.1 (or 10%) n x p p-value 100 10 0.1 0.319 150 15 0.1 0.265 200 20 0.1 0.225 250 25 0.1 0.194 300 30 0.1 0.169 350 35 0.1 0.148 400 40 0.1 0.130 450 45 0.1 0.114 500 50 0.1 0.101 550 55 0.1 0.090 600 60 0.1 0.080 650 65 0.1 0.071 700 70 0.1 0.063 750 75 0.1 0.057 800 80 0.1 0.051 850 85 0.1 0.045 900 90 0.1 0.041 950 95 0.1 0.036 1000 100 0.1 0.033 The dependency of the p values on the sample size has two important implications. First, the p value (or, its “significance”), is not particularly informative in general. Rather, the point and CI estimates are much more informative. Sometimes it is said that different studies have conflicting results only because some of them report a “statistically significant” result (p &lt; 0.05) while others do not (p &gt; 0.05). However, if the point estimates are similar there is no conflict at all, and a different sample size could be enough to explain the apparent conflict. The second implication is that, when designing a study, we can choose a sample size that will produce a statistically significant result with high probability. This is called sample size determination, and is an important aspect of study design. The computation of the sample size for a particular study will depend on some study design characteristics, and on the test planned for the inferential analysis of its primary objective. 6.10 Types of tests There are lots of statistical tests available, and choosing the appropriate one for a particular problem may be challenging. Tests can be classified according to several criteria, such as the form of the null and alternative hypotheses, the parameter(s) involved, or the assumptions they are based on. Depending on the form of the null hypothesis, we distinguish one-sample, two-samples or many-samples tests. The general form of the null hypothesis for these three types of test is shown below, where \\(\\theta\\) is some parameter of interest, and \\(k\\) some specific value: One-sample tests \\(H_0: \\theta = k\\) Two-sample tests \\(H_0: \\theta_1 = \\theta_2\\) Many samples \\(H_0: \\theta_1 = \\theta_2 = ...= \\theta_S\\) In one-sample tests, a particular value is proposed for a parameter. This was the case of the binomial test addressed in the previous section, where the parameter was a population proportion (\\(H_0: \\quad \\pi = 0.0831\\)). Two-sample tests compare a parameter in two populations. For example, we could compare the proportion of low birthweight cases in the populations of smoker and non-smoker mothers, setting the null hypothesis \\(H_0: \\quad \\pi_{smoker} = \\pi_{non-smoker}\\). Many-samples tests compare a parameter in more than two populations. For example, we could compare the proportion of low birthweight cases in the populations defined by race, setting the null hypothesis \\(H_0: \\quad \\pi_{white} = \\pi_{black} = \\pi_{other}\\). Correspondingly similar examples would result by changing the parameter to a population mean (\\(\\mu\\)) of birth weights in grams. According to the form of the alternative hypothesis, we can distinguish two-sided or one-sided tests. In a two-sided test, \\(H_1\\) is a strict inequality. In a one-sided test, only one of the two directions (greater or lower) is of interest: Two-sided \\(H_1: \\theta \\ne k\\) One-sided \\(H_1: \\theta \\gt k \\qquad \\text{or} \\qquad H_1: \\theta \\lt k\\) The test conducted in the previous section was a one-sided test, since the alternative hypothesis was \\(H_1: \\quad \\pi &gt; 0.0831\\). We decided to set this alternative hypothesis because the working hypothesis was “we suspect that the proportion of low birthweight could be higher than in the whole US”. In general, one- and two-sided tests will produce a different p-value. For this reason it is important to be careful when setting \\(H_1\\). Unless the working hypothesis clearly states an expected direction and the other direction is of no interest at all, two-sided tests should be used. Some tests are based on assumptions about the data. In particular, many classic tests on means assume that the variable follows a normal distribution in the population(s). These are generically called parametric tests. Conversely, some other tests do not require such an assumption, and are called non-parametric or distribution-free tests. Last, tests are usually described by the type of parameter involved. When analyzing categorical variables, tests on proportions are usually performed. When analyzing quantitative variables, tests on means are most used. As stated at the beginning of this section, the number of significance tests available is endless (every day new tests are published in statistical journals), and any attempt to present a “complete” collection of tests covering all possible data analysis scenarios would be naive. Rather, in the following chapters, we will cover a selection of few tests commonly used in clinical research. Among them: Test for categorical variables: Binomial test and one-sample proportion z-test Independence Chi-square test McNemar’s test Parametric tests for quantitative variables: One-sample (or paired) Student’s t-test Two-sample (or unpaired) Student’s t-test Welch test Distribution-free tests for quantitative variables: Wilcoxon’s rank sum test or Mann-Whitney’s test Wilcoxon’s signed rank test Linear independence test on the Pearson’s correlation coefficient Linear independence test on the Spearman’s correlation coefficient Intercept and slope test of a regression line Resources An interesting read about common missinterpretations of p-values and confidence intervals. Journal collections devoted to statistical methods: Nature’s Points of Significance. BMJ’s Statistics notes. StatQuest is a youtube channel with several lists of videos, one of them devoted to Statistics fundamentals. Package presize includes functions to compute the required sample size to estimate many parameters other than a mean or a proportion. In addition, a point-and-click web app implementing these functions is available here. For a more detailed explanation of inferential statistics (covering both hypothesis testing, confidence intervals and how they relate to each other) see this R-bloggers post. Exercises What is the term used in inferential statistics to refer to population quantities of interest? What a are the two parameters of a normal distribution, and how they relate to its graphical representation? What type of sample is assumed by common inferential methods (such as those in these book and in many other books and courses on statistics)? What property of a sampling procedure allows to claim that the resulting sample is a simple random sample? What is the interpretation of a 95% confidence interval? What is a p value? What is the difference between two-sided and one-sided tests? What is the difference between parametric and non-parametric tests? A dataframe contains the hospital record numbers (HRN), and other variables, for a population of 8000 patients. Run the following code to simulate such a dataframe. Then look at the help of function sample_n() from package dplyr, and use it to randomly select 20 patients from dataframe d. In a study newborns conducted in a hospital of a different geographical area, only 2 out of 150 newborns were low weight. Look at the help of the binom.test() function in the mosaicpackage, and use it to compute a 95% CI using the Wald method. Then repeat using the default Clopper-Pearson method. You are designing a study to estimate the proportion of COVID-19 cases seen in the emergency department of a tertiary hospital who present dyspnea on admission. Determine the required sample size to get 95% CI of width 0.10 in three different scenarios: if you guess that this proportion is about 0.2 (or 50%). if you guess that this proportion is about 0.5 (or 50%). if you guess that this proportion is about 0.8 (or 50%). What value would you choose to determine the sample size if you have no idea what this value can be? Why? What is the sample size required to estimate the mean weight of newborns with 95% confidence, so that the CI width is 100 grams, assuming a SD of 730 grams and a mean of 3000 grams? What is the result if you assume a mean of 2000 grams? An what if you assume an SD = 800 grams? in some cases, statistical hypotheses refer to probabilistic models rather than simple parameters. However, most of the statistical hypotheses we are dealing with in this book do test hypotheses on parameters.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
